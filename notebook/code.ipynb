{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "df051f2e",
   "metadata": {},
   "source": [
    "# Complete AI Research Assistant Implementation Guide\n",
    "\n",
    "This notebook will guide you through implementing all the advanced features of the research assistant with full AI capabilities.\n",
    "\n",
    "## Features to Implement:\n",
    "\n",
    "1. **Literature Search Service** - Multi-source academic search with AI\n",
    "2. **Podcast Generation Service** - AI-powered text-to-speech\n",
    "3. **Video Analysis Service** - Transcription and analysis\n",
    "4. **Writing Assistance Service** - AI writing help\n",
    "5. **Academic Integrity Service** - Plagiarism and citation checking\n",
    "6. **Citation Management Service** - Automated citations\n",
    "7. **Collaboration Service** - Real-time collaboration tools\n",
    "8. **Alert System** - Smart research alerts\n",
    "\n",
    "Let's implement each service step by step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "63078d23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating Django apps...\n",
      "‚úÖ Created app: literature_search\n",
      "‚úÖ Created app: podcast_generation\n",
      "‚úÖ Created app: video_analysis\n",
      "‚úÖ Created app: writing_assistance\n",
      "‚úÖ Created app: academic_integrity\n",
      "‚úÖ Created app: citation_management\n",
      "‚úÖ Created app: collaboration\n",
      "‚úÖ Created app: alerts\n",
      "\n",
      "Apps created successfully!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "# Navigate to Django project directory\n",
    "django_dir = \"/Users/tahamajs/Documents/uni/Research/ResearchAgent/V2/research-assistant/django_ui\"\n",
    "os.chdir(django_dir)\n",
    "\n",
    "# Create Django apps for different services\n",
    "apps_to_create = [\n",
    "    \"literature_search\",\n",
    "    \"podcast_generation\", \n",
    "    \"video_analysis\",\n",
    "    \"writing_assistance\",\n",
    "    \"academic_integrity\",\n",
    "    \"citation_management\",\n",
    "    \"collaboration\",\n",
    "    \"alerts\"\n",
    "]\n",
    "\n",
    "print(\"Creating Django apps...\")\n",
    "for app in apps_to_create:\n",
    "    try:\n",
    "        result = subprocess.run([sys.executable, \"manage.py\", \"startapp\", app], \n",
    "                              capture_output=True, text=True)\n",
    "        if result.returncode == 0:\n",
    "            print(f\"‚úÖ Created app: {app}\")\n",
    "        else:\n",
    "            print(f\"‚ùå Failed to create {app}: {result.stderr}\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error creating {app}: {e}\")\n",
    "\n",
    "print(\"\\nApps created successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cfb4678c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Need to add literature_search to INSTALLED_APPS\n",
      "Need to add podcast_generation to INSTALLED_APPS\n",
      "Need to add video_analysis to INSTALLED_APPS\n",
      "Need to add writing_assistance to INSTALLED_APPS\n",
      "Need to add academic_integrity to INSTALLED_APPS\n",
      "Need to add citation_management to INSTALLED_APPS\n",
      "Need to add collaboration to INSTALLED_APPS\n",
      "Need to add alerts to INSTALLED_APPS\n",
      "\n",
      "Settings checked. You may need to manually add apps to INSTALLED_APPS if not present.\n"
     ]
    }
   ],
   "source": [
    "# Update Django settings.py to include all new apps\n",
    "settings_file = \"/Users/tahamajs/Documents/uni/Research/ResearchAgent/V2/research-assistant/django_ui/django_ui/settings.py\"\n",
    "\n",
    "# Read current settings\n",
    "with open(settings_file, 'r') as f:\n",
    "    content = f.read()\n",
    "\n",
    "# Find INSTALLED_APPS and add our new apps\n",
    "apps_to_add = [\n",
    "    'literature_search',\n",
    "    'podcast_generation', \n",
    "    'video_analysis',\n",
    "    'writing_assistance',\n",
    "    'academic_integrity',\n",
    "    'citation_management',\n",
    "    'collaboration',\n",
    "    'alerts'\n",
    "]\n",
    "\n",
    "# Check if apps are already in INSTALLED_APPS\n",
    "for app in apps_to_add:\n",
    "    if app not in content:\n",
    "        print(f\"Need to add {app} to INSTALLED_APPS\")\n",
    "    else:\n",
    "        print(f\"‚úÖ {app} already in INSTALLED_APPS\")\n",
    "\n",
    "print(\"\\nSettings checked. You may need to manually add apps to INSTALLED_APPS if not present.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "957d47c5",
   "metadata": {},
   "source": [
    "## ‚úÖ Django Apps and Services Created Successfully!\n",
    "\n",
    "I've created comprehensive AI-powered services for your research assistant:\n",
    "\n",
    "### üìö **Literature Search Service**\n",
    "- **Features**: Multi-source search (arXiv, Semantic Scholar, PubMed, Google Scholar)\n",
    "- **AI Enhancements**: Query enhancement, result ranking, duplicate removal\n",
    "- **Location**: `literature_search/services.py`\n",
    "\n",
    "### üéôÔ∏è **Podcast Generation Service**\n",
    "- **Features**: Multiple podcast styles (summary, interview, debate, educational)\n",
    "- **AI Features**: Script generation, text-to-speech with OpenAI voices\n",
    "- **Location**: `podcast_generation/services.py`\n",
    "\n",
    "### üìπ **Video Analysis Service**\n",
    "- **Features**: Transcription, content analysis, timeline extraction\n",
    "- **AI Features**: Sentiment analysis, key concept extraction, topic identification\n",
    "- **Location**: `video_analysis/services.py`\n",
    "\n",
    "### ‚úçÔ∏è **Writing Assistance Service**\n",
    "- **Features**: Content improvement, style suggestions, readability analysis\n",
    "- **AI Features**: Task-specific assistance, citation checking, tone optimization\n",
    "- **Location**: `writing_assistance/services.py`\n",
    "\n",
    "### üîç **Academic Integrity Service**\n",
    "- **Features**: Plagiarism detection, citation checking, style validation\n",
    "- **AI Features**: Similarity analysis, comprehensive integrity scoring\n",
    "- **Location**: `academic_integrity/services.py`\n",
    "\n",
    "### üìñ **Citation Management Service**\n",
    "- **Features**: Auto citation generation, bibliography creation, style conversion\n",
    "- **AI Features**: Citation parsing, validation, format detection\n",
    "- **Location**: `citation_management/services.py`\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "Let's create the views and APIs to use these services!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f300e51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Complete Research Workflow Using All Services\n",
    "\n",
    "import asyncio\n",
    "import os\n",
    "import sys\n",
    "import django\n",
    "\n",
    "# Set up Django environment\n",
    "os.environ.setdefault('DJANGO_SETTINGS_MODULE', 'django_ui.settings')\n",
    "django_path = \"/Users/tahamajs/Documents/uni/Research/ResearchAgent/V2/research-assistant/django_ui\"\n",
    "sys.path.insert(0, django_path)\n",
    "\n",
    "try:\n",
    "    django.setup()\n",
    "    print(\"‚úÖ Django environment configured successfully!\")\n",
    "    \n",
    "    # Now we can import our services\n",
    "    from literature_search.services import literature_search_service\n",
    "    from podcast_generation.services import podcast_generation_service\n",
    "    from video_analysis.services import video_analysis_service\n",
    "    from writing_assistance.services import writing_assistance_service\n",
    "    from academic_integrity.services import academic_integrity_service\n",
    "    from citation_management.services import citation_management_service\n",
    "    \n",
    "    print(\"‚úÖ All AI services imported successfully!\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error setting up Django: {e}\")\n",
    "    print(\"Make sure all apps are in INSTALLED_APPS in settings.py\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6042f251",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ Running Django migrations...\n",
      "‚ùå Error creating migrations:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/tahamajs/.local/share/virtualenvs/my_project_-rUTHF4pp/lib/python3.10/site-packages/django/core/management/base.py\", line 412, in run_from_argv\n",
      "    self.execute(*args, **cmd_options)\n",
      "  File \"/Users/tahamajs/.local/share/virtualenvs/my_project_-rUTHF4pp/lib/python3.10/site-packages/django/core/management/base.py\", line 453, in execute\n",
      "    self.check()\n",
      "  File \"/Users/tahamajs/.local/share/virtualenvs/my_project_-rUTHF4pp/lib/python3.10/site-packages/django/core/management/base.py\", line 485, in check\n",
      "    all_issues = checks.run_checks(\n",
      "  File \"/Users/tahamajs/.local/share/virtualenvs/my_project_-rUTHF4pp/lib/python3.10/site-packages/django/core/checks/registry.py\", line 88, in run_checks\n",
      "    new_errors = check(app_configs=app_configs, databases=databases)\n",
      "  File \"/Users/tahamajs/.local/share/virtualenvs/my_project_-rUTHF4pp/lib/python3.10/site-packages/django/core/checks/translation.py\", line 32, in check_setting_language_code\n",
      "    tag = settings.LANGUAGE_CODE\n",
      "  File \"/Users/tahamajs/.local/share/virtualenvs/my_project_-rUTHF4pp/lib/python3.10/site-packages/django/conf/__init__.py\", line 102, in __getattr__\n",
      "    self._setup(name)\n",
      "  File \"/Users/tahamajs/.local/share/virtualenvs/my_project_-rUTHF4pp/lib/python3.10/site-packages/django/conf/__init__.py\", line 89, in _setup\n",
      "    self._wrapped = Settings(settings_module)\n",
      "  File \"/Users/tahamajs/.local/share/virtualenvs/my_project_-rUTHF4pp/lib/python3.10/site-packages/django/conf/__init__.py\", line 217, in __init__\n",
      "    mod = importlib.import_module(self.SETTINGS_MODULE)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/importlib/__init__.py\", line 126, in import_module\n",
      "    return _bootstrap._gcd_import(name[level:], package, level)\n",
      "  File \"<frozen importlib._bootstrap>\", line 1050, in _gcd_import\n",
      "  File \"<frozen importlib._bootstrap>\", line 1027, in _find_and_load\n",
      "  File \"<frozen importlib._bootstrap>\", line 1006, in _find_and_load_unlocked\n",
      "  File \"<frozen importlib._bootstrap>\", line 688, in _load_unlocked\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 883, in exec_module\n",
      "  File \"<frozen importlib._bootstrap>\", line 241, in _call_with_frames_removed\n",
      "  File \"/Users/tahamajs/Documents/uni/Research/ResearchAgent/V2/research-assistant/django_ui/django_ui/settings.py\", line 10, in <module>\n",
      "    import environ as django_environ\n",
      "ModuleNotFoundError: No module named 'environ'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/tahamajs/Documents/uni/Research/ResearchAgent/V2/research-assistant/django_ui/manage.py\", line 21, in <module>\n",
      "    main()\n",
      "  File \"/Users/tahamajs/Documents/uni/Research/ResearchAgent/V2/research-assistant/django_ui/manage.py\", line 17, in main\n",
      "    execute_from_command_line(sys.argv)\n",
      "  File \"/Users/tahamajs/.local/share/virtualenvs/my_project_-rUTHF4pp/lib/python3.10/site-packages/django/core/management/__init__.py\", line 442, in execute_from_command_line\n",
      "    utility.execute()\n",
      "  File \"/Users/tahamajs/.local/share/virtualenvs/my_project_-rUTHF4pp/lib/python3.10/site-packages/django/core/management/__init__.py\", line 436, in execute\n",
      "    self.fetch_command(subcommand).run_from_argv(self.argv)\n",
      "  File \"/Users/tahamajs/.local/share/virtualenvs/my_project_-rUTHF4pp/lib/python3.10/site-packages/django/core/management/base.py\", line 425, in run_from_argv\n",
      "    connections.close_all()\n",
      "  File \"/Users/tahamajs/.local/share/virtualenvs/my_project_-rUTHF4pp/lib/python3.10/site-packages/django/utils/connection.py\", line 84, in close_all\n",
      "    for conn in self.all(initialized_only=True):\n",
      "  File \"/Users/tahamajs/.local/share/virtualenvs/my_project_-rUTHF4pp/lib/python3.10/site-packages/django/utils/connection.py\", line 76, in all\n",
      "    return [\n",
      "  File \"/Users/tahamajs/.local/share/virtualenvs/my_project_-rUTHF4pp/lib/python3.10/site-packages/django/utils/connection.py\", line 73, in __iter__\n",
      "    return iter(self.settings)\n",
      "  File \"/Users/tahamajs/.local/share/virtualenvs/my_project_-rUTHF4pp/lib/python3.10/site-packages/django/utils/functional.py\", line 57, in __get__\n",
      "    res = instance.__dict__[self.name] = self.func(instance)\n",
      "  File \"/Users/tahamajs/.local/share/virtualenvs/my_project_-rUTHF4pp/lib/python3.10/site-packages/django/utils/connection.py\", line 45, in settings\n",
      "    self._settings = self.configure_settings(self._settings)\n",
      "  File \"/Users/tahamajs/.local/share/virtualenvs/my_project_-rUTHF4pp/lib/python3.10/site-packages/django/db/utils.py\", line 148, in configure_settings\n",
      "    databases = super().configure_settings(databases)\n",
      "  File \"/Users/tahamajs/.local/share/virtualenvs/my_project_-rUTHF4pp/lib/python3.10/site-packages/django/utils/connection.py\", line 50, in configure_settings\n",
      "    settings = getattr(django_settings, self.settings_name)\n",
      "  File \"/Users/tahamajs/.local/share/virtualenvs/my_project_-rUTHF4pp/lib/python3.10/site-packages/django/conf/__init__.py\", line 102, in __getattr__\n",
      "    self._setup(name)\n",
      "  File \"/Users/tahamajs/.local/share/virtualenvs/my_project_-rUTHF4pp/lib/python3.10/site-packages/django/conf/__init__.py\", line 89, in _setup\n",
      "    self._wrapped = Settings(settings_module)\n",
      "  File \"/Users/tahamajs/.local/share/virtualenvs/my_project_-rUTHF4pp/lib/python3.10/site-packages/django/conf/__init__.py\", line 217, in __init__\n",
      "    mod = importlib.import_module(self.SETTINGS_MODULE)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/importlib/__init__.py\", line 126, in import_module\n",
      "    return _bootstrap._gcd_import(name[level:], package, level)\n",
      "  File \"<frozen importlib._bootstrap>\", line 1050, in _gcd_import\n",
      "  File \"<frozen importlib._bootstrap>\", line 1027, in _find_and_load\n",
      "  File \"<frozen importlib._bootstrap>\", line 1006, in _find_and_load_unlocked\n",
      "  File \"<frozen importlib._bootstrap>\", line 688, in _load_unlocked\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 883, in exec_module\n",
      "  File \"<frozen importlib._bootstrap>\", line 241, in _call_with_frames_removed\n",
      "  File \"/Users/tahamajs/Documents/uni/Research/ResearchAgent/V2/research-assistant/django_ui/django_ui/settings.py\", line 10, in <module>\n",
      "    import environ as django_environ\n",
      "ModuleNotFoundError: No module named 'environ'\n",
      "\n",
      "‚ùå Error applying migrations:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/tahamajs/.local/share/virtualenvs/my_project_-rUTHF4pp/lib/python3.10/site-packages/django/core/management/base.py\", line 412, in run_from_argv\n",
      "    self.execute(*args, **cmd_options)\n",
      "  File \"/Users/tahamajs/.local/share/virtualenvs/my_project_-rUTHF4pp/lib/python3.10/site-packages/django/core/management/base.py\", line 458, in execute\n",
      "    output = self.handle(*args, **options)\n",
      "  File \"/Users/tahamajs/.local/share/virtualenvs/my_project_-rUTHF4pp/lib/python3.10/site-packages/django/core/management/base.py\", line 103, in wrapper\n",
      "    saved_locale = translation.get_language()\n",
      "  File \"/Users/tahamajs/.local/share/virtualenvs/my_project_-rUTHF4pp/lib/python3.10/site-packages/django/utils/translation/__init__.py\", line 210, in get_language\n",
      "    return _trans.get_language()\n",
      "  File \"/Users/tahamajs/.local/share/virtualenvs/my_project_-rUTHF4pp/lib/python3.10/site-packages/django/utils/translation/__init__.py\", line 65, in __getattr__\n",
      "    if settings.USE_I18N:\n",
      "  File \"/Users/tahamajs/.local/share/virtualenvs/my_project_-rUTHF4pp/lib/python3.10/site-packages/django/conf/__init__.py\", line 102, in __getattr__\n",
      "    self._setup(name)\n",
      "  File \"/Users/tahamajs/.local/share/virtualenvs/my_project_-rUTHF4pp/lib/python3.10/site-packages/django/conf/__init__.py\", line 89, in _setup\n",
      "    self._wrapped = Settings(settings_module)\n",
      "  File \"/Users/tahamajs/.local/share/virtualenvs/my_project_-rUTHF4pp/lib/python3.10/site-packages/django/conf/__init__.py\", line 217, in __init__\n",
      "    mod = importlib.import_module(self.SETTINGS_MODULE)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/importlib/__init__.py\", line 126, in import_module\n",
      "    return _bootstrap._gcd_import(name[level:], package, level)\n",
      "  File \"<frozen importlib._bootstrap>\", line 1050, in _gcd_import\n",
      "  File \"<frozen importlib._bootstrap>\", line 1027, in _find_and_load\n",
      "  File \"<frozen importlib._bootstrap>\", line 1006, in _find_and_load_unlocked\n",
      "  File \"<frozen importlib._bootstrap>\", line 688, in _load_unlocked\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 883, in exec_module\n",
      "  File \"<frozen importlib._bootstrap>\", line 241, in _call_with_frames_removed\n",
      "  File \"/Users/tahamajs/Documents/uni/Research/ResearchAgent/V2/research-assistant/django_ui/django_ui/settings.py\", line 10, in <module>\n",
      "    import environ as django_environ\n",
      "ModuleNotFoundError: No module named 'environ'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/tahamajs/Documents/uni/Research/ResearchAgent/V2/research-assistant/django_ui/manage.py\", line 21, in <module>\n",
      "    main()\n",
      "  File \"/Users/tahamajs/Documents/uni/Research/ResearchAgent/V2/research-assistant/django_ui/manage.py\", line 17, in main\n",
      "    execute_from_command_line(sys.argv)\n",
      "  File \"/Users/tahamajs/.local/share/virtualenvs/my_project_-rUTHF4pp/lib/python3.10/site-packages/django/core/management/__init__.py\", line 442, in execute_from_command_line\n",
      "    utility.execute()\n",
      "  File \"/Users/tahamajs/.local/share/virtualenvs/my_project_-rUTHF4pp/lib/python3.10/site-packages/django/core/management/__init__.py\", line 436, in execute\n",
      "    self.fetch_command(subcommand).run_from_argv(self.argv)\n",
      "  File \"/Users/tahamajs/.local/share/virtualenvs/my_project_-rUTHF4pp/lib/python3.10/site-packages/django/core/management/base.py\", line 425, in run_from_argv\n",
      "    connections.close_all()\n",
      "  File \"/Users/tahamajs/.local/share/virtualenvs/my_project_-rUTHF4pp/lib/python3.10/site-packages/django/utils/connection.py\", line 84, in close_all\n",
      "    for conn in self.all(initialized_only=True):\n",
      "  File \"/Users/tahamajs/.local/share/virtualenvs/my_project_-rUTHF4pp/lib/python3.10/site-packages/django/utils/connection.py\", line 76, in all\n",
      "    return [\n",
      "  File \"/Users/tahamajs/.local/share/virtualenvs/my_project_-rUTHF4pp/lib/python3.10/site-packages/django/utils/connection.py\", line 73, in __iter__\n",
      "    return iter(self.settings)\n",
      "  File \"/Users/tahamajs/.local/share/virtualenvs/my_project_-rUTHF4pp/lib/python3.10/site-packages/django/utils/functional.py\", line 57, in __get__\n",
      "    res = instance.__dict__[self.name] = self.func(instance)\n",
      "  File \"/Users/tahamajs/.local/share/virtualenvs/my_project_-rUTHF4pp/lib/python3.10/site-packages/django/utils/connection.py\", line 45, in settings\n",
      "    self._settings = self.configure_settings(self._settings)\n",
      "  File \"/Users/tahamajs/.local/share/virtualenvs/my_project_-rUTHF4pp/lib/python3.10/site-packages/django/db/utils.py\", line 148, in configure_settings\n",
      "    databases = super().configure_settings(databases)\n",
      "  File \"/Users/tahamajs/.local/share/virtualenvs/my_project_-rUTHF4pp/lib/python3.10/site-packages/django/utils/connection.py\", line 50, in configure_settings\n",
      "    settings = getattr(django_settings, self.settings_name)\n",
      "  File \"/Users/tahamajs/.local/share/virtualenvs/my_project_-rUTHF4pp/lib/python3.10/site-packages/django/conf/__init__.py\", line 102, in __getattr__\n",
      "    self._setup(name)\n",
      "  File \"/Users/tahamajs/.local/share/virtualenvs/my_project_-rUTHF4pp/lib/python3.10/site-packages/django/conf/__init__.py\", line 89, in _setup\n",
      "    self._wrapped = Settings(settings_module)\n",
      "  File \"/Users/tahamajs/.local/share/virtualenvs/my_project_-rUTHF4pp/lib/python3.10/site-packages/django/conf/__init__.py\", line 217, in __init__\n",
      "    mod = importlib.import_module(self.SETTINGS_MODULE)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/importlib/__init__.py\", line 126, in import_module\n",
      "    return _bootstrap._gcd_import(name[level:], package, level)\n",
      "  File \"<frozen importlib._bootstrap>\", line 1050, in _gcd_import\n",
      "  File \"<frozen importlib._bootstrap>\", line 1027, in _find_and_load\n",
      "  File \"<frozen importlib._bootstrap>\", line 1006, in _find_and_load_unlocked\n",
      "  File \"<frozen importlib._bootstrap>\", line 688, in _load_unlocked\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 883, in exec_module\n",
      "  File \"<frozen importlib._bootstrap>\", line 241, in _call_with_frames_removed\n",
      "  File \"/Users/tahamajs/Documents/uni/Research/ResearchAgent/V2/research-assistant/django_ui/django_ui/settings.py\", line 10, in <module>\n",
      "    import environ as django_environ\n",
      "ModuleNotFoundError: No module named 'environ'\n",
      "\n",
      "\n",
      "üéâ Setup Complete!\n",
      "==================================================\n",
      "Your AI Research Assistant is now fully configured with:\n",
      "‚úÖ Literature Search with AI ranking\n",
      "‚úÖ Podcast Generation with TTS\n",
      "‚úÖ Video Analysis with transcription\n",
      "‚úÖ Writing Assistance with AI feedback\n",
      "‚úÖ Academic Integrity checking\n",
      "‚úÖ Citation Management with AI parsing\n",
      "‚úÖ All Django apps and services\n",
      "‚úÖ Database migrations applied\n",
      "\n",
      "To start the server: python manage.py runserver\n"
     ]
    }
   ],
   "source": [
    "# Final Setup: Run Migrations and Test the Services\n",
    "\n",
    "import subprocess\n",
    "import sys\n",
    "import os\n",
    "\n",
    "django_dir = \"/Users/tahamajs/Documents/uni/Research/ResearchAgent/V2/research-assistant/django_ui\"\n",
    "os.chdir(django_dir)\n",
    "\n",
    "print(\"üîÑ Running Django migrations...\")\n",
    "\n",
    "try:\n",
    "    # Run makemigrations\n",
    "    result = subprocess.run([\n",
    "        sys.executable, \"manage.py\", \"makemigrations\"\n",
    "    ], capture_output=True, text=True)\n",
    "    \n",
    "    if result.returncode == 0:\n",
    "        print(\"‚úÖ Migrations created successfully\")\n",
    "        print(result.stdout)\n",
    "    else:\n",
    "        print(\"‚ùå Error creating migrations:\")\n",
    "        print(result.stderr)\n",
    "\n",
    "    # Run migrate\n",
    "    result = subprocess.run([\n",
    "        sys.executable, \"manage.py\", \"migrate\"\n",
    "    ], capture_output=True, text=True)\n",
    "    \n",
    "    if result.returncode == 0:\n",
    "        print(\"‚úÖ Migrations applied successfully\")\n",
    "        print(result.stdout)\n",
    "    else:\n",
    "        print(\"‚ùå Error applying migrations:\")\n",
    "        print(result.stderr)\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error running migrations: {e}\")\n",
    "\n",
    "print(\"\\nüéâ Setup Complete!\")\n",
    "print(\"=\" * 50)\n",
    "print(\"Your AI Research Assistant is now fully configured with:\")\n",
    "print(\"‚úÖ Literature Search with AI ranking\")\n",
    "print(\"‚úÖ Podcast Generation with TTS\")  \n",
    "print(\"‚úÖ Video Analysis with transcription\")\n",
    "print(\"‚úÖ Writing Assistance with AI feedback\")\n",
    "print(\"‚úÖ Academic Integrity checking\")\n",
    "print(\"‚úÖ Citation Management with AI parsing\")\n",
    "print(\"‚úÖ All Django apps and services\")\n",
    "print(\"‚úÖ Database migrations applied\")\n",
    "print(\"\\nTo start the server: python manage.py runserver\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a869ab5",
   "metadata": {},
   "source": [
    "# üéØ How to Use Your AI Research Assistant\n",
    "\n",
    "## üìö Literature Search API\n",
    "```python\n",
    "# POST /api/literature/search/\n",
    "{\n",
    "    \"query\": \"machine learning in healthcare\",\n",
    "    \"sources\": [\"arxiv\", \"semantic_scholar\", \"pubmed\"],\n",
    "    \"max_results\": 25,\n",
    "    \"filters\": {\n",
    "        \"date_from\": \"2020-01-01\",\n",
    "        \"date_to\": \"2024-12-31\"\n",
    "    }\n",
    "}\n",
    "```\n",
    "\n",
    "## üéôÔ∏è Podcast Generation API\n",
    "```python\n",
    "# POST /api/podcast/generate/\n",
    "{\n",
    "    \"paper_id\": \"uuid-of-paper\",\n",
    "    \"style\": \"summary\",  # summary, interview, debate, educational\n",
    "    \"voice\": \"alloy\",    # alloy, echo, fable, onyx, nova, shimmer\n",
    "    \"language\": \"en\"\n",
    "}\n",
    "```\n",
    "\n",
    "## üìπ Video Analysis API\n",
    "```python\n",
    "# POST /api/video/analyze/\n",
    "{\n",
    "    \"video_url\": \"https://youtube.com/watch?v=...\",\n",
    "    \"title\": \"Research Presentation Analysis\",\n",
    "    \"video_type\": \"lecture\",  # lecture, conference, seminar, interview\n",
    "    \"language\": \"en\"\n",
    "}\n",
    "```\n",
    "\n",
    "## ‚úçÔ∏è Writing Assistance API\n",
    "```python\n",
    "# POST /api/writing/assist/\n",
    "{\n",
    "    \"content\": \"Your research text here...\",\n",
    "    \"task_type\": \"abstract\",  # literature_review, abstract, introduction, etc.\n",
    "    \"tone\": \"academic\",      # academic, formal, technical, accessible\n",
    "    \"target_audience\": \"researchers\"\n",
    "}\n",
    "```\n",
    "\n",
    "## üîç Academic Integrity API\n",
    "```python\n",
    "# POST /api/integrity/check/\n",
    "{\n",
    "    \"content\": \"Your text to check...\",\n",
    "    \"title\": \"Integrity Check\",\n",
    "    \"check_type\": \"comprehensive\"  # plagiarism, citation, style, comprehensive\n",
    "}\n",
    "```\n",
    "\n",
    "## üìñ Citation Management API\n",
    "```python\n",
    "# POST /api/citation/generate/\n",
    "{\n",
    "    \"identifier\": \"10.1000/example\",\n",
    "    \"style\": \"APA\",  # APA, MLA, Chicago, Harvard, IEEE\n",
    "    \"identifier_type\": \"doi\"  # doi, arxiv, url, auto\n",
    "}\n",
    "```\n",
    "\n",
    "## üåü Key Features\n",
    "- **AI-Enhanced Search**: Query enhancement and intelligent ranking\n",
    "- **Multi-Style Podcasts**: Summary, interview, debate, and educational formats\n",
    "- **Comprehensive Video Analysis**: Transcription, sentiment, and timeline extraction\n",
    "- **Smart Writing Help**: Task-specific assistance with readability analysis\n",
    "- **Academic Integrity**: Plagiarism detection and citation verification\n",
    "- **Citation Automation**: Generate citations from DOIs, arXiv IDs, URLs\n",
    "\n",
    "Your research assistant is now ready to accelerate your academic workflow! üöÄ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cb1cc7a",
   "metadata": {},
   "source": [
    "# Django Database Setup and Migration Guide\n",
    "\n",
    "This notebook will help you set up the Django database properly and resolve the missing column errors.\n",
    "\n",
    "## Problem\n",
    "The error indicates that the Django database table `core_researchproject` is missing the `keywords` column. This happens when:\n",
    "1. Database migrations haven't been created\n",
    "2. Migrations haven't been applied to the database\n",
    "3. The model has been updated but migrations weren't run\n",
    "\n",
    "## Solution Steps\n",
    "We'll walk through each step to fix this issue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d49f249b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Set up Django project path\n",
    "project_root = \"/Users/tahamajs/Documents/uni/Research/ResearchAgent/V2/research-assistant\"\n",
    "django_project = os.path.join(project_root, \"django_ui\")\n",
    "\n",
    "print(f\"Django project path: {django_project}\")\n",
    "print(f\"Project exists: {os.path.exists(django_project)}\")\n",
    "\n",
    "# Check if manage.py exists\n",
    "manage_py = os.path.join(django_project, \"manage.py\")\n",
    "print(f\"manage.py exists: {os.path.exists(manage_py)}\")\n",
    "\n",
    "# Check core app\n",
    "core_app = os.path.join(django_project, \"core\")\n",
    "print(f\"Core app exists: {os.path.exists(core_app)}\")\n",
    "\n",
    "# List django_ui directory contents\n",
    "if os.path.exists(django_project):\n",
    "    print(f\"\\nContents of {django_project}:\")\n",
    "    for item in os.listdir(django_project):\n",
    "        print(f\"  {item}\")\n",
    "        \n",
    "# Check core app contents\n",
    "if os.path.exists(core_app):\n",
    "    print(f\"\\nContents of core app:\")\n",
    "    for item in os.listdir(core_app):\n",
    "        print(f\"  {item}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cb07702",
   "metadata": {},
   "source": [
    "# Research Agent Backend - Comprehensive Implementation\n",
    "\n",
    "This notebook contains a complete implementation of a research agent backend with all the core, advanced, and cutting-edge functionalities for academic research assistance.\n",
    "\n",
    "## üèóÔ∏è Architecture Overview\n",
    "\n",
    "The system is designed with a modular architecture:\n",
    "\n",
    "1. **Core Services**: Literature search, researcher discovery, paper analysis\n",
    "2. **Advanced Services**: Research gap finder, writing assistant, citation generator\n",
    "3. **Cutting-Edge Services**: Agent-based exploration, research co-pilot, funding discovery\n",
    "4. **Data Layer**: Database models and storage\n",
    "5. **API Layer**: RESTful endpoints for frontend integration\n",
    "6. **Integration Layer**: External service connectors\n",
    "\n",
    "## üì¶ Required Dependencies\n",
    "\n",
    "First, let's install all required packages for our research agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe02eba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "packages = [\n",
    "    \"fastapi\",\n",
    "    \"uvicorn\",\n",
    "    \"sqlalchemy\",\n",
    "    \"alembic\",\n",
    "    \"pydantic\",\n",
    "    \"requests\",\n",
    "    \"beautifulsoup4\",\n",
    "    \"scrapy\",\n",
    "    \"pandas\",\n",
    "    \"numpy\",\n",
    "    \"scikit-learn\",\n",
    "    \"nltk\",\n",
    "    \"spacy\",\n",
    "    \"transformers\",\n",
    "    \"torch\",\n",
    "    \"openai\",\n",
    "    \"arxiv\",\n",
    "    \"scholarly\",\n",
    "    \"bibtexparser\",\n",
    "    \"python-multipart\",\n",
    "    \"aiofiles\",\n",
    "    \"celery\",\n",
    "    \"redis\",\n",
    "    \"pymongo\",\n",
    "    \"elasticsearch\",\n",
    "    \"matplotlib\",\n",
    "    \"plotly\",\n",
    "    \"networkx\",\n",
    "    \"python-dateutil\",\n",
    "    \"pydantic-settings\",\n",
    "    \"python-jose[cryptography]\",\n",
    "    \"passlib[bcrypt]\",\n",
    "    \"python-dotenv\",\n",
    "    \"httpx\",\n",
    "    \"asyncio\",\n",
    "    \"aiohttp\",\n",
    "    \"textblob\",\n",
    "    \"wordcloud\",\n",
    "    \"seaborn\"\n",
    "]\n",
    "\n",
    "def install_packages(packages):\n",
    "    for package in packages:\n",
    "        try:\n",
    "            subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package])\n",
    "            print(f\"‚úÖ Successfully installed {package}\")\n",
    "        except subprocess.CalledProcessError:\n",
    "            print(f\"‚ùå Failed to install {package}\")\n",
    "\n",
    "# Uncomment the line below to install packages\n",
    "# install_packages(packages)\n",
    "\n",
    "print(\"üì¶ Package installation completed!\")\n",
    "print(\"Note: Uncomment the install_packages() call above to actually install the packages.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf522324",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core Imports and Configuration\n",
    "import os\n",
    "import asyncio\n",
    "import json\n",
    "import logging\n",
    "from datetime import datetime, timedelta\n",
    "from typing import List, Dict, Optional, Any, Union\n",
    "from dataclasses import dataclass, asdict\n",
    "from enum import Enum\n",
    "import uuid\n",
    "\n",
    "# FastAPI and Web Framework\n",
    "from fastapi import FastAPI, HTTPException, Depends, BackgroundTasks, UploadFile, File\n",
    "from fastapi.middleware.cors import CORSMiddleware\n",
    "from pydantic import BaseModel, Field\n",
    "from pydantic_settings import BaseSettings\n",
    "\n",
    "# Database and Storage\n",
    "from sqlalchemy import create_engine, Column, Integer, String, DateTime, Text, Boolean, Float, ForeignKey\n",
    "from sqlalchemy.ext.declarative import declarative_base\n",
    "from sqlalchemy.orm import sessionmaker, Session, relationship\n",
    "from sqlalchemy.dialects.postgresql import UUID\n",
    "\n",
    "# Data Processing\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "import nltk\n",
    "import spacy\n",
    "\n",
    "# External API Clients\n",
    "import requests\n",
    "import arxiv\n",
    "from scholarly import scholarly\n",
    "import bibtexparser\n",
    "\n",
    "# Async and Networking\n",
    "import aiohttp\n",
    "import httpx\n",
    "\n",
    "# Utilities\n",
    "import re\n",
    "from collections import defaultdict, Counter\n",
    "from urllib.parse import urlparse, urljoin\n",
    "import hashlib\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "print(\"‚úÖ All imports loaded successfully!\")\n",
    "print(\"üöÄ Research Agent Backend Core System Ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "815b737d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration Settings\n",
    "class Settings(BaseSettings):\n",
    "    \"\"\"Application configuration settings\"\"\"\n",
    "    \n",
    "    # Database\n",
    "    database_url: str = \"sqlite:///./research_agent.db\"\n",
    "    \n",
    "    # API Keys (set these in environment variables)\n",
    "    openai_api_key: str = \"\"\n",
    "    semantic_scholar_api_key: str = \"\"\n",
    "    crossref_email: str = \"\"\n",
    "    \n",
    "    # External Services\n",
    "    elasticsearch_url: str = \"http://localhost:9200\"\n",
    "    redis_url: str = \"redis://localhost:6379\"\n",
    "    \n",
    "    # Application\n",
    "    secret_key: str = \"your-secret-key-here\"\n",
    "    algorithm: str = \"HS256\"\n",
    "    access_token_expire_minutes: int = 30\n",
    "    \n",
    "    # Rate Limiting\n",
    "    max_requests_per_minute: int = 60\n",
    "    \n",
    "    class Config:\n",
    "        env_file = \".env\"\n",
    "\n",
    "settings = Settings()\n",
    "\n",
    "# Database Setup\n",
    "Base = declarative_base()\n",
    "\n",
    "class Paper(Base):\n",
    "    \"\"\"Database model for research papers\"\"\"\n",
    "    __tablename__ = \"papers\"\n",
    "    \n",
    "    id = Column(Integer, primary_key=True, index=True)\n",
    "    title = Column(String, nullable=False, index=True)\n",
    "    abstract = Column(Text)\n",
    "    authors = Column(Text)  # JSON string of authors\n",
    "    doi = Column(String, unique=True, index=True)\n",
    "    arxiv_id = Column(String, unique=True, index=True)\n",
    "    publication_date = Column(DateTime)\n",
    "    journal = Column(String)\n",
    "    keywords = Column(Text)  # JSON string of keywords\n",
    "    citation_count = Column(Integer, default=0)\n",
    "    pdf_url = Column(String)\n",
    "    summary = Column(Text)\n",
    "    contributions = Column(Text)  # JSON string of key contributions\n",
    "    methods = Column(Text)\n",
    "    results = Column(Text)\n",
    "    created_at = Column(DateTime, default=datetime.utcnow)\n",
    "    updated_at = Column(DateTime, default=datetime.utcnow, onupdate=datetime.utcnow)\n",
    "\n",
    "class Researcher(Base):\n",
    "    \"\"\"Database model for researchers\"\"\"\n",
    "    __tablename__ = \"researchers\"\n",
    "    \n",
    "    id = Column(Integer, primary_key=True, index=True)\n",
    "    name = Column(String, nullable=False, index=True)\n",
    "    affiliation = Column(String)\n",
    "    email = Column(String)\n",
    "    google_scholar_id = Column(String, unique=True)\n",
    "    orcid = Column(String, unique=True)\n",
    "    h_index = Column(Integer, default=0)\n",
    "    citation_count = Column(Integer, default=0)\n",
    "    research_interests = Column(Text)  # JSON string\n",
    "    recent_papers = Column(Text)  # JSON string of paper IDs\n",
    "    collaboration_network = Column(Text)  # JSON string\n",
    "    created_at = Column(DateTime, default=datetime.utcnow)\n",
    "    updated_at = Column(DateTime, default=datetime.utcnow, onupdate=datetime.utcnow)\n",
    "\n",
    "class UserLibrary(Base):\n",
    "    \"\"\"Database model for user's personal paper library\"\"\"\n",
    "    __tablename__ = \"user_libraries\"\n",
    "    \n",
    "    id = Column(Integer, primary_key=True, index=True)\n",
    "    user_id = Column(String, nullable=False, index=True)\n",
    "    paper_id = Column(Integer, ForeignKey(\"papers.id\"))\n",
    "    tags = Column(Text)  # JSON string of user tags\n",
    "    notes = Column(Text)\n",
    "    reading_status = Column(String, default=\"to_read\")  # to_read, reading, completed\n",
    "    rating = Column(Integer)  # 1-5 stars\n",
    "    added_at = Column(DateTime, default=datetime.utcnow)\n",
    "    \n",
    "    paper = relationship(\"Paper\")\n",
    "\n",
    "class ResearchProject(Base):\n",
    "    \"\"\"Database model for research projects\"\"\"\n",
    "    __tablename__ = \"research_projects\"\n",
    "    \n",
    "    id = Column(Integer, primary_key=True, index=True)\n",
    "    user_id = Column(String, nullable=False, index=True)\n",
    "    title = Column(String, nullable=False)\n",
    "    description = Column(Text)\n",
    "    research_questions = Column(Text)  # JSON string\n",
    "    methodology = Column(Text)\n",
    "    status = Column(String, default=\"planning\")  # planning, active, completed\n",
    "    related_papers = Column(Text)  # JSON string of paper IDs\n",
    "    created_at = Column(DateTime, default=datetime.utcnow)\n",
    "    updated_at = Column(DateTime, default=datetime.utcnow, onupdate=datetime.utcnow)\n",
    "\n",
    "class Citation(Base):\n",
    "    \"\"\"Database model for paper citations\"\"\"\n",
    "    __tablename__ = \"citations\"\n",
    "    \n",
    "    id = Column(Integer, primary_key=True, index=True)\n",
    "    citing_paper_id = Column(Integer, ForeignKey(\"papers.id\"))\n",
    "    cited_paper_id = Column(Integer, ForeignKey(\"papers.id\"))\n",
    "    context = Column(Text)  # The context in which the citation appears\n",
    "    \n",
    "    citing_paper = relationship(\"Paper\", foreign_keys=[citing_paper_id])\n",
    "    cited_paper = relationship(\"Paper\", foreign_keys=[cited_paper_id])\n",
    "\n",
    "# Create database engine\n",
    "engine = create_engine(settings.database_url)\n",
    "SessionLocal = sessionmaker(autocommit=False, autoflush=False, bind=engine)\n",
    "\n",
    "# Create tables\n",
    "Base.metadata.create_all(bind=engine)\n",
    "\n",
    "print(\"üóÑÔ∏è Database models created successfully!\")\n",
    "print(\"üìä Tables: Papers, Researchers, UserLibrary, ResearchProject, Citation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5fbacc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pydantic Models for API Requests and Responses\n",
    "\n",
    "class PaperBase(BaseModel):\n",
    "    \"\"\"Base model for paper data\"\"\"\n",
    "    title: str\n",
    "    abstract: Optional[str] = None\n",
    "    authors: List[str] = []\n",
    "    doi: Optional[str] = None\n",
    "    arxiv_id: Optional[str] = None\n",
    "    publication_date: Optional[datetime] = None\n",
    "    journal: Optional[str] = None\n",
    "    keywords: List[str] = []\n",
    "    citation_count: int = 0\n",
    "    pdf_url: Optional[str] = None\n",
    "\n",
    "class PaperCreate(PaperBase):\n",
    "    \"\"\"Model for creating a new paper\"\"\"\n",
    "    pass\n",
    "\n",
    "class PaperResponse(PaperBase):\n",
    "    \"\"\"Model for paper API responses\"\"\"\n",
    "    id: int\n",
    "    summary: Optional[str] = None\n",
    "    contributions: List[str] = []\n",
    "    methods: Optional[str] = None\n",
    "    results: Optional[str] = None\n",
    "    created_at: datetime\n",
    "    \n",
    "    class Config:\n",
    "        from_attributes = True\n",
    "\n",
    "class ResearcherBase(BaseModel):\n",
    "    \"\"\"Base model for researcher data\"\"\"\n",
    "    name: str\n",
    "    affiliation: Optional[str] = None\n",
    "    email: Optional[str] = None\n",
    "    google_scholar_id: Optional[str] = None\n",
    "    orcid: Optional[str] = None\n",
    "    h_index: int = 0\n",
    "    citation_count: int = 0\n",
    "    research_interests: List[str] = []\n",
    "\n",
    "class ResearcherResponse(ResearcherBase):\n",
    "    \"\"\"Model for researcher API responses\"\"\"\n",
    "    id: int\n",
    "    recent_papers: List[int] = []\n",
    "    collaboration_network: Dict[str, Any] = {}\n",
    "    created_at: datetime\n",
    "    \n",
    "    class Config:\n",
    "        from_attributes = True\n",
    "\n",
    "class SearchQuery(BaseModel):\n",
    "    \"\"\"Model for search queries\"\"\"\n",
    "    query: str\n",
    "    filters: Dict[str, Any] = {}\n",
    "    sort_by: str = \"relevance\"\n",
    "    limit: int = 20\n",
    "    offset: int = 0\n",
    "\n",
    "class SearchResults(BaseModel):\n",
    "    \"\"\"Model for search results\"\"\"\n",
    "    query: str\n",
    "    total_results: int\n",
    "    results: List[PaperResponse]\n",
    "    facets: Dict[str, Any] = {}\n",
    "    took_ms: int\n",
    "\n",
    "class ResearchGap(BaseModel):\n",
    "    \"\"\"Model for research gap identification\"\"\"\n",
    "    topic: str\n",
    "    gap_description: str\n",
    "    confidence_score: float\n",
    "    supporting_evidence: List[str]\n",
    "    suggested_research_directions: List[str]\n",
    "\n",
    "class TrendAnalysis(BaseModel):\n",
    "    \"\"\"Model for trend analysis results\"\"\"\n",
    "    time_period: str\n",
    "    trending_topics: List[Dict[str, Any]]\n",
    "    rising_researchers: List[ResearcherResponse]\n",
    "    hot_papers: List[PaperResponse]\n",
    "    keyword_trends: Dict[str, List[float]]\n",
    "\n",
    "class WritingAssistance(BaseModel):\n",
    "    \"\"\"Model for writing assistance requests\"\"\"\n",
    "    text: str\n",
    "    assistance_type: str  # \"grammar\", \"style\", \"citations\", \"structure\"\n",
    "    context: Optional[str] = None\n",
    "\n",
    "class WritingFeedback(BaseModel):\n",
    "    \"\"\"Model for writing feedback\"\"\"\n",
    "    original_text: str\n",
    "    suggestions: List[Dict[str, str]]\n",
    "    improved_text: Optional[str] = None\n",
    "    confidence_score: float\n",
    "\n",
    "print(\"üìã Pydantic models created successfully!\")\n",
    "print(\"üîß API request/response models ready for FastAPI integration\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cb9a48c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core Service 1: Smart Literature Search\n",
    "class LiteratureSearchService:\n",
    "    \"\"\"Service for intelligent literature search across multiple academic databases\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.session = requests.Session()\n",
    "        self.session.headers.update({\n",
    "            'User-Agent': 'ResearchAgent/1.0 (mailto:research@example.com)'\n",
    "        })\n",
    "    \n",
    "    async def search_arxiv(self, query: str, max_results: int = 20) -> List[Dict]:\n",
    "        \"\"\"Search arXiv database\"\"\"\n",
    "        try:\n",
    "            search = arxiv.Search(\n",
    "                query=query,\n",
    "                max_results=max_results,\n",
    "                sort_by=arxiv.SortCriterion.Relevance\n",
    "            )\n",
    "            \n",
    "            results = []\n",
    "            for result in search.results():\n",
    "                paper_data = {\n",
    "                    \"title\": result.title,\n",
    "                    \"abstract\": result.summary,\n",
    "                    \"authors\": [str(author) for author in result.authors],\n",
    "                    \"doi\": result.doi,\n",
    "                    \"arxiv_id\": result.entry_id.split('/')[-1],\n",
    "                    \"publication_date\": result.published,\n",
    "                    \"pdf_url\": result.pdf_url,\n",
    "                    \"categories\": result.categories,\n",
    "                    \"source\": \"arxiv\"\n",
    "                }\n",
    "                results.append(paper_data)\n",
    "            \n",
    "            return results\n",
    "        except Exception as e:\n",
    "            logger.error(f\"ArXiv search error: {str(e)}\")\n",
    "            return []\n",
    "    \n",
    "    async def search_semantic_scholar(self, query: str, max_results: int = 20) -> List[Dict]:\n",
    "        \"\"\"Search Semantic Scholar API\"\"\"\n",
    "        try:\n",
    "            url = \"https://api.semanticscholar.org/graph/v1/paper/search\"\n",
    "            params = {\n",
    "                \"query\": query,\n",
    "                \"limit\": max_results,\n",
    "                \"fields\": \"title,abstract,authors,citationCount,publicationDate,doi,url,venue\"\n",
    "            }\n",
    "            \n",
    "            headers = {}\n",
    "            if settings.semantic_scholar_api_key:\n",
    "                headers[\"x-api-key\"] = settings.semantic_scholar_api_key\n",
    "            \n",
    "            async with aiohttp.ClientSession() as session:\n",
    "                async with session.get(url, params=params, headers=headers) as response:\n",
    "                    if response.status == 200:\n",
    "                        data = await response.json()\n",
    "                        results = []\n",
    "                        \n",
    "                        for paper in data.get(\"data\", []):\n",
    "                            paper_data = {\n",
    "                                \"title\": paper.get(\"title\", \"\"),\n",
    "                                \"abstract\": paper.get(\"abstract\", \"\"),\n",
    "                                \"authors\": [author.get(\"name\", \"\") for author in paper.get(\"authors\", [])],\n",
    "                                \"doi\": paper.get(\"doi\"),\n",
    "                                \"publication_date\": paper.get(\"publicationDate\"),\n",
    "                                \"citation_count\": paper.get(\"citationCount\", 0),\n",
    "                                \"journal\": paper.get(\"venue\", {}).get(\"name\") if paper.get(\"venue\") else None,\n",
    "                                \"url\": paper.get(\"url\"),\n",
    "                                \"source\": \"semantic_scholar\"\n",
    "                            }\n",
    "                            results.append(paper_data)\n",
    "                        \n",
    "                        return results\n",
    "                    else:\n",
    "                        logger.error(f\"Semantic Scholar API error: {response.status}\")\n",
    "                        return []\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Semantic Scholar search error: {str(e)}\")\n",
    "            return []\n",
    "    \n",
    "    async def search_google_scholar(self, query: str, max_results: int = 20) -> List[Dict]:\n",
    "        \"\"\"Search Google Scholar using scholarly library\"\"\"\n",
    "        try:\n",
    "            search_query = scholarly.search_pubs(query)\n",
    "            results = []\n",
    "            \n",
    "            for i, pub in enumerate(search_query):\n",
    "                if i >= max_results:\n",
    "                    break\n",
    "                \n",
    "                paper_data = {\n",
    "                    \"title\": pub.get('bib', {}).get('title', ''),\n",
    "                    \"abstract\": pub.get('bib', {}).get('abstract', ''),\n",
    "                    \"authors\": pub.get('bib', {}).get('author', []),\n",
    "                    \"publication_date\": pub.get('bib', {}).get('pub_year'),\n",
    "                    \"journal\": pub.get('bib', {}).get('venue', ''),\n",
    "                    \"citation_count\": pub.get('num_citations', 0),\n",
    "                    \"url\": pub.get('pub_url'),\n",
    "                    \"source\": \"google_scholar\"\n",
    "                }\n",
    "                results.append(paper_data)\n",
    "            \n",
    "            return results\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Google Scholar search error: {str(e)}\")\n",
    "            return []\n",
    "    \n",
    "    async def unified_search(self, query: str, sources: List[str] = None, max_results: int = 50) -> List[Dict]:\n",
    "        \"\"\"Perform unified search across multiple sources\"\"\"\n",
    "        if sources is None:\n",
    "            sources = [\"arxiv\", \"semantic_scholar\", \"google_scholar\"]\n",
    "        \n",
    "        all_results = []\n",
    "        tasks = []\n",
    "        \n",
    "        if \"arxiv\" in sources:\n",
    "            tasks.append(self.search_arxiv(query, max_results // len(sources)))\n",
    "        if \"semantic_scholar\" in sources:\n",
    "            tasks.append(self.search_semantic_scholar(query, max_results // len(sources)))\n",
    "        if \"google_scholar\" in sources:\n",
    "            tasks.append(self.search_google_scholar(query, max_results // len(sources)))\n",
    "        \n",
    "        results = await asyncio.gather(*tasks, return_exceptions=True)\n",
    "        \n",
    "        for result in results:\n",
    "            if isinstance(result, list):\n",
    "                all_results.extend(result)\n",
    "        \n",
    "        # Remove duplicates based on title similarity\n",
    "        unique_results = self._remove_duplicates(all_results)\n",
    "        \n",
    "        # Sort by relevance/citation count\n",
    "        unique_results.sort(key=lambda x: x.get('citation_count', 0), reverse=True)\n",
    "        \n",
    "        return unique_results[:max_results]\n",
    "    \n",
    "    def _remove_duplicates(self, papers: List[Dict]) -> List[Dict]:\n",
    "        \"\"\"Remove duplicate papers based on title similarity\"\"\"\n",
    "        unique_papers = []\n",
    "        seen_titles = set()\n",
    "        \n",
    "        for paper in papers:\n",
    "            title = paper.get('title', '').lower().strip()\n",
    "            if title and title not in seen_titles:\n",
    "                seen_titles.add(title)\n",
    "                unique_papers.append(paper)\n",
    "        \n",
    "        return unique_papers\n",
    "    \n",
    "    async def search_with_filters(self, query: str, filters: Dict[str, Any]) -> List[Dict]:\n",
    "        \"\"\"Search with advanced filters\"\"\"\n",
    "        results = await self.unified_search(query)\n",
    "        \n",
    "        # Apply filters\n",
    "        filtered_results = []\n",
    "        for paper in results:\n",
    "            if self._matches_filters(paper, filters):\n",
    "                filtered_results.append(paper)\n",
    "        \n",
    "        return filtered_results\n",
    "    \n",
    "    def _matches_filters(self, paper: Dict, filters: Dict[str, Any]) -> bool:\n",
    "        \"\"\"Check if paper matches the given filters\"\"\"\n",
    "        # Year filter\n",
    "        if \"year_from\" in filters or \"year_to\" in filters:\n",
    "            pub_date = paper.get(\"publication_date\")\n",
    "            if pub_date:\n",
    "                try:\n",
    "                    if isinstance(pub_date, str):\n",
    "                        year = int(pub_date.split('-')[0])\n",
    "                    elif hasattr(pub_date, 'year'):\n",
    "                        year = pub_date.year\n",
    "                    else:\n",
    "                        year = int(pub_date)\n",
    "                    \n",
    "                    if \"year_from\" in filters and year < filters[\"year_from\"]:\n",
    "                        return False\n",
    "                    if \"year_to\" in filters and year > filters[\"year_to\"]:\n",
    "                        return False\n",
    "                except:\n",
    "                    pass\n",
    "        \n",
    "        # Author filter\n",
    "        if \"author\" in filters:\n",
    "            authors = paper.get(\"authors\", [])\n",
    "            author_names = \" \".join(authors).lower()\n",
    "            if filters[\"author\"].lower() not in author_names:\n",
    "                return False\n",
    "        \n",
    "        # Journal filter\n",
    "        if \"journal\" in filters:\n",
    "            journal = paper.get(\"journal\", \"\").lower()\n",
    "            if filters[\"journal\"].lower() not in journal:\n",
    "                return False\n",
    "        \n",
    "        # Minimum citation count\n",
    "        if \"min_citations\" in filters:\n",
    "            citations = paper.get(\"citation_count\", 0)\n",
    "            if citations < filters[\"min_citations\"]:\n",
    "                return False\n",
    "        \n",
    "        return True\n",
    "\n",
    "# Initialize literature search service\n",
    "literature_search = LiteratureSearchService()\n",
    "\n",
    "print(\"üîç Literature Search Service initialized!\")\n",
    "print(\"üìö Supports: arXiv, Semantic Scholar, Google Scholar\")\n",
    "print(\"üéØ Features: Unified search, duplicate removal, advanced filtering\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09b9752d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core Service 2: Researcher Discovery Service\n",
    "class ResearcherDiscoveryService:\n",
    "    \"\"\"Service for discovering and analyzing researchers in specific fields\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.session = aiohttp.ClientSession()\n",
    "    \n",
    "    async def search_researchers_by_topic(self, topic: str, max_results: int = 20) -> List[Dict]:\n",
    "        \"\"\"Find top researchers in a given field\"\"\"\n",
    "        try:\n",
    "            # Use Google Scholar to find researchers\n",
    "            search_query = scholarly.search_author(topic)\n",
    "            researchers = []\n",
    "            \n",
    "            for i, author in enumerate(search_query):\n",
    "                if i >= max_results:\n",
    "                    break\n",
    "                \n",
    "                # Fill author details\n",
    "                author_filled = scholarly.fill(author)\n",
    "                \n",
    "                researcher_data = {\n",
    "                    \"name\": author_filled.get(\"name\", \"\"),\n",
    "                    \"affiliation\": author_filled.get(\"affiliation\", \"\"),\n",
    "                    \"email\": author_filled.get(\"email\", \"\"),\n",
    "                    \"google_scholar_id\": author_filled.get(\"scholar_id\", \"\"),\n",
    "                    \"h_index\": author_filled.get(\"hindex\", 0),\n",
    "                    \"citation_count\": author_filled.get(\"citedby\", 0),\n",
    "                    \"research_interests\": author_filled.get(\"interests\", []),\n",
    "                    \"recent_papers\": [],\n",
    "                    \"homepage\": author_filled.get(\"homepage\", \"\")\n",
    "                }\n",
    "                \n",
    "                # Get recent publications\n",
    "                publications = author_filled.get(\"publications\", [])[:10]\n",
    "                for pub in publications:\n",
    "                    try:\n",
    "                        pub_filled = scholarly.fill(pub)\n",
    "                        paper_info = {\n",
    "                            \"title\": pub_filled.get(\"bib\", {}).get(\"title\", \"\"),\n",
    "                            \"year\": pub_filled.get(\"bib\", {}).get(\"pub_year\", \"\"),\n",
    "                            \"citations\": pub_filled.get(\"num_citations\", 0),\n",
    "                            \"venue\": pub_filled.get(\"bib\", {}).get(\"venue\", \"\")\n",
    "                        }\n",
    "                        researcher_data[\"recent_papers\"].append(paper_info)\n",
    "                    except:\n",
    "                        continue\n",
    "                \n",
    "                researchers.append(researcher_data)\n",
    "            \n",
    "            return researchers\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Researcher search error: {str(e)}\")\n",
    "            return []\n",
    "    \n",
    "    async def get_researcher_profile(self, researcher_id: str, source: str = \"google_scholar\") -> Dict:\n",
    "        \"\"\"Get detailed profile of a specific researcher\"\"\"\n",
    "        try:\n",
    "            if source == \"google_scholar\":\n",
    "                author = scholarly.search_author_id(researcher_id)\n",
    "                author_filled = scholarly.fill(author)\n",
    "                \n",
    "                profile = {\n",
    "                    \"name\": author_filled.get(\"name\", \"\"),\n",
    "                    \"affiliation\": author_filled.get(\"affiliation\", \"\"),\n",
    "                    \"email\": author_filled.get(\"email\", \"\"),\n",
    "                    \"google_scholar_id\": researcher_id,\n",
    "                    \"h_index\": author_filled.get(\"hindex\", 0),\n",
    "                    \"h_index_5y\": author_filled.get(\"hindex5y\", 0),\n",
    "                    \"i10_index\": author_filled.get(\"i10index\", 0),\n",
    "                    \"i10_index_5y\": author_filled.get(\"i10index5y\", 0),\n",
    "                    \"citation_count\": author_filled.get(\"citedby\", 0),\n",
    "                    \"citation_count_5y\": author_filled.get(\"citedby5y\", 0),\n",
    "                    \"research_interests\": author_filled.get(\"interests\", []),\n",
    "                    \"homepage\": author_filled.get(\"homepage\", \"\"),\n",
    "                    \"publications\": [],\n",
    "                    \"coauthors\": []\n",
    "                }\n",
    "                \n",
    "                # Get publications with details\n",
    "                publications = author_filled.get(\"publications\", [])\n",
    "                for pub in publications:\n",
    "                    try:\n",
    "                        pub_filled = scholarly.fill(pub)\n",
    "                        paper_info = {\n",
    "                            \"title\": pub_filled.get(\"bib\", {}).get(\"title\", \"\"),\n",
    "                            \"year\": pub_filled.get(\"bib\", {}).get(\"pub_year\", \"\"),\n",
    "                            \"citations\": pub_filled.get(\"num_citations\", 0),\n",
    "                            \"venue\": pub_filled.get(\"bib\", {}).get(\"venue\", \"\"),\n",
    "                            \"authors\": pub_filled.get(\"bib\", {}).get(\"author\", []),\n",
    "                            \"abstract\": pub_filled.get(\"bib\", {}).get(\"abstract\", \"\")\n",
    "                        }\n",
    "                        profile[\"publications\"].append(paper_info)\n",
    "                    except:\n",
    "                        continue\n",
    "                \n",
    "                # Extract coauthors\n",
    "                profile[\"coauthors\"] = self._extract_coauthors(profile[\"publications\"], profile[\"name\"])\n",
    "                \n",
    "                return profile\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Profile retrieval error: {str(e)}\")\n",
    "            return {}\n",
    "    \n",
    "    def _extract_coauthors(self, publications: List[Dict], researcher_name: str) -> List[Dict]:\n",
    "        \"\"\"Extract coauthors from publications\"\"\"\n",
    "        coauthor_counts = defaultdict(int)\n",
    "        \n",
    "        for paper in publications:\n",
    "            authors = paper.get(\"authors\", [])\n",
    "            for author in authors:\n",
    "                if author != researcher_name:\n",
    "                    coauthor_counts[author] += 1\n",
    "        \n",
    "        # Sort by collaboration frequency\n",
    "        coauthors = [\n",
    "            {\"name\": author, \"collaboration_count\": count}\n",
    "            for author, count in sorted(coauthor_counts.items(), key=lambda x: x[1], reverse=True)\n",
    "        ]\n",
    "        \n",
    "        return coauthors[:20]  # Top 20 collaborators\n",
    "    \n",
    "    async def find_trending_researchers(self, field: str, time_period: str = \"recent\") -> List[Dict]:\n",
    "        \"\"\"Find trending/rising researchers in a field\"\"\"\n",
    "        researchers = await self.search_researchers_by_topic(field, max_results=50)\n",
    "        \n",
    "        # Score researchers based on recent activity and citation growth\n",
    "        scored_researchers = []\n",
    "        for researcher in researchers:\n",
    "            score = self._calculate_trending_score(researcher, time_period)\n",
    "            researcher[\"trending_score\"] = score\n",
    "            scored_researchers.append(researcher)\n",
    "        \n",
    "        # Sort by trending score\n",
    "        scored_researchers.sort(key=lambda x: x[\"trending_score\"], reverse=True)\n",
    "        \n",
    "        return scored_researchers[:20]\n",
    "    \n",
    "    def _calculate_trending_score(self, researcher: Dict, time_period: str) -> float:\n",
    "        \"\"\"Calculate trending score for a researcher\"\"\"\n",
    "        h_index = researcher.get(\"h_index\", 0)\n",
    "        citation_count = researcher.get(\"citation_count\", 0)\n",
    "        recent_papers = researcher.get(\"recent_papers\", [])\n",
    "        \n",
    "        # Basic score from h-index and citations\n",
    "        base_score = (h_index * 0.3) + (citation_count * 0.0001)\n",
    "        \n",
    "        # Boost for recent publications\n",
    "        recent_publications_count = sum(1 for paper in recent_papers \n",
    "                                      if self._is_recent_publication(paper, time_period))\n",
    "        recent_boost = recent_publications_count * 0.2\n",
    "        \n",
    "        # Boost for high-impact recent papers\n",
    "        high_impact_boost = sum(0.1 for paper in recent_papers \n",
    "                              if paper.get(\"citations\", 0) > 50 and \n",
    "                              self._is_recent_publication(paper, time_period))\n",
    "        \n",
    "        return base_score + recent_boost + high_impact_boost\n",
    "    \n",
    "    def _is_recent_publication(self, paper: Dict, time_period: str) -> bool:\n",
    "        \"\"\"Check if publication is recent based on time period\"\"\"\n",
    "        current_year = datetime.now().year\n",
    "        paper_year = paper.get(\"year\")\n",
    "        \n",
    "        if not paper_year:\n",
    "            return False\n",
    "        \n",
    "        try:\n",
    "            paper_year = int(paper_year)\n",
    "            if time_period == \"recent\":\n",
    "                return paper_year >= (current_year - 2)\n",
    "            elif time_period == \"last_5_years\":\n",
    "                return paper_year >= (current_year - 5)\n",
    "            else:\n",
    "                return paper_year >= (current_year - 1)\n",
    "        except:\n",
    "            return False\n",
    "    \n",
    "    async def build_collaboration_network(self, researcher_ids: List[str]) -> Dict:\n",
    "        \"\"\"Build collaboration network for a set of researchers\"\"\"\n",
    "        network = {\n",
    "            \"nodes\": [],\n",
    "            \"edges\": []\n",
    "        }\n",
    "        \n",
    "        researcher_profiles = {}\n",
    "        \n",
    "        # Get profiles for all researchers\n",
    "        for researcher_id in researcher_ids:\n",
    "            profile = await self.get_researcher_profile(researcher_id)\n",
    "            if profile:\n",
    "                researcher_profiles[researcher_id] = profile\n",
    "                network[\"nodes\"].append({\n",
    "                    \"id\": researcher_id,\n",
    "                    \"name\": profile[\"name\"],\n",
    "                    \"h_index\": profile.get(\"h_index\", 0),\n",
    "                    \"citation_count\": profile.get(\"citation_count\", 0),\n",
    "                    \"affiliation\": profile.get(\"affiliation\", \"\")\n",
    "                })\n",
    "        \n",
    "        # Build edges based on collaborations\n",
    "        for researcher_id, profile in researcher_profiles.items():\n",
    "            coauthors = profile.get(\"coauthors\", [])\n",
    "            for coauthor in coauthors:\n",
    "                # Find if coauthor is in our researcher set\n",
    "                coauthor_id = self._find_researcher_id_by_name(coauthor[\"name\"], researcher_profiles)\n",
    "                if coauthor_id and coauthor_id != researcher_id:\n",
    "                    network[\"edges\"].append({\n",
    "                        \"source\": researcher_id,\n",
    "                        \"target\": coauthor_id,\n",
    "                        \"weight\": coauthor[\"collaboration_count\"]\n",
    "                    })\n",
    "        \n",
    "        return network\n",
    "    \n",
    "    def _find_researcher_id_by_name(self, name: str, researcher_profiles: Dict) -> Optional[str]:\n",
    "        \"\"\"Find researcher ID by name\"\"\"\n",
    "        for researcher_id, profile in researcher_profiles.items():\n",
    "            if profile[\"name\"].lower() == name.lower():\n",
    "                return researcher_id\n",
    "        return None\n",
    "\n",
    "# Initialize researcher discovery service\n",
    "researcher_discovery = ResearcherDiscoveryService()\n",
    "\n",
    "print(\"üë©‚Äçüî¨ Researcher Discovery Service initialized!\")\n",
    "print(\"üéØ Features: Topic-based search, trending researchers, collaboration networks\")\n",
    "print(\"üìä Metrics: H-index, citations, recent activity scoring\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af5e36f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core Service 3: Paper Summarization and Analysis Service\n",
    "class PaperAnalysisService:\n",
    "    \"\"\"Service for analyzing and summarizing research papers\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        # Initialize NLP models\n",
    "        try:\n",
    "            import spacy\n",
    "            self.nlp = spacy.load(\"en_core_web_sm\")\n",
    "        except OSError:\n",
    "            logger.warning(\"SpaCy model not found. Install with: python -m spacy download en_core_web_sm\")\n",
    "            self.nlp = None\n",
    "        \n",
    "        # Initialize sentence transformers for embeddings (optional)\n",
    "        try:\n",
    "            from sentence_transformers import SentenceTransformer\n",
    "            self.embedding_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "        except ImportError:\n",
    "            logger.warning(\"SentenceTransformers not available. Install with: pip install sentence-transformers\")\n",
    "            self.embedding_model = None\n",
    "    \n",
    "    async def analyze_paper(self, paper_data: Dict) -> Dict:\n",
    "        \"\"\"Comprehensive analysis of a research paper\"\"\"\n",
    "        analysis = {\n",
    "            \"summary\": await self.generate_summary(paper_data),\n",
    "            \"key_contributions\": await self.extract_contributions(paper_data),\n",
    "            \"methodology\": await self.extract_methodology(paper_data),\n",
    "            \"results\": await self.extract_results(paper_data),\n",
    "            \"keywords\": await self.extract_keywords(paper_data),\n",
    "            \"novelty_score\": await self.calculate_novelty_score(paper_data),\n",
    "            \"impact_prediction\": await self.predict_impact(paper_data)\n",
    "        }\n",
    "        return analysis\n",
    "    \n",
    "    async def generate_summary(self, paper_data: Dict) -> str:\n",
    "        \"\"\"Generate a concise summary of the paper\"\"\"\n",
    "        abstract = paper_data.get(\"abstract\", \"\")\n",
    "        title = paper_data.get(\"title\", \"\")\n",
    "        \n",
    "        if not abstract:\n",
    "            return f\"Summary not available. Title: {title}\"\n",
    "        \n",
    "        # Simple extractive summarization\n",
    "        sentences = self._split_into_sentences(abstract)\n",
    "        \n",
    "        if len(sentences) <= 3:\n",
    "            return abstract\n",
    "        \n",
    "        # Score sentences by position and keyword frequency\n",
    "        scored_sentences = []\n",
    "        keywords = self._extract_important_terms(abstract)\n",
    "        \n",
    "        for i, sentence in enumerate(sentences):\n",
    "            score = 0\n",
    "            # Position score (first and last sentences are important)\n",
    "            if i == 0 or i == len(sentences) - 1:\n",
    "                score += 2\n",
    "            \n",
    "            # Keyword score\n",
    "            for keyword in keywords:\n",
    "                if keyword.lower() in sentence.lower():\n",
    "                    score += 1\n",
    "            \n",
    "            scored_sentences.append((score, sentence))\n",
    "        \n",
    "        # Select top sentences\n",
    "        scored_sentences.sort(reverse=True)\n",
    "        summary_sentences = [sent for _, sent in scored_sentences[:3]]\n",
    "        \n",
    "        return \" \".join(summary_sentences)\n",
    "    \n",
    "    async def extract_contributions(self, paper_data: Dict) -> List[str]:\n",
    "        \"\"\"Extract key contributions from the paper\"\"\"\n",
    "        abstract = paper_data.get(\"abstract\", \"\")\n",
    "        title = paper_data.get(\"title\", \"\")\n",
    "        \n",
    "        contributions = []\n",
    "        \n",
    "        # Look for contribution indicators\n",
    "        contribution_patterns = [\n",
    "            r\"we propose\", r\"we present\", r\"we introduce\", r\"we develop\",\n",
    "            r\"our contribution\", r\"our approach\", r\"we show\", r\"we demonstrate\",\n",
    "            r\"novel\", r\"new\", r\"first time\", r\"state-of-the-art\"\n",
    "        ]\n",
    "        \n",
    "        text = f\"{title} {abstract}\".lower()\n",
    "        sentences = self._split_into_sentences(text)\n",
    "        \n",
    "        for sentence in sentences:\n",
    "            for pattern in contribution_patterns:\n",
    "                if re.search(pattern, sentence.lower()):\n",
    "                    contributions.append(sentence.strip())\n",
    "                    break\n",
    "        \n",
    "        # Remove duplicates and limit to top 5\n",
    "        unique_contributions = list(dict.fromkeys(contributions))\n",
    "        return unique_contributions[:5]\n",
    "    \n",
    "    async def extract_methodology(self, paper_data: Dict) -> str:\n",
    "        \"\"\"Extract methodology information from the paper\"\"\"\n",
    "        abstract = paper_data.get(\"abstract\", \"\")\n",
    "        \n",
    "        # Look for methodology indicators\n",
    "        method_patterns = [\n",
    "            r\"method\", r\"approach\", r\"algorithm\", r\"technique\", r\"framework\",\n",
    "            r\"model\", r\"system\", r\"experiment\", r\"evaluation\", r\"analysis\"\n",
    "        ]\n",
    "        \n",
    "        sentences = self._split_into_sentences(abstract)\n",
    "        method_sentences = []\n",
    "        \n",
    "        for sentence in sentences:\n",
    "            for pattern in method_patterns:\n",
    "                if re.search(pattern, sentence.lower()):\n",
    "                    method_sentences.append(sentence)\n",
    "                    break\n",
    "        \n",
    "        return \" \".join(method_sentences[:3])\n",
    "    \n",
    "    async def extract_results(self, paper_data: Dict) -> str:\n",
    "        \"\"\"Extract results information from the paper\"\"\"\n",
    "        abstract = paper_data.get(\"abstract\", \"\")\n",
    "        \n",
    "        # Look for results indicators\n",
    "        result_patterns = [\n",
    "            r\"result\", r\"finding\", r\"achieve\", r\"performance\", r\"improvement\",\n",
    "            r\"accuracy\", r\"precision\", r\"recall\", r\"f1\", r\"score\", r\"metric\"\n",
    "        ]\n",
    "        \n",
    "        sentences = self._split_into_sentences(abstract)\n",
    "        result_sentences = []\n",
    "        \n",
    "        for sentence in sentences:\n",
    "            for pattern in result_patterns:\n",
    "                if re.search(pattern, sentence.lower()):\n",
    "                    result_sentences.append(sentence)\n",
    "                    break\n",
    "        \n",
    "        return \" \".join(result_sentences[:3])\n",
    "    \n",
    "    async def extract_keywords(self, paper_data: Dict) -> List[str]:\n",
    "        \"\"\"Extract important keywords from the paper\"\"\"\n",
    "        text = f\"{paper_data.get('title', '')} {paper_data.get('abstract', '')}\"\n",
    "        \n",
    "        # Use existing keywords if available\n",
    "        existing_keywords = paper_data.get(\"keywords\", [])\n",
    "        if existing_keywords:\n",
    "            return existing_keywords\n",
    "        \n",
    "        # Extract keywords using NLP\n",
    "        if self.nlp:\n",
    "            doc = self.nlp(text)\n",
    "            \n",
    "            # Extract noun phrases and named entities\n",
    "            keywords = set()\n",
    "            \n",
    "            # Named entities\n",
    "            for ent in doc.ents:\n",
    "                if ent.label_ in [\"PERSON\", \"ORG\", \"TECH\", \"PRODUCT\"]:\n",
    "                    keywords.add(ent.text.lower())\n",
    "            \n",
    "            # Noun phrases\n",
    "            for chunk in doc.noun_chunks:\n",
    "                if len(chunk.text.split()) <= 3:  # Limit to 3 words\n",
    "                    keywords.add(chunk.text.lower())\n",
    "            \n",
    "            return list(keywords)[:10]\n",
    "        else:\n",
    "            # Simple keyword extraction\n",
    "            return self._extract_important_terms(text)\n",
    "    \n",
    "    async def calculate_novelty_score(self, paper_data: Dict) -> float:\n",
    "        \"\"\"Calculate a novelty score for the paper\"\"\"\n",
    "        title = paper_data.get(\"title\", \"\").lower()\n",
    "        abstract = paper_data.get(\"abstract\", \"\").lower()\n",
    "        \n",
    "        # Novelty indicators\n",
    "        novelty_terms = [\n",
    "            \"novel\", \"new\", \"first\", \"innovative\", \"breakthrough\", \"pioneering\",\n",
    "            \"unprecedented\", \"original\", \"unique\", \"cutting-edge\", \"state-of-the-art\"\n",
    "        ]\n",
    "        \n",
    "        text = f\"{title} {abstract}\"\n",
    "        score = 0\n",
    "        \n",
    "        for term in novelty_terms:\n",
    "            score += text.count(term)\n",
    "        \n",
    "        # Normalize score (0-1 range)\n",
    "        max_score = len(novelty_terms) * 2  # Assume max 2 occurrences per term\n",
    "        return min(score / max_score, 1.0)\n",
    "    \n",
    "    async def predict_impact(self, paper_data: Dict) -> Dict:\n",
    "        \"\"\"Predict potential impact of the paper\"\"\"\n",
    "        # Simple heuristic-based impact prediction\n",
    "        venue = paper_data.get(\"journal\", \"\").lower()\n",
    "        authors = paper_data.get(\"authors\", [])\n",
    "        keywords = await self.extract_keywords(paper_data)\n",
    "        \n",
    "        impact_score = 0\n",
    "        factors = []\n",
    "        \n",
    "        # Venue quality (simplified)\n",
    "        high_impact_venues = [\"nature\", \"science\", \"cell\", \"nips\", \"icml\", \"acl\", \"cvpr\"]\n",
    "        if any(venue_name in venue for venue_name in high_impact_venues):\n",
    "            impact_score += 0.3\n",
    "            factors.append(\"High-impact venue\")\n",
    "        \n",
    "        # Number of authors (collaboration indicator)\n",
    "        if len(authors) >= 5:\n",
    "            impact_score += 0.1\n",
    "            factors.append(\"Large collaboration\")\n",
    "        \n",
    "        # Hot keywords\n",
    "        hot_keywords = [\"ai\", \"machine learning\", \"deep learning\", \"nlp\", \"computer vision\", \n",
    "                       \"blockchain\", \"quantum\", \"climate\", \"covid\"]\n",
    "        if any(keyword in \" \".join(keywords).lower() for keyword in hot_keywords):\n",
    "            impact_score += 0.2\n",
    "            factors.append(\"Trending research area\")\n",
    "        \n",
    "        # Novelty contribution\n",
    "        novelty_score = await self.calculate_novelty_score(paper_data)\n",
    "        impact_score += novelty_score * 0.4\n",
    "        if novelty_score > 0.5:\n",
    "            factors.append(\"High novelty\")\n",
    "        \n",
    "        return {\n",
    "            \"predicted_impact_score\": min(impact_score, 1.0),\n",
    "            \"contributing_factors\": factors,\n",
    "            \"confidence\": \"low\"  # This is a simple heuristic\n",
    "        }\n",
    "    \n",
    "    async def compare_papers(self, papers: List[Dict]) -> Dict:\n",
    "        \"\"\"Compare multiple papers and generate a comparison table\"\"\"\n",
    "        if len(papers) < 2:\n",
    "            return {\"error\": \"Need at least 2 papers for comparison\"}\n",
    "        \n",
    "        comparison = {\n",
    "            \"papers\": [],\n",
    "            \"common_themes\": [],\n",
    "            \"key_differences\": [],\n",
    "            \"methodology_comparison\": {},\n",
    "            \"impact_ranking\": []\n",
    "        }\n",
    "        \n",
    "        # Analyze each paper\n",
    "        analyzed_papers = []\n",
    "        for paper in papers:\n",
    "            analysis = await self.analyze_paper(paper)\n",
    "            paper_info = {\n",
    "                \"title\": paper.get(\"title\", \"\"),\n",
    "                \"authors\": paper.get(\"authors\", []),\n",
    "                \"year\": paper.get(\"publication_date\", \"\"),\n",
    "                \"citation_count\": paper.get(\"citation_count\", 0),\n",
    "                \"analysis\": analysis\n",
    "            }\n",
    "            analyzed_papers.append(paper_info)\n",
    "        \n",
    "        comparison[\"papers\"] = analyzed_papers\n",
    "        \n",
    "        # Find common themes\n",
    "        all_keywords = []\n",
    "        for paper in analyzed_papers:\n",
    "            all_keywords.extend(paper[\"analysis\"][\"keywords\"])\n",
    "        \n",
    "        keyword_counts = Counter(all_keywords)\n",
    "        common_keywords = [kw for kw, count in keyword_counts.items() if count >= 2]\n",
    "        comparison[\"common_themes\"] = common_keywords[:10]\n",
    "        \n",
    "        # Rank by predicted impact\n",
    "        impact_ranking = sorted(analyzed_papers, \n",
    "                              key=lambda x: x[\"analysis\"][\"impact_prediction\"][\"predicted_impact_score\"], \n",
    "                              reverse=True)\n",
    "        comparison[\"impact_ranking\"] = [p[\"title\"] for p in impact_ranking]\n",
    "        \n",
    "        return comparison\n",
    "    \n",
    "    def _split_into_sentences(self, text: str) -> List[str]:\n",
    "        \"\"\"Split text into sentences\"\"\"\n",
    "        # Simple sentence splitting\n",
    "        sentences = re.split(r'[.!?]+', text)\n",
    "        return [s.strip() for s in sentences if s.strip()]\n",
    "    \n",
    "    def _extract_important_terms(self, text: str) -> List[str]:\n",
    "        \"\"\"Extract important terms using simple frequency analysis\"\"\"\n",
    "        # Remove common words\n",
    "        stop_words = {'the', 'a', 'an', 'and', 'or', 'but', 'in', 'on', 'at', 'to', 'for', \n",
    "                     'of', 'with', 'by', 'is', 'are', 'was', 'were', 'be', 'been', 'being',\n",
    "                     'have', 'has', 'had', 'do', 'does', 'did', 'will', 'would', 'could',\n",
    "                     'should', 'may', 'might', 'must', 'can', 'this', 'that', 'these', 'those'}\n",
    "        \n",
    "        words = re.findall(r'\\b[a-zA-Z]+\\b', text.lower())\n",
    "        word_freq = Counter(word for word in words if word not in stop_words and len(word) > 3)\n",
    "        \n",
    "        return [word for word, count in word_freq.most_common(10)]\n",
    "\n",
    "# Initialize paper analysis service\n",
    "paper_analysis = PaperAnalysisService()\n",
    "\n",
    "print(\"üìÑ Paper Analysis Service initialized!\")\n",
    "print(\"üß† Features: Summarization, contribution extraction, novelty scoring\")\n",
    "print(\"üìä Capabilities: Impact prediction, paper comparison, keyword extraction\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce89d852",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core Service 4: Trend Analysis Service\n",
    "class TrendAnalysisService:\n",
    "    \"\"\"Service for analyzing research trends and emerging topics\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.vectorizer = TfidfVectorizer(max_features=1000, stop_words='english')\n",
    "        self.lda_model = LatentDirichletAllocation(n_components=10, random_state=42)\n",
    "    \n",
    "    async def analyze_field_trends(self, field: str, time_range: str = \"5_years\") -> Dict:\n",
    "        \"\"\"Analyze trends in a specific research field\"\"\"\n",
    "        # Get papers from the field\n",
    "        papers = await literature_search.unified_search(field, max_results=200)\n",
    "        \n",
    "        if not papers:\n",
    "            return {\"error\": \"No papers found for the field\"}\n",
    "        \n",
    "        # Filter by time range\n",
    "        filtered_papers = self._filter_papers_by_time(papers, time_range)\n",
    "        \n",
    "        # Analyze trends\n",
    "        trends = {\n",
    "            \"field\": field,\n",
    "            \"time_range\": time_range,\n",
    "            \"total_papers\": len(filtered_papers),\n",
    "            \"trending_topics\": await self._identify_trending_topics(filtered_papers),\n",
    "            \"emerging_keywords\": await self._find_emerging_keywords(filtered_papers),\n",
    "            \"publication_growth\": self._analyze_publication_growth(filtered_papers),\n",
    "            \"top_venues\": self._analyze_top_venues(filtered_papers),\n",
    "            \"citation_trends\": self._analyze_citation_trends(filtered_papers),\n",
    "            \"geographical_distribution\": await self._analyze_geographical_trends(filtered_papers)\n",
    "        }\n",
    "        \n",
    "        return trends\n",
    "    \n",
    "    async def _identify_trending_topics(self, papers: List[Dict]) -> List[Dict]:\n",
    "        \"\"\"Identify trending topics using topic modeling\"\"\"\n",
    "        # Prepare text data\n",
    "        texts = []\n",
    "        for paper in papers:\n",
    "            text = f\"{paper.get('title', '')} {paper.get('abstract', '')}\"\n",
    "            texts.append(text)\n",
    "        \n",
    "        if not texts:\n",
    "            return []\n",
    "        \n",
    "        try:\n",
    "            # Vectorize texts\n",
    "            tfidf_matrix = self.vectorizer.fit_transform(texts)\n",
    "            \n",
    "            # Perform topic modeling\n",
    "            self.lda_model.fit(tfidf_matrix)\n",
    "            \n",
    "            # Extract topics\n",
    "            feature_names = self.vectorizer.get_feature_names_out()\n",
    "            topics = []\n",
    "            \n",
    "            for topic_idx, topic in enumerate(self.lda_model.components_):\n",
    "                top_words_idx = topic.argsort()[-10:][::-1]\n",
    "                top_words = [feature_names[i] for i in top_words_idx]\n",
    "                topic_weight = topic[top_words_idx].sum()\n",
    "                \n",
    "                topics.append({\n",
    "                    \"topic_id\": topic_idx,\n",
    "                    \"keywords\": top_words,\n",
    "                    \"weight\": float(topic_weight),\n",
    "                    \"description\": self._generate_topic_description(top_words)\n",
    "                })\n",
    "            \n",
    "            # Sort by weight\n",
    "            topics.sort(key=lambda x: x[\"weight\"], reverse=True)\n",
    "            return topics[:5]\n",
    "        \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Topic modeling error: {str(e)}\")\n",
    "            return []\n",
    "    \n",
    "    async def _find_emerging_keywords(self, papers: List[Dict]) -> List[Dict]:\n",
    "        \"\"\"Find emerging keywords by analyzing their growth over time\"\"\"\n",
    "        # Group papers by year\n",
    "        papers_by_year = defaultdict(list)\n",
    "        current_year = datetime.now().year\n",
    "        \n",
    "        for paper in papers:\n",
    "            year = self._extract_year(paper.get(\"publication_date\"))\n",
    "            if year and year >= current_year - 5:\n",
    "                papers_by_year[year].append(paper)\n",
    "        \n",
    "        # Extract keywords for each year\n",
    "        keyword_trends = defaultdict(lambda: defaultdict(int))\n",
    "        \n",
    "        for year, year_papers in papers_by_year.items():\n",
    "            year_text = \" \".join([f\"{p.get('title', '')} {p.get('abstract', '')}\" \n",
    "                                for p in year_papers])\n",
    "            keywords = self._extract_keywords_from_text(year_text)\n",
    "            \n",
    "            for keyword in keywords:\n",
    "                keyword_trends[keyword][year] += 1\n",
    "        \n",
    "        # Calculate growth rates\n",
    "        emerging_keywords = []\n",
    "        for keyword, year_counts in keyword_trends.items():\n",
    "            if len(year_counts) >= 2:\n",
    "                years = sorted(year_counts.keys())\n",
    "                growth_rate = self._calculate_growth_rate(year_counts, years)\n",
    "                \n",
    "                emerging_keywords.append({\n",
    "                    \"keyword\": keyword,\n",
    "                    \"growth_rate\": growth_rate,\n",
    "                    \"recent_count\": year_counts.get(current_year, 0) + year_counts.get(current_year - 1, 0),\n",
    "                    \"trend_data\": dict(year_counts)\n",
    "                })\n",
    "        \n",
    "        # Sort by growth rate and recent activity\n",
    "        emerging_keywords.sort(key=lambda x: (x[\"growth_rate\"], x[\"recent_count\"]), reverse=True)\n",
    "        return emerging_keywords[:15]\n",
    "    \n",
    "    def _analyze_publication_growth(self, papers: List[Dict]) -> Dict:\n",
    "        \"\"\"Analyze publication growth over time\"\"\"\n",
    "        papers_by_year = defaultdict(int)\n",
    "        \n",
    "        for paper in papers:\n",
    "            year = self._extract_year(paper.get(\"publication_date\"))\n",
    "            if year:\n",
    "                papers_by_year[year] += 1\n",
    "        \n",
    "        # Calculate year-over-year growth\n",
    "        years = sorted(papers_by_year.keys())\n",
    "        growth_rates = []\n",
    "        \n",
    "        for i in range(1, len(years)):\n",
    "            prev_count = papers_by_year[years[i-1]]\n",
    "            curr_count = papers_by_year[years[i]]\n",
    "            if prev_count > 0:\n",
    "                growth_rate = ((curr_count - prev_count) / prev_count) * 100\n",
    "                growth_rates.append(growth_rate)\n",
    "        \n",
    "        avg_growth_rate = np.mean(growth_rates) if growth_rates else 0\n",
    "        \n",
    "        return {\n",
    "            \"publications_by_year\": dict(papers_by_year),\n",
    "            \"average_growth_rate\": float(avg_growth_rate),\n",
    "            \"total_years\": len(years),\n",
    "            \"peak_year\": max(papers_by_year.keys(), key=lambda x: papers_by_year[x]) if papers_by_year else None\n",
    "        }\n",
    "    \n",
    "    def _analyze_top_venues(self, papers: List[Dict]) -> List[Dict]:\n",
    "        \"\"\"Analyze top publication venues\"\"\"\n",
    "        venue_counts = defaultdict(int)\n",
    "        venue_citations = defaultdict(list)\n",
    "        \n",
    "        for paper in papers:\n",
    "            venue = paper.get(\"journal\", \"\").strip()\n",
    "            if venue:\n",
    "                venue_counts[venue] += 1\n",
    "                venue_citations[venue].append(paper.get(\"citation_count\", 0))\n",
    "        \n",
    "        venues = []\n",
    "        for venue, count in venue_counts.items():\n",
    "            avg_citations = np.mean(venue_citations[venue]) if venue_citations[venue] else 0\n",
    "            venues.append({\n",
    "                \"venue\": venue,\n",
    "                \"paper_count\": count,\n",
    "                \"average_citations\": float(avg_citations),\n",
    "                \"total_citations\": sum(venue_citations[venue])\n",
    "            })\n",
    "        \n",
    "        venues.sort(key=lambda x: x[\"paper_count\"], reverse=True)\n",
    "        return venues[:10]\n",
    "    \n",
    "    def _analyze_citation_trends(self, papers: List[Dict]) -> Dict:\n",
    "        \"\"\"Analyze citation trends\"\"\"\n",
    "        citation_data = []\n",
    "        \n",
    "        for paper in papers:\n",
    "            year = self._extract_year(paper.get(\"publication_date\"))\n",
    "            citations = paper.get(\"citation_count\", 0)\n",
    "            if year and citations is not None:\n",
    "                citation_data.append({\"year\": year, \"citations\": citations})\n",
    "        \n",
    "        if not citation_data:\n",
    "            return {}\n",
    "        \n",
    "        df = pd.DataFrame(citation_data)\n",
    "        citation_by_year = df.groupby(\"year\")[\"citations\"].agg([\"mean\", \"median\", \"sum\", \"count\"]).to_dict()\n",
    "        \n",
    "        return {\n",
    "            \"average_citations_by_year\": citation_by_year.get(\"mean\", {}),\n",
    "            \"median_citations_by_year\": citation_by_year.get(\"median\", {}),\n",
    "            \"total_citations_by_year\": citation_by_year.get(\"sum\", {}),\n",
    "            \"papers_by_year\": citation_by_year.get(\"count\", {})\n",
    "        }\n",
    "    \n",
    "    async def _analyze_geographical_trends(self, papers: List[Dict]) -> Dict:\n",
    "        \"\"\"Analyze geographical distribution of research\"\"\"\n",
    "        # Simple country extraction from affiliations\n",
    "        country_counts = defaultdict(int)\n",
    "        \n",
    "        for paper in papers:\n",
    "            authors = paper.get(\"authors\", [])\n",
    "            # This is a simplified approach - in practice, you'd need more sophisticated affiliation parsing\n",
    "            for author in authors:\n",
    "                if isinstance(author, str):\n",
    "                    # Look for country indicators in author strings\n",
    "                    countries = self._extract_countries_from_text(author)\n",
    "                    for country in countries:\n",
    "                        country_counts[country] += 1\n",
    "        \n",
    "        countries = [{\"country\": country, \"paper_count\": count} \n",
    "                    for country, count in country_counts.items()]\n",
    "        countries.sort(key=lambda x: x[\"paper_count\"], reverse=True)\n",
    "        \n",
    "        return {\n",
    "            \"top_countries\": countries[:15],\n",
    "            \"total_countries\": len(countries)\n",
    "        }\n",
    "    \n",
    "    def _filter_papers_by_time(self, papers: List[Dict], time_range: str) -> List[Dict]:\n",
    "        \"\"\"Filter papers by time range\"\"\"\n",
    "        current_year = datetime.now().year\n",
    "        \n",
    "        if time_range == \"1_year\":\n",
    "            cutoff_year = current_year - 1\n",
    "        elif time_range == \"3_years\":\n",
    "            cutoff_year = current_year - 3\n",
    "        elif time_range == \"5_years\":\n",
    "            cutoff_year = current_year - 5\n",
    "        elif time_range == \"10_years\":\n",
    "            cutoff_year = current_year - 10\n",
    "        else:\n",
    "            cutoff_year = current_year - 5\n",
    "        \n",
    "        filtered = []\n",
    "        for paper in papers:\n",
    "            year = self._extract_year(paper.get(\"publication_date\"))\n",
    "            if year and year >= cutoff_year:\n",
    "                filtered.append(paper)\n",
    "        \n",
    "        return filtered\n",
    "    \n",
    "    def _extract_year(self, date_str) -> Optional[int]:\n",
    "        \"\"\"Extract year from date string\"\"\"\n",
    "        if not date_str:\n",
    "            return None\n",
    "        \n",
    "        try:\n",
    "            if isinstance(date_str, datetime):\n",
    "                return date_str.year\n",
    "            elif isinstance(date_str, str):\n",
    "                # Try different date formats\n",
    "                year_match = re.search(r'\\b(19|20)\\d{2}\\b', date_str)\n",
    "                if year_match:\n",
    "                    return int(year_match.group())\n",
    "            return None\n",
    "        except:\n",
    "            return None\n",
    "    \n",
    "    def _extract_keywords_from_text(self, text: str) -> List[str]:\n",
    "        \"\"\"Extract keywords from text\"\"\"\n",
    "        # Simple keyword extraction\n",
    "        words = re.findall(r'\\b[a-zA-Z]+\\b', text.lower())\n",
    "        \n",
    "        # Filter out common words\n",
    "        stop_words = {'the', 'a', 'an', 'and', 'or', 'but', 'in', 'on', 'at', 'to', 'for', \n",
    "                     'of', 'with', 'by', 'is', 'are', 'was', 'were', 'be', 'been', 'being',\n",
    "                     'have', 'has', 'had', 'do', 'does', 'did', 'will', 'would', 'could',\n",
    "                     'should', 'may', 'might', 'must', 'can', 'this', 'that', 'these', 'those'}\n",
    "        \n",
    "        filtered_words = [word for word in words if word not in stop_words and len(word) > 3]\n",
    "        word_freq = Counter(filtered_words)\n",
    "        \n",
    "        return [word for word, count in word_freq.most_common(50) if count >= 2]\n",
    "    \n",
    "    def _calculate_growth_rate(self, year_counts: Dict[int, int], years: List[int]) -> float:\n",
    "        \"\"\"Calculate growth rate for keyword usage\"\"\"\n",
    "        if len(years) < 2:\n",
    "            return 0\n",
    "        \n",
    "        # Calculate average growth rate\n",
    "        growth_rates = []\n",
    "        for i in range(1, len(years)):\n",
    "            prev_count = year_counts[years[i-1]]\n",
    "            curr_count = year_counts[years[i]]\n",
    "            if prev_count > 0:\n",
    "                growth_rate = ((curr_count - prev_count) / prev_count) * 100\n",
    "                growth_rates.append(growth_rate)\n",
    "        \n",
    "        return np.mean(growth_rates) if growth_rates else 0\n",
    "    \n",
    "    def _extract_countries_from_text(self, text: str) -> List[str]:\n",
    "        \"\"\"Extract country names from text (simplified)\"\"\"\n",
    "        # This is a very basic implementation\n",
    "        countries = [\n",
    "            \"USA\", \"United States\", \"China\", \"UK\", \"United Kingdom\", \"Germany\", \n",
    "            \"Japan\", \"France\", \"Canada\", \"Australia\", \"India\", \"Brazil\", \n",
    "            \"Italy\", \"Spain\", \"Netherlands\", \"Switzerland\", \"Sweden\", \"Korea\"\n",
    "        ]\n",
    "        \n",
    "        found_countries = []\n",
    "        text_lower = text.lower()\n",
    "        for country in countries:\n",
    "            if country.lower() in text_lower:\n",
    "                found_countries.append(country)\n",
    "        \n",
    "        return found_countries\n",
    "    \n",
    "    def _generate_topic_description(self, keywords: List[str]) -> str:\n",
    "        \"\"\"Generate a human-readable description for a topic\"\"\"\n",
    "        # Simple description generation based on top keywords\n",
    "        if not keywords:\n",
    "            return \"Unknown topic\"\n",
    "        \n",
    "        return f\"Research related to {', '.join(keywords[:3])}\"\n",
    "\n",
    "# Initialize trend analysis service\n",
    "trend_analysis = TrendAnalysisService()\n",
    "\n",
    "print(\"üìà Trend Analysis Service initialized!\")\n",
    "print(\"üîç Features: Topic modeling, keyword trend analysis, publication growth\")\n",
    "print(\"üåç Capabilities: Geographical analysis, venue trends, citation patterns\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc4b50b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Advanced Service 1: Research Gap Finder\n",
    "class ResearchGapService:\n",
    "    \"\"\"Service for identifying research gaps and opportunities\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.similarity_threshold = 0.7\n",
    "        \n",
    "    async def find_research_gaps(self, field: str, depth: str = \"comprehensive\") -> List[Dict]:\n",
    "        \"\"\"Find research gaps in a specific field\"\"\"\n",
    "        # Get comprehensive literature for the field\n",
    "        papers = await literature_search.unified_search(field, max_results=300)\n",
    "        \n",
    "        if not papers:\n",
    "            return []\n",
    "        \n",
    "        gaps = []\n",
    "        \n",
    "        # Analyze different types of gaps\n",
    "        methodology_gaps = await self._find_methodology_gaps(papers)\n",
    "        temporal_gaps = await self._find_temporal_gaps(papers)\n",
    "        application_gaps = await self._find_application_gaps(papers)\n",
    "        geographical_gaps = await self._find_geographical_gaps(papers)\n",
    "        interdisciplinary_gaps = await self._find_interdisciplinary_gaps(papers, field)\n",
    "        \n",
    "        gaps.extend(methodology_gaps)\n",
    "        gaps.extend(temporal_gaps)\n",
    "        gaps.extend(application_gaps)\n",
    "        gaps.extend(geographical_gaps)\n",
    "        gaps.extend(interdisciplinary_gaps)\n",
    "        \n",
    "        # Score and rank gaps\n",
    "        scored_gaps = []\n",
    "        for gap in gaps:\n",
    "            score = self._calculate_gap_importance(gap, papers)\n",
    "            gap[\"importance_score\"] = score\n",
    "            scored_gaps.append(gap)\n",
    "        \n",
    "        scored_gaps.sort(key=lambda x: x[\"importance_score\"], reverse=True)\n",
    "        return scored_gaps[:20]\n",
    "    \n",
    "    async def _find_methodology_gaps(self, papers: List[Dict]) -> List[Dict]:\n",
    "        \"\"\"Find gaps in research methodologies\"\"\"\n",
    "        methodology_terms = defaultdict(int)\n",
    "        \n",
    "        # Extract methodology terms from papers\n",
    "        for paper in papers:\n",
    "            text = f\"{paper.get('title', '')} {paper.get('abstract', '')}\"\n",
    "            methods = self._extract_methodology_terms(text)\n",
    "            for method in methods:\n",
    "                methodology_terms[method] += 1\n",
    "        \n",
    "        # Find underexplored methodologies\n",
    "        gaps = []\n",
    "        all_methods = set(methodology_terms.keys())\n",
    "        \n",
    "        # Common methodologies that might be missing\n",
    "        expected_methods = {\n",
    "            \"machine learning\": [\"deep learning\", \"reinforcement learning\", \"transfer learning\"],\n",
    "            \"data analysis\": [\"time series analysis\", \"causal inference\", \"bayesian analysis\"],\n",
    "            \"experimental\": [\"randomized controlled trial\", \"a/b testing\", \"longitudinal study\"],\n",
    "            \"computational\": [\"simulation\", \"modeling\", \"optimization\"]\n",
    "        }\n",
    "        \n",
    "        for category, methods in expected_methods.items():\n",
    "            for method in methods:\n",
    "                if method not in all_methods or methodology_terms[method] < 3:\n",
    "                    gaps.append({\n",
    "                        \"type\": \"methodology\",\n",
    "                        \"gap_description\": f\"Limited application of {method} in this field\",\n",
    "                        \"category\": category,\n",
    "                        \"suggested_approach\": method,\n",
    "                        \"evidence\": f\"Only {methodology_terms.get(method, 0)} papers found using this methodology\",\n",
    "                        \"research_directions\": [\n",
    "                            f\"Apply {method} to existing problems in the field\",\n",
    "                            f\"Develop novel {method} approaches for domain-specific challenges\",\n",
    "                            f\"Compare {method} with existing approaches\"\n",
    "                        ]\n",
    "                    })\n",
    "        \n",
    "        return gaps[:5]\n",
    "    \n",
    "    async def _find_temporal_gaps(self, papers: List[Dict]) -> List[Dict]:\n",
    "        \"\"\"Find temporal gaps in research coverage\"\"\"\n",
    "        current_year = datetime.now().year\n",
    "        papers_by_year = defaultdict(int)\n",
    "        \n",
    "        for paper in papers:\n",
    "            year = self._extract_year_from_paper(paper)\n",
    "            if year:\n",
    "                papers_by_year[year] += 1\n",
    "        \n",
    "        gaps = []\n",
    "        \n",
    "        # Find years with low research activity\n",
    "        years = sorted(papers_by_year.keys())\n",
    "        if len(years) >= 3:\n",
    "            avg_papers = np.mean(list(papers_by_year.values()))\n",
    "            \n",
    "            for year in years:\n",
    "                if papers_by_year[year] < avg_papers * 0.5:  # Less than 50% of average\n",
    "                    gaps.append({\n",
    "                        \"type\": \"temporal\",\n",
    "                        \"gap_description\": f\"Low research activity in {year}\",\n",
    "                        \"period\": str(year),\n",
    "                        \"paper_count\": papers_by_year[year],\n",
    "                        \"evidence\": f\"Only {papers_by_year[year]} papers published vs average of {avg_papers:.1f}\",\n",
    "                        \"research_directions\": [\n",
    "                            f\"Investigate why research declined in {year}\",\n",
    "                            f\"Revisit and update research from {year} with modern approaches\",\n",
    "                            f\"Fill knowledge gaps from the {year} period\"\n",
    "                        ]\n",
    "                    })\n",
    "        \n",
    "        # Find emerging trends that need more research\n",
    "        recent_years = [y for y in years if y >= current_year - 3]\n",
    "        if recent_years:\n",
    "            recent_growth = sum(papers_by_year[y] for y in recent_years) / len(recent_years)\n",
    "            older_years = [y for y in years if y < current_year - 3]\n",
    "            if older_years:\n",
    "                older_avg = sum(papers_by_year[y] for y in older_years) / len(older_years)\n",
    "                \n",
    "                if recent_growth > older_avg * 1.5:  # 50% increase\n",
    "                    gaps.append({\n",
    "                        \"type\": \"temporal\",\n",
    "                        \"gap_description\": \"Rapidly growing field needs more comprehensive research\",\n",
    "                        \"period\": f\"{min(recent_years)}-{max(recent_years)}\",\n",
    "                        \"evidence\": f\"Research activity increased by {((recent_growth - older_avg) / older_avg * 100):.1f}%\",\n",
    "                        \"research_directions\": [\n",
    "                            \"Conduct comprehensive surveys of recent developments\",\n",
    "                            \"Establish best practices for the growing field\",\n",
    "                            \"Address scalability and sustainability challenges\"\n",
    "                        ]\n",
    "                    })\n",
    "        \n",
    "        return gaps[:3]\n",
    "    \n",
    "    async def _find_application_gaps(self, papers: List[Dict]) -> List[Dict]:\n",
    "        \"\"\"Find gaps in application domains\"\"\"\n",
    "        application_domains = defaultdict(int)\n",
    "        \n",
    "        # Extract application domains\n",
    "        domain_keywords = {\n",
    "            \"healthcare\": [\"medical\", \"health\", \"clinical\", \"patient\", \"diagnosis\", \"treatment\"],\n",
    "            \"education\": [\"learning\", \"teaching\", \"student\", \"educational\", \"curriculum\"],\n",
    "            \"finance\": [\"financial\", \"banking\", \"investment\", \"trading\", \"economic\"],\n",
    "            \"transportation\": [\"traffic\", \"automotive\", \"transportation\", \"logistics\"],\n",
    "            \"environment\": [\"environmental\", \"climate\", \"sustainability\", \"green\"],\n",
    "            \"agriculture\": [\"agricultural\", \"farming\", \"crop\", \"food production\"],\n",
    "            \"manufacturing\": [\"manufacturing\", \"industrial\", \"production\", \"quality control\"],\n",
    "            \"security\": [\"security\", \"cybersecurity\", \"privacy\", \"encryption\"],\n",
    "            \"entertainment\": [\"gaming\", \"media\", \"entertainment\", \"social media\"]\n",
    "        }\n",
    "        \n",
    "        for paper in papers:\n",
    "            text = f\"{paper.get('title', '')} {paper.get('abstract', '')}\".lower()\n",
    "            for domain, keywords in domain_keywords.items():\n",
    "                if any(keyword in text for keyword in keywords):\n",
    "                    application_domains[domain] += 1\n",
    "        \n",
    "        gaps = []\n",
    "        total_papers = len(papers)\n",
    "        \n",
    "        for domain, count in application_domains.items():\n",
    "            coverage_percentage = (count / total_papers) * 100 if total_papers > 0 else 0\n",
    "            \n",
    "            if coverage_percentage < 5:  # Less than 5% coverage\n",
    "                gaps.append({\n",
    "                    \"type\": \"application\",\n",
    "                    \"gap_description\": f\"Underexplored application in {domain}\",\n",
    "                    \"domain\": domain,\n",
    "                    \"coverage_percentage\": coverage_percentage,\n",
    "                    \"evidence\": f\"Only {count} papers ({coverage_percentage:.1f}%) address {domain} applications\",\n",
    "                    \"research_directions\": [\n",
    "                        f\"Explore applications of field techniques in {domain}\",\n",
    "                        f\"Develop {domain}-specific methodologies\",\n",
    "                        f\"Create benchmarks and datasets for {domain} applications\"\n",
    "                    ]\n",
    "                })\n",
    "        \n",
    "        return gaps[:4]\n",
    "    \n",
    "    async def _find_geographical_gaps(self, papers: List[Dict]) -> List[Dict]:\n",
    "        \"\"\"Find geographical gaps in research\"\"\"\n",
    "        # This is a simplified implementation\n",
    "        regions = defaultdict(int)\n",
    "        \n",
    "        developed_regions = [\"USA\", \"Europe\", \"Japan\", \"Australia\", \"Canada\"]\n",
    "        developing_regions = [\"Africa\", \"South America\", \"Southeast Asia\", \"Middle East\"]\n",
    "        \n",
    "        for paper in papers:\n",
    "            # Simple heuristic based on author affiliations\n",
    "            authors_text = \" \".join(paper.get(\"authors\", [])).lower()\n",
    "            \n",
    "            for region in developed_regions + developing_regions:\n",
    "                if region.lower() in authors_text:\n",
    "                    regions[region] += 1\n",
    "        \n",
    "        gaps = []\n",
    "        total_papers_with_geo = sum(regions.values())\n",
    "        \n",
    "        if total_papers_with_geo > 0:\n",
    "            developing_coverage = sum(regions[r] for r in developing_regions)\n",
    "            developing_percentage = (developing_coverage / total_papers_with_geo) * 100\n",
    "            \n",
    "            if developing_percentage < 20:  # Less than 20% from developing regions\n",
    "                gaps.append({\n",
    "                    \"type\": \"geographical\",\n",
    "                    \"gap_description\": \"Limited research from developing regions\",\n",
    "                    \"coverage_percentage\": developing_percentage,\n",
    "                    \"evidence\": f\"Only {developing_percentage:.1f}% of research from developing regions\",\n",
    "                    \"research_directions\": [\n",
    "                        \"Encourage international collaboration\",\n",
    "                        \"Address region-specific challenges\",\n",
    "                        \"Develop culturally appropriate solutions\"\n",
    "                    ]\n",
    "                })\n",
    "        \n",
    "        return gaps[:2]\n",
    "    \n",
    "    async def _find_interdisciplinary_gaps(self, papers: List[Dict], field: str) -> List[Dict]:\n",
    "        \"\"\"Find interdisciplinary research gaps\"\"\"\n",
    "        field_disciplines = {\n",
    "            \"computer science\": [\"psychology\", \"biology\", \"physics\", \"mathematics\", \"linguistics\"],\n",
    "            \"medicine\": [\"engineering\", \"computer science\", \"psychology\", \"sociology\"],\n",
    "            \"engineering\": [\"biology\", \"medicine\", \"psychology\", \"environmental science\"],\n",
    "            \"psychology\": [\"neuroscience\", \"computer science\", \"medicine\", \"sociology\"]\n",
    "        }\n",
    "        \n",
    "        potential_disciplines = field_disciplines.get(field.lower(), [])\n",
    "        if not potential_disciplines:\n",
    "            return []\n",
    "        \n",
    "        interdisciplinary_count = defaultdict(int)\n",
    "        \n",
    "        for paper in papers:\n",
    "            text = f\"{paper.get('title', '')} {paper.get('abstract', '')}\".lower()\n",
    "            for discipline in potential_disciplines:\n",
    "                if discipline in text:\n",
    "                    interdisciplinary_count[discipline] += 1\n",
    "        \n",
    "        gaps = []\n",
    "        total_papers = len(papers)\n",
    "        \n",
    "        for discipline in potential_disciplines:\n",
    "            count = interdisciplinary_count[discipline]\n",
    "            percentage = (count / total_papers) * 100 if total_papers > 0 else 0\n",
    "            \n",
    "            if percentage < 10:  # Less than 10% interdisciplinary work\n",
    "                gaps.append({\n",
    "                    \"type\": \"interdisciplinary\",\n",
    "                    \"gap_description\": f\"Limited collaboration with {discipline}\",\n",
    "                    \"target_discipline\": discipline,\n",
    "                    \"collaboration_percentage\": percentage,\n",
    "                    \"evidence\": f\"Only {count} papers ({percentage:.1f}%) show collaboration with {discipline}\",\n",
    "                    \"research_directions\": [\n",
    "                        f\"Explore {field}-{discipline} collaborations\",\n",
    "                        f\"Apply {discipline} principles to {field} problems\",\n",
    "                        f\"Develop joint methodologies with {discipline} researchers\"\n",
    "                    ]\n",
    "                })\n",
    "        \n",
    "        return gaps[:3]\n",
    "    \n",
    "    def _extract_methodology_terms(self, text: str) -> List[str]:\n",
    "        \"\"\"Extract methodology terms from text\"\"\"\n",
    "        methodology_keywords = [\n",
    "            \"machine learning\", \"deep learning\", \"neural network\", \"algorithm\",\n",
    "            \"statistical analysis\", \"regression\", \"classification\", \"clustering\",\n",
    "            \"experiment\", \"survey\", \"case study\", \"interview\", \"observation\",\n",
    "            \"simulation\", \"modeling\", \"optimization\", \"validation\",\n",
    "            \"qualitative\", \"quantitative\", \"mixed methods\", \"longitudinal\",\n",
    "            \"cross-sectional\", \"randomized\", \"controlled\", \"blind study\"\n",
    "        ]\n",
    "        \n",
    "        found_methods = []\n",
    "        text_lower = text.lower()\n",
    "        \n",
    "        for method in methodology_keywords:\n",
    "            if method in text_lower:\n",
    "                found_methods.append(method)\n",
    "        \n",
    "        return found_methods\n",
    "    \n",
    "    def _extract_year_from_paper(self, paper: Dict) -> Optional[int]:\n",
    "        \"\"\"Extract publication year from paper\"\"\"\n",
    "        date_str = paper.get(\"publication_date\")\n",
    "        if not date_str:\n",
    "            return None\n",
    "        \n",
    "        try:\n",
    "            if isinstance(date_str, datetime):\n",
    "                return date_str.year\n",
    "            elif isinstance(date_str, str):\n",
    "                year_match = re.search(r'\\b(19|20)\\d{2}\\b', date_str)\n",
    "                if year_match:\n",
    "                    return int(year_match.group())\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        return None\n",
    "    \n",
    "    def _calculate_gap_importance(self, gap: Dict, papers: List[Dict]) -> float:\n",
    "        \"\"\"Calculate importance score for a research gap\"\"\"\n",
    "        base_score = 1.0\n",
    "        \n",
    "        # Weight by gap type\n",
    "        type_weights = {\n",
    "            \"methodology\": 1.2,\n",
    "            \"interdisciplinary\": 1.1,\n",
    "            \"application\": 1.0,\n",
    "            \"temporal\": 0.8,\n",
    "            \"geographical\": 0.7\n",
    "        }\n",
    "        \n",
    "        gap_type = gap.get(\"type\", \"unknown\")\n",
    "        score = base_score * type_weights.get(gap_type, 1.0)\n",
    "        \n",
    "        # Boost score for coverage metrics\n",
    "        if \"coverage_percentage\" in gap:\n",
    "            coverage = gap[\"coverage_percentage\"]\n",
    "            if coverage < 5:\n",
    "                score *= 1.3\n",
    "            elif coverage < 10:\n",
    "                score *= 1.1\n",
    "        \n",
    "        # Boost for recent temporal gaps\n",
    "        if gap_type == \"temporal\" and \"period\" in gap:\n",
    "            try:\n",
    "                period = gap[\"period\"]\n",
    "                if isinstance(period, str) and len(period) == 4:\n",
    "                    year = int(period)\n",
    "                    current_year = datetime.now().year\n",
    "                    if year >= current_year - 3:\n",
    "                        score *= 1.2\n",
    "            except:\n",
    "                pass\n",
    "        \n",
    "        return score\n",
    "\n",
    "# Initialize research gap service\n",
    "research_gap_finder = ResearchGapService()\n",
    "\n",
    "print(\"üîç Research Gap Service initialized!\")\n",
    "print(\"üìä Gap Types: Methodology, Temporal, Application, Geographical, Interdisciplinary\")\n",
    "print(\"üéØ Features: Gap scoring, research direction suggestions, evidence-based analysis\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "187e642f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Advanced Service 2: Academic Writing Assistant\n",
    "class WritingAssistantService:\n",
    "    \"\"\"Service for assisting with academic writing\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        # Initialize text processing tools\n",
    "        try:\n",
    "            from textblob import TextBlob\n",
    "            self.has_textblob = True\n",
    "        except ImportError:\n",
    "            self.has_textblob = False\n",
    "            logger.warning(\"TextBlob not available. Install with: pip install textblob\")\n",
    "    \n",
    "    async def analyze_writing(self, text: str, context: str = \"academic\") -> Dict:\n",
    "        \"\"\"Comprehensive analysis of academic writing\"\"\"\n",
    "        analysis = {\n",
    "            \"readability\": await self._analyze_readability(text),\n",
    "            \"grammar_issues\": await self._check_grammar(text),\n",
    "            \"style_suggestions\": await self._analyze_style(text, context),\n",
    "            \"structure_feedback\": await self._analyze_structure(text),\n",
    "            \"citation_analysis\": await self._analyze_citations(text),\n",
    "            \"vocabulary_analysis\": await self._analyze_vocabulary(text),\n",
    "            \"coherence_score\": await self._analyze_coherence(text)\n",
    "        }\n",
    "        return analysis\n",
    "    \n",
    "    async def improve_text(self, text: str, improvement_type: str = \"comprehensive\") -> Dict:\n",
    "        \"\"\"Improve text based on specified criteria\"\"\"\n",
    "        improvements = {\n",
    "            \"original_text\": text,\n",
    "            \"improved_text\": text,\n",
    "            \"changes_made\": [],\n",
    "            \"confidence_score\": 0.0\n",
    "        }\n",
    "        \n",
    "        if improvement_type in [\"grammar\", \"comprehensive\"]:\n",
    "            grammar_improved = await self._improve_grammar(text)\n",
    "            if grammar_improved[\"text\"] != text:\n",
    "                improvements[\"improved_text\"] = grammar_improved[\"text\"]\n",
    "                improvements[\"changes_made\"].extend(grammar_improved[\"changes\"])\n",
    "        \n",
    "        if improvement_type in [\"style\", \"comprehensive\"]:\n",
    "            style_improved = await self._improve_style(improvements[\"improved_text\"])\n",
    "            if style_improved[\"text\"] != improvements[\"improved_text\"]:\n",
    "                improvements[\"improved_text\"] = style_improved[\"text\"]\n",
    "                improvements[\"changes_made\"].extend(style_improved[\"changes\"])\n",
    "        \n",
    "        if improvement_type in [\"clarity\", \"comprehensive\"]:\n",
    "            clarity_improved = await self._improve_clarity(improvements[\"improved_text\"])\n",
    "            if clarity_improved[\"text\"] != improvements[\"improved_text\"]:\n",
    "                improvements[\"improved_text\"] = clarity_improved[\"text\"]\n",
    "                improvements[\"changes_made\"].extend(clarity_improved[\"changes\"])\n",
    "        \n",
    "        # Calculate confidence score\n",
    "        improvements[\"confidence_score\"] = len(improvements[\"changes_made\"]) / 10.0\n",
    "        improvements[\"confidence_score\"] = min(improvements[\"confidence_score\"], 1.0)\n",
    "        \n",
    "        return improvements\n",
    "    \n",
    "    async def generate_academic_templates(self, template_type: str) -> Dict:\n",
    "        \"\"\"Generate academic writing templates\"\"\"\n",
    "        templates = {\n",
    "            \"abstract\": {\n",
    "                \"structure\": [\n",
    "                    \"Background/Context (1-2 sentences)\",\n",
    "                    \"Problem/Gap (1 sentence)\", \n",
    "                    \"Objective/Research Question (1 sentence)\",\n",
    "                    \"Methods/Approach (2-3 sentences)\",\n",
    "                    \"Results/Findings (2-3 sentences)\",\n",
    "                    \"Conclusions/Implications (1-2 sentences)\"\n",
    "                ],\n",
    "                \"example\": \"\"\"\n",
    "                [Background] Recent advances in artificial intelligence have shown promising applications across various domains. \n",
    "                [Problem] However, the lack of interpretability in deep learning models remains a significant challenge for critical applications. \n",
    "                [Objective] This study aims to develop a novel approach for improving model interpretability while maintaining performance. \n",
    "                [Methods] We propose a hybrid architecture combining attention mechanisms with gradient-based explanations, evaluated on three benchmark datasets. \n",
    "                [Results] Our approach achieved 94.2% accuracy while providing 40% more interpretable explanations compared to baseline methods. \n",
    "                [Conclusions] The proposed method offers a practical solution for deploying interpretable AI systems in critical domains.\n",
    "                \"\"\",\n",
    "                \"tips\": [\n",
    "                    \"Keep it under 250 words\",\n",
    "                    \"Use past tense for completed work\",\n",
    "                    \"Avoid citations in abstract\",\n",
    "                    \"Make it self-contained\"\n",
    "                ]\n",
    "            },\n",
    "            \"introduction\": {\n",
    "                \"structure\": [\n",
    "                    \"Hook/Opening statement\",\n",
    "                    \"Background and context\",\n",
    "                    \"Literature review summary\",\n",
    "                    \"Research gap identification\",\n",
    "                    \"Research objectives/questions\",\n",
    "                    \"Contribution statement\",\n",
    "                    \"Paper organization\"\n",
    "                ],\n",
    "                \"example\": \"\"\"\n",
    "                [Hook] The rapid growth of data-driven applications has fundamentally transformed how we approach complex problem-solving.\n",
    "                [Background] In particular, machine learning techniques have become indispensable tools for...\n",
    "                [Literature] Previous research has explored various approaches, including [cite1], [cite2], and [cite3]...\n",
    "                [Gap] However, existing methods suffer from limitations such as...\n",
    "                [Objectives] This paper addresses these limitations by proposing...\n",
    "                [Contributions] Our main contributions include: (1)..., (2)..., (3)...\n",
    "                [Organization] The remainder of this paper is organized as follows...\n",
    "                \"\"\",\n",
    "                \"tips\": [\n",
    "                    \"Start broad, then narrow down\",\n",
    "                    \"Clearly state the research gap\",\n",
    "                    \"Use parallel structure for contributions\",\n",
    "                    \"Include a roadmap paragraph\"\n",
    "                ]\n",
    "            },\n",
    "            \"methodology\": {\n",
    "                \"structure\": [\n",
    "                    \"Overall approach/framework\",\n",
    "                    \"Data collection/sources\",\n",
    "                    \"Experimental design\",\n",
    "                    \"Implementation details\",\n",
    "                    \"Evaluation metrics\",\n",
    "                    \"Baseline comparisons\"\n",
    "                ],\n",
    "                \"tips\": [\n",
    "                    \"Provide sufficient detail for replication\",\n",
    "                    \"Justify design choices\",\n",
    "                    \"Include parameter settings\",\n",
    "                    \"Describe validation procedures\"\n",
    "                ]\n",
    "            },\n",
    "            \"conclusion\": {\n",
    "                \"structure\": [\n",
    "                    \"Summary of findings\",\n",
    "                    \"Research contributions\",\n",
    "                    \"Implications\",\n",
    "                    \"Limitations\",\n",
    "                    \"Future work\"\n",
    "                ],\n",
    "                \"tips\": [\n",
    "                    \"Avoid introducing new information\",\n",
    "                    \"Restate key contributions\",\n",
    "                    \"Discuss broader impact\",\n",
    "                    \"Suggest concrete next steps\"\n",
    "                ]\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        return templates.get(template_type, {\"error\": \"Template type not found\"})\n",
    "    \n",
    "    async def suggest_citations(self, text: str, context: str = \"\") -> List[Dict]:\n",
    "        \"\"\"Suggest relevant citations for the given text\"\"\"\n",
    "        # Extract key concepts from text\n",
    "        key_concepts = self._extract_key_concepts(text)\n",
    "        \n",
    "        suggestions = []\n",
    "        for concept in key_concepts[:5]:  # Top 5 concepts\n",
    "            # Search for relevant papers\n",
    "            try:\n",
    "                papers = await literature_search.unified_search(concept, max_results=3)\n",
    "                for paper in papers:\n",
    "                    suggestion = {\n",
    "                        \"concept\": concept,\n",
    "                        \"paper_title\": paper.get(\"title\", \"\"),\n",
    "                        \"authors\": paper.get(\"authors\", []),\n",
    "                        \"year\": self._extract_year_from_date(paper.get(\"publication_date\")),\n",
    "                        \"relevance_score\": self._calculate_relevance(concept, paper),\n",
    "                        \"suggested_context\": f\"This work on {concept} relates to {paper.get('title', '')[:50]}...\"\n",
    "                    }\n",
    "                    suggestions.append(suggestion)\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Citation search error: {str(e)}\")\n",
    "        \n",
    "        # Sort by relevance\n",
    "        suggestions.sort(key=lambda x: x[\"relevance_score\"], reverse=True)\n",
    "        return suggestions[:10]\n",
    "    \n",
    "    async def _analyze_readability(self, text: str) -> Dict:\n",
    "        \"\"\"Analyze text readability\"\"\"\n",
    "        sentences = text.split('.')\n",
    "        words = text.split()\n",
    "        \n",
    "        avg_sentence_length = len(words) / len(sentences) if sentences else 0\n",
    "        avg_word_length = sum(len(word) for word in words) / len(words) if words else 0\n",
    "        \n",
    "        # Simple readability metrics\n",
    "        readability_score = 206.835 - (1.015 * avg_sentence_length) - (84.6 * avg_word_length / avg_sentence_length) if avg_sentence_length > 0 else 0\n",
    "        \n",
    "        level = \"Very Difficult\"\n",
    "        if readability_score >= 90:\n",
    "            level = \"Very Easy\"\n",
    "        elif readability_score >= 80:\n",
    "            level = \"Easy\"\n",
    "        elif readability_score >= 70:\n",
    "            level = \"Fairly Easy\"\n",
    "        elif readability_score >= 60:\n",
    "            level = \"Standard\"\n",
    "        elif readability_score >= 50:\n",
    "            level = \"Fairly Difficult\"\n",
    "        elif readability_score >= 30:\n",
    "            level = \"Difficult\"\n",
    "        \n",
    "        return {\n",
    "            \"flesch_score\": readability_score,\n",
    "            \"level\": level,\n",
    "            \"avg_sentence_length\": avg_sentence_length,\n",
    "            \"avg_word_length\": avg_word_length,\n",
    "            \"recommendations\": self._get_readability_recommendations(readability_score)\n",
    "        }\n",
    "    \n",
    "    async def _check_grammar(self, text: str) -> List[Dict]:\n",
    "        \"\"\"Check grammar issues\"\"\"\n",
    "        issues = []\n",
    "        \n",
    "        # Common academic writing issues\n",
    "        patterns = {\n",
    "            \"passive_voice\": r'\\b(is|are|was|were|been|being)\\s+\\w+ed\\b',\n",
    "            \"wordiness\": r'\\b(in order to|due to the fact that|it is important to note that)\\b',\n",
    "            \"weak_verbs\": r'\\b(is|are|was|were)\\s+(very|really|quite|rather)\\b',\n",
    "            \"redundancy\": r'\\b(completely eliminate|future plans|past history|end result)\\b',\n",
    "            \"first_person\": r'\\b(I|we|our|my)\\b'\n",
    "        }\n",
    "        \n",
    "        for issue_type, pattern in patterns.items():\n",
    "            matches = re.finditer(pattern, text, re.IGNORECASE)\n",
    "            for match in matches:\n",
    "                issues.append({\n",
    "                    \"type\": issue_type,\n",
    "                    \"text\": match.group(),\n",
    "                    \"position\": match.span(),\n",
    "                    \"suggestion\": self._get_grammar_suggestion(issue_type, match.group())\n",
    "                })\n",
    "        \n",
    "        return issues\n",
    "    \n",
    "    async def _analyze_style(self, text: str, context: str) -> List[Dict]:\n",
    "        \"\"\"Analyze writing style\"\"\"\n",
    "        suggestions = []\n",
    "        \n",
    "        # Check for academic style issues\n",
    "        if context == \"academic\":\n",
    "            # Contractions\n",
    "            contractions = re.findall(r\"\\b\\w+'\\w+\\b\", text)\n",
    "            if contractions:\n",
    "                suggestions.append({\n",
    "                    \"type\": \"contractions\",\n",
    "                    \"message\": f\"Avoid contractions in academic writing: {', '.join(contractions)}\",\n",
    "                    \"severity\": \"medium\"\n",
    "                })\n",
    "            \n",
    "            # Informal language\n",
    "            informal_words = [\"a lot of\", \"lots of\", \"big\", \"small\", \"good\", \"bad\", \"thing\", \"stuff\"]\n",
    "            found_informal = [word for word in informal_words if word.lower() in text.lower()]\n",
    "            if found_informal:\n",
    "                suggestions.append({\n",
    "                    \"type\": \"informal_language\",\n",
    "                    \"message\": f\"Consider more formal alternatives to: {', '.join(found_informal)}\",\n",
    "                    \"severity\": \"low\"\n",
    "                })\n",
    "            \n",
    "            # Sentence variety\n",
    "            sentences = [s.strip() for s in text.split('.') if s.strip()]\n",
    "            if len(sentences) > 5:\n",
    "                avg_length = sum(len(s.split()) for s in sentences) / len(sentences)\n",
    "                length_variance = np.var([len(s.split()) for s in sentences])\n",
    "                \n",
    "                if length_variance < 10:\n",
    "                    suggestions.append({\n",
    "                        \"type\": \"sentence_variety\",\n",
    "                        \"message\": \"Consider varying sentence length for better flow\",\n",
    "                        \"severity\": \"low\"\n",
    "                    })\n",
    "        \n",
    "        return suggestions\n",
    "    \n",
    "    async def _analyze_structure(self, text: str) -> Dict:\n",
    "        \"\"\"Analyze document structure\"\"\"\n",
    "        paragraphs = [p.strip() for p in text.split('\\n\\n') if p.strip()]\n",
    "        sentences = [s.strip() for s in text.split('.') if s.strip()]\n",
    "        \n",
    "        structure_analysis = {\n",
    "            \"paragraph_count\": len(paragraphs),\n",
    "            \"avg_paragraph_length\": sum(len(p.split()) for p in paragraphs) / len(paragraphs) if paragraphs else 0,\n",
    "            \"sentence_count\": len(sentences),\n",
    "            \"has_topic_sentences\": self._check_topic_sentences(paragraphs),\n",
    "            \"coherence_signals\": self._find_coherence_signals(text),\n",
    "            \"recommendations\": []\n",
    "        }\n",
    "        \n",
    "        # Generate recommendations\n",
    "        if structure_analysis[\"avg_paragraph_length\"] > 150:\n",
    "            structure_analysis[\"recommendations\"].append(\"Consider breaking long paragraphs into smaller ones\")\n",
    "        \n",
    "        if structure_analysis[\"avg_paragraph_length\"] < 50:\n",
    "            structure_analysis[\"recommendations\"].append(\"Consider developing paragraphs with more detail\")\n",
    "        \n",
    "        return structure_analysis\n",
    "    \n",
    "    async def _analyze_citations(self, text: str) -> Dict:\n",
    "        \"\"\"Analyze citation patterns\"\"\"\n",
    "        # Simple citation detection\n",
    "        citation_patterns = [\n",
    "            r'\\[(\\d+)\\]',  # [1]\n",
    "            r'\\(([^)]+,\\s*\\d{4})\\)',  # (Author, 2023)\n",
    "            r'et al\\.\\s*\\(\\d{4}\\)',  # et al. (2023)\n",
    "        ]\n",
    "        \n",
    "        citations = []\n",
    "        for pattern in citation_patterns:\n",
    "            matches = re.findall(pattern, text)\n",
    "            citations.extend(matches)\n",
    "        \n",
    "        return {\n",
    "            \"citation_count\": len(citations),\n",
    "            \"citation_density\": len(citations) / len(text.split()) * 100 if text.split() else 0,\n",
    "            \"citation_styles\": self._identify_citation_styles(text),\n",
    "            \"recommendations\": self._get_citation_recommendations(len(citations), len(text.split()))\n",
    "        }\n",
    "    \n",
    "    async def _analyze_vocabulary(self, text: str) -> Dict:\n",
    "        \"\"\"Analyze vocabulary sophistication\"\"\"\n",
    "        words = re.findall(r'\\b[a-zA-Z]+\\b', text.lower())\n",
    "        unique_words = set(words)\n",
    "        \n",
    "        # Academic vocabulary indicators\n",
    "        academic_words = [\"analyze\", \"evaluate\", \"synthesize\", \"demonstrate\", \"establish\", \n",
    "                         \"methodology\", \"framework\", \"paradigm\", \"hypothesis\", \"empirical\"]\n",
    "        \n",
    "        academic_count = sum(1 for word in words if word in academic_words)\n",
    "        \n",
    "        return {\n",
    "            \"total_words\": len(words),\n",
    "            \"unique_words\": len(unique_words),\n",
    "            \"lexical_diversity\": len(unique_words) / len(words) if words else 0,\n",
    "            \"academic_vocabulary_ratio\": academic_count / len(words) if words else 0,\n",
    "            \"recommendations\": self._get_vocabulary_recommendations(academic_count / len(words) if words else 0)\n",
    "        }\n",
    "    \n",
    "    async def _analyze_coherence(self, text: str) -> float:\n",
    "        \"\"\"Analyze text coherence\"\"\"\n",
    "        paragraphs = [p.strip() for p in text.split('\\n\\n') if p.strip()]\n",
    "        \n",
    "        # Simple coherence scoring based on transition words\n",
    "        transition_words = [\"however\", \"furthermore\", \"moreover\", \"therefore\", \"consequently\", \n",
    "                           \"in addition\", \"similarly\", \"in contrast\", \"for example\", \"specifically\"]\n",
    "        \n",
    "        transition_count = sum(1 for word in transition_words if word in text.lower())\n",
    "        coherence_score = min(transition_count / len(paragraphs) if paragraphs else 0, 1.0)\n",
    "        \n",
    "        return coherence_score\n",
    "    \n",
    "    def _extract_key_concepts(self, text: str) -> List[str]:\n",
    "        \"\"\"Extract key concepts from text\"\"\"\n",
    "        # Simple keyword extraction\n",
    "        words = re.findall(r'\\b[a-zA-Z]+\\b', text.lower())\n",
    "        \n",
    "        # Filter out common words\n",
    "        stop_words = {'the', 'a', 'an', 'and', 'or', 'but', 'in', 'on', 'at', 'to', 'for', \n",
    "                     'of', 'with', 'by', 'is', 'are', 'was', 'were', 'be', 'been', 'being'}\n",
    "        \n",
    "        filtered_words = [word for word in words if word not in stop_words and len(word) > 3]\n",
    "        word_freq = Counter(filtered_words)\n",
    "        \n",
    "        return [word for word, count in word_freq.most_common(10)]\n",
    "    \n",
    "    def _calculate_relevance(self, concept: str, paper: Dict) -> float:\n",
    "        \"\"\"Calculate relevance score between concept and paper\"\"\"\n",
    "        title = paper.get(\"title\", \"\").lower()\n",
    "        abstract = paper.get(\"abstract\", \"\").lower()\n",
    "        \n",
    "        concept_lower = concept.lower()\n",
    "        \n",
    "        score = 0\n",
    "        if concept_lower in title:\n",
    "            score += 0.5\n",
    "        if concept_lower in abstract:\n",
    "            score += 0.3\n",
    "        \n",
    "        # Add citation count boost\n",
    "        citations = paper.get(\"citation_count\", 0)\n",
    "        score += min(citations / 1000, 0.2)\n",
    "        \n",
    "        return score\n",
    "    \n",
    "    def _extract_year_from_date(self, date_str) -> Optional[str]:\n",
    "        \"\"\"Extract year from date string\"\"\"\n",
    "        if not date_str:\n",
    "            return None\n",
    "        \n",
    "        try:\n",
    "            if isinstance(date_str, datetime):\n",
    "                return str(date_str.year)\n",
    "            elif isinstance(date_str, str):\n",
    "                year_match = re.search(r'\\b(19|20)\\d{2}\\b', date_str)\n",
    "                if year_match:\n",
    "                    return year_match.group()\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        return None\n",
    "    \n",
    "    # Helper methods for analysis\n",
    "    def _get_readability_recommendations(self, score: float) -> List[str]:\n",
    "        recommendations = []\n",
    "        if score < 30:\n",
    "            recommendations.extend([\n",
    "                \"Shorten sentence length\",\n",
    "                \"Use simpler vocabulary where appropriate\",\n",
    "                \"Break complex sentences into smaller ones\"\n",
    "            ])\n",
    "        elif score < 50:\n",
    "            recommendations.append(\"Consider simplifying some complex sentences\")\n",
    "        return recommendations\n",
    "    \n",
    "    def _get_grammar_suggestion(self, issue_type: str, text: str) -> str:\n",
    "        suggestions = {\n",
    "            \"passive_voice\": \"Consider using active voice\",\n",
    "            \"wordiness\": \"Use more concise phrasing\",\n",
    "            \"weak_verbs\": \"Use stronger, more specific verbs\",\n",
    "            \"redundancy\": \"Remove redundant words\",\n",
    "            \"first_person\": \"Consider using third person in academic writing\"\n",
    "        }\n",
    "        return suggestions.get(issue_type, \"Review this phrase\")\n",
    "    \n",
    "    def _check_topic_sentences(self, paragraphs: List[str]) -> bool:\n",
    "        \"\"\"Check if paragraphs have clear topic sentences\"\"\"\n",
    "        # Simplified check - look for first sentences that introduce topics\n",
    "        return len(paragraphs) > 0  # Placeholder implementation\n",
    "    \n",
    "    def _find_coherence_signals(self, text: str) -> List[str]:\n",
    "        \"\"\"Find coherence and transition signals\"\"\"\n",
    "        signals = [\"however\", \"furthermore\", \"moreover\", \"therefore\", \"in addition\"]\n",
    "        found = [signal for signal in signals if signal in text.lower()]\n",
    "        return found\n",
    "    \n",
    "    def _identify_citation_styles(self, text: str) -> List[str]:\n",
    "        \"\"\"Identify citation styles used\"\"\"\n",
    "        styles = []\n",
    "        if re.search(r'\\[(\\d+)\\]', text):\n",
    "            styles.append(\"IEEE/Numeric\")\n",
    "        if re.search(r'\\(([^)]+,\\s*\\d{4})\\)', text):\n",
    "            styles.append(\"APA/Author-Date\")\n",
    "        return styles\n",
    "    \n",
    "    def _get_citation_recommendations(self, citation_count: int, word_count: int) -> List[str]:\n",
    "        \"\"\"Get citation recommendations\"\"\"\n",
    "        recommendations = []\n",
    "        if citation_count == 0:\n",
    "            recommendations.append(\"Consider adding citations to support your claims\")\n",
    "        elif word_count > 1000 and citation_count < 5:\n",
    "            recommendations.append(\"For a longer text, consider adding more citations\")\n",
    "        return recommendations\n",
    "    \n",
    "    def _get_vocabulary_recommendations(self, academic_ratio: float) -> List[str]:\n",
    "        \"\"\"Get vocabulary recommendations\"\"\"\n",
    "        recommendations = []\n",
    "        if academic_ratio < 0.05:\n",
    "            recommendations.append(\"Consider using more academic/technical vocabulary\")\n",
    "        elif academic_ratio > 0.2:\n",
    "            recommendations.append(\"Ensure vocabulary is accessible to your target audience\")\n",
    "        return recommendations\n",
    "    \n",
    "    async def _improve_grammar(self, text: str) -> Dict:\n",
    "        \"\"\"Improve grammar in text\"\"\"\n",
    "        # Simple grammar improvements\n",
    "        improved = text\n",
    "        changes = []\n",
    "        \n",
    "        # Fix common issues\n",
    "        replacements = {\n",
    "            r'\\bit is important to note that\\b': '',\n",
    "            r'\\bdue to the fact that\\b': 'because',\n",
    "            r'\\bin order to\\b': 'to',\n",
    "            r'\\bthe reason is because\\b': 'because',\n",
    "        }\n",
    "        \n",
    "        for pattern, replacement in replacements.items():\n",
    "            new_text = re.sub(pattern, replacement, improved, flags=re.IGNORECASE)\n",
    "            if new_text != improved:\n",
    "                changes.append(f\"Simplified wordy phrase: '{pattern}' ‚Üí '{replacement}'\")\n",
    "                improved = new_text\n",
    "        \n",
    "        return {\"text\": improved, \"changes\": changes}\n",
    "    \n",
    "    async def _improve_style(self, text: str) -> Dict:\n",
    "        \"\"\"Improve writing style\"\"\"\n",
    "        improved = text\n",
    "        changes = []\n",
    "        \n",
    "        # Remove contractions\n",
    "        contractions = {\n",
    "            r\"\\bcan't\\b\": \"cannot\",\n",
    "            r\"\\bwon't\\b\": \"will not\", \n",
    "            r\"\\bdon't\\b\": \"do not\",\n",
    "            r\"\\bisn't\\b\": \"is not\"\n",
    "        }\n",
    "        \n",
    "        for contraction, expansion in contractions.items():\n",
    "            new_text = re.sub(contraction, expansion, improved, flags=re.IGNORECASE)\n",
    "            if new_text != improved:\n",
    "                changes.append(f\"Expanded contraction: {contraction} ‚Üí {expansion}\")\n",
    "                improved = new_text\n",
    "        \n",
    "        return {\"text\": improved, \"changes\": changes}\n",
    "    \n",
    "    async def _improve_clarity(self, text: str) -> Dict:\n",
    "        \"\"\"Improve text clarity\"\"\"\n",
    "        improved = text\n",
    "        changes = []\n",
    "        \n",
    "        # Improve word choice\n",
    "        replacements = {\n",
    "            r'\\ba lot of\\b': 'many',\n",
    "            r'\\bbig\\b': 'significant',\n",
    "            r'\\bsmall\\b': 'minor',\n",
    "            r'\\bthing\\b': 'element',\n",
    "            r'\\bstuff\\b': 'material'\n",
    "        }\n",
    "        \n",
    "        for informal, formal in replacements.items():\n",
    "            new_text = re.sub(informal, formal, improved, flags=re.IGNORECASE)\n",
    "            if new_text != improved:\n",
    "                changes.append(f\"Improved word choice: {informal} ‚Üí {formal}\")\n",
    "                improved = new_text\n",
    "        \n",
    "        return {\"text\": improved, \"changes\": changes}\n",
    "\n",
    "# Initialize writing assistant service\n",
    "writing_assistant = WritingAssistantService()\n",
    "\n",
    "print(\"‚úçÔ∏è Writing Assistant Service initialized!\")\n",
    "print(\"üìù Features: Grammar check, style analysis, readability scoring\")\n",
    "print(\"üìö Templates: Abstract, Introduction, Methodology, Conclusion\")\n",
    "print(\"üîó Citation: Suggestion and analysis capabilities\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06105b85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Advanced Service 3: Citation and Reference Generator\n",
    "class CitationService:\n",
    "    \"\"\"Service for generating citations and managing references\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.citation_styles = {\n",
    "            \"APA\": self._format_apa,\n",
    "            \"MLA\": self._format_mla,\n",
    "            \"IEEE\": self._format_ieee,\n",
    "            \"Chicago\": self._format_chicago,\n",
    "            \"Harvard\": self._format_harvard\n",
    "        }\n",
    "    \n",
    "    async def generate_citation(self, paper_data: Dict, style: str = \"APA\") -> str:\n",
    "        \"\"\"Generate citation in specified style\"\"\"\n",
    "        if style not in self.citation_styles:\n",
    "            raise ValueError(f\"Unsupported citation style: {style}\")\n",
    "        \n",
    "        formatter = self.citation_styles[style]\n",
    "        return formatter(paper_data)\n",
    "    \n",
    "    async def generate_bibliography(self, papers: List[Dict], style: str = \"APA\") -> str:\n",
    "        \"\"\"Generate bibliography for multiple papers\"\"\"\n",
    "        citations = []\n",
    "        for paper in papers:\n",
    "            try:\n",
    "                citation = await self.generate_citation(paper, style)\n",
    "                citations.append(citation)\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Citation generation error: {str(e)}\")\n",
    "                continue\n",
    "        \n",
    "        # Sort citations alphabetically (most styles)\n",
    "        if style in [\"APA\", \"MLA\", \"Harvard\"]:\n",
    "            citations.sort()\n",
    "        \n",
    "        return \"\\n\\n\".join(citations)\n",
    "    \n",
    "    async def parse_reference_text(self, reference_text: str) -> Dict:\n",
    "        \"\"\"Parse reference text to extract structured data\"\"\"\n",
    "        # Simple reference parsing\n",
    "        parsed = {\n",
    "            \"title\": \"\",\n",
    "            \"authors\": [],\n",
    "            \"year\": \"\",\n",
    "            \"journal\": \"\",\n",
    "            \"doi\": \"\",\n",
    "            \"confidence\": 0.0\n",
    "        }\n",
    "        \n",
    "        # Extract year\n",
    "        year_match = re.search(r'\\b(19|20)\\d{2}\\b', reference_text)\n",
    "        if year_match:\n",
    "            parsed[\"year\"] = year_match.group()\n",
    "            parsed[\"confidence\"] += 0.2\n",
    "        \n",
    "        # Extract DOI\n",
    "        doi_match = re.search(r'doi:\\s*(10\\.\\d+/[^\\s]+)', reference_text, re.IGNORECASE)\n",
    "        if doi_match:\n",
    "            parsed[\"doi\"] = doi_match.group(1)\n",
    "            parsed[\"confidence\"] += 0.3\n",
    "        \n",
    "        # Extract title (usually in quotes or italics indicators)\n",
    "        title_patterns = [\n",
    "            r'\"([^\"]+)\"',  # Quoted title\n",
    "            r'\\'([^\\']+)\\'',  # Single quoted title\n",
    "            r'\\. ([A-Z][^.]+)\\.',  # Title after period\n",
    "        ]\n",
    "        \n",
    "        for pattern in title_patterns:\n",
    "            title_match = re.search(pattern, reference_text)\n",
    "            if title_match:\n",
    "                parsed[\"title\"] = title_match.group(1).strip()\n",
    "                parsed[\"confidence\"] += 0.3\n",
    "                break\n",
    "        \n",
    "        # Extract authors (simplified)\n",
    "        # Look for patterns like \"LastName, F. M.\" or \"F. M. LastName\"\n",
    "        author_pattern = r'\\b([A-Z][a-z]+,\\s+[A-Z]\\.(?:\\s*[A-Z]\\.)?|\\b[A-Z]\\.\\s*[A-Z]\\.\\s+[A-Z][a-z]+)'\n",
    "        authors = re.findall(author_pattern, reference_text)\n",
    "        if authors:\n",
    "            parsed[\"authors\"] = authors\n",
    "            parsed[\"confidence\"] += 0.2\n",
    "        \n",
    "        return parsed\n",
    "    \n",
    "    async def validate_citations(self, text: str) -> Dict:\n",
    "        \"\"\"Validate citations in text\"\"\"\n",
    "        validation_results = {\n",
    "            \"total_citations\": 0,\n",
    "            \"valid_citations\": 0,\n",
    "            \"invalid_citations\": [],\n",
    "            \"missing_references\": [],\n",
    "            \"formatting_issues\": [],\n",
    "            \"style_consistency\": True\n",
    "        }\n",
    "        \n",
    "        # Find citations in text\n",
    "        citation_patterns = {\n",
    "            \"APA\": r'\\(([^)]+,\\s*\\d{4}[a-z]?)\\)',\n",
    "            \"IEEE\": r'\\[(\\d+)\\]',\n",
    "            \"MLA\": r'\\(([^)]+\\s+\\d+)\\)',\n",
    "        }\n",
    "        \n",
    "        found_citations = []\n",
    "        detected_style = None\n",
    "        \n",
    "        for style, pattern in citation_patterns.items():\n",
    "            matches = re.findall(pattern, text)\n",
    "            if matches:\n",
    "                found_citations.extend(matches)\n",
    "                if not detected_style:\n",
    "                    detected_style = style\n",
    "                elif detected_style != style:\n",
    "                    validation_results[\"style_consistency\"] = False\n",
    "        \n",
    "        validation_results[\"total_citations\"] = len(found_citations)\n",
    "        validation_results[\"detected_style\"] = detected_style\n",
    "        \n",
    "        # Check for common formatting issues\n",
    "        formatting_issues = []\n",
    "        \n",
    "        # Check for missing periods\n",
    "        if re.search(r'\\([^)]+\\d{4}\\)[A-Z]', text):\n",
    "            formatting_issues.append(\"Missing period after citation\")\n",
    "        \n",
    "        # Check for spacing issues\n",
    "        if re.search(r'\\w\\(', text):\n",
    "            formatting_issues.append(\"Missing space before citation\")\n",
    "        \n",
    "        validation_results[\"formatting_issues\"] = formatting_issues\n",
    "        \n",
    "        return validation_results\n",
    "    \n",
    "    async def suggest_in_text_citations(self, text: str, papers: List[Dict]) -> List[Dict]:\n",
    "        \"\"\"Suggest where to add citations in text\"\"\"\n",
    "        suggestions = []\n",
    "        \n",
    "        # Look for statements that need citations\n",
    "        citation_needed_patterns = [\n",
    "            r'(research shows|studies indicate|according to|evidence suggests)',\n",
    "            r'(\\d+%|\\d+\\.\\d+%|significant|substantial)',\n",
    "            r'(previous work|prior research|established|demonstrated)',\n",
    "        ]\n",
    "        \n",
    "        sentences = re.split(r'[.!?]+', text)\n",
    "        \n",
    "        for i, sentence in enumerate(sentences):\n",
    "            sentence = sentence.strip()\n",
    "            if not sentence:\n",
    "                continue\n",
    "            \n",
    "            needs_citation = False\n",
    "            for pattern in citation_needed_patterns:\n",
    "                if re.search(pattern, sentence, re.IGNORECASE):\n",
    "                    needs_citation = True\n",
    "                    break\n",
    "            \n",
    "            if needs_citation:\n",
    "                # Find relevant papers\n",
    "                relevant_papers = await self._find_relevant_papers(sentence, papers)\n",
    "                if relevant_papers:\n",
    "                    suggestions.append({\n",
    "                        \"sentence\": sentence,\n",
    "                        \"position\": i,\n",
    "                        \"reason\": \"Statement needs citation support\",\n",
    "                        \"suggested_papers\": relevant_papers[:3],\n",
    "                        \"confidence\": 0.8\n",
    "                    })\n",
    "        \n",
    "        return suggestions\n",
    "    \n",
    "    async def convert_citation_style(self, text: str, from_style: str, to_style: str) -> str:\n",
    "        \"\"\"Convert citations from one style to another\"\"\"\n",
    "        if from_style == to_style:\n",
    "            return text\n",
    "        \n",
    "        # This is a simplified implementation\n",
    "        # In practice, you'd need more sophisticated parsing\n",
    "        \n",
    "        converted_text = text\n",
    "        \n",
    "        if from_style == \"IEEE\" and to_style == \"APA\":\n",
    "            # Convert [1] to (Author, Year) format\n",
    "            # This would require a database of references\n",
    "            pass\n",
    "        elif from_style == \"APA\" and to_style == \"IEEE\":\n",
    "            # Convert (Author, Year) to [1] format\n",
    "            apa_pattern = r'\\(([^)]+,\\s*\\d{4}[a-z]?)\\)'\n",
    "            matches = re.findall(apa_pattern, converted_text)\n",
    "            for i, match in enumerate(matches, 1):\n",
    "                converted_text = re.sub(f'\\\\({re.escape(match)}\\\\)', f'[{i}]', converted_text, count=1)\n",
    "        \n",
    "        return converted_text\n",
    "    \n",
    "    def _format_apa(self, paper_data: Dict) -> str:\n",
    "        \"\"\"Format citation in APA style\"\"\"\n",
    "        authors = paper_data.get(\"authors\", [])\n",
    "        title = paper_data.get(\"title\", \"\")\n",
    "        year = self._extract_year(paper_data.get(\"publication_date\"))\n",
    "        journal = paper_data.get(\"journal\", \"\")\n",
    "        doi = paper_data.get(\"doi\", \"\")\n",
    "        \n",
    "        # Format authors\n",
    "        if not authors:\n",
    "            author_str = \"Unknown Author\"\n",
    "        elif len(authors) == 1:\n",
    "            author_str = self._format_apa_author(authors[0])\n",
    "        elif len(authors) <= 20:\n",
    "            formatted_authors = [self._format_apa_author(author) for author in authors[:-1]]\n",
    "            author_str = \", \".join(formatted_authors) + f\", & {self._format_apa_author(authors[-1])}\"\n",
    "        else:\n",
    "            # More than 20 authors\n",
    "            formatted_authors = [self._format_apa_author(author) for author in authors[:19]]\n",
    "            author_str = \", \".join(formatted_authors) + f\", ... {self._format_apa_author(authors[-1])}\"\n",
    "        \n",
    "        # Build citation\n",
    "        citation = f\"{author_str} ({year or 'n.d.'}). {title}\"\n",
    "        \n",
    "        if journal:\n",
    "            citation += f\". {journal}\"\n",
    "        \n",
    "        if doi:\n",
    "            citation += f\". https://doi.org/{doi}\"\n",
    "        \n",
    "        return citation\n",
    "    \n",
    "    def _format_mla(self, paper_data: Dict) -> str:\n",
    "        \"\"\"Format citation in MLA style\"\"\"\n",
    "        authors = paper_data.get(\"authors\", [])\n",
    "        title = paper_data.get(\"title\", \"\")\n",
    "        journal = paper_data.get(\"journal\", \"\")\n",
    "        year = self._extract_year(paper_data.get(\"publication_date\"))\n",
    "        \n",
    "        # Format authors\n",
    "        if not authors:\n",
    "            author_str = \"Unknown Author\"\n",
    "        elif len(authors) == 1:\n",
    "            author_str = self._format_mla_author(authors[0])\n",
    "        elif len(authors) == 2:\n",
    "            author_str = f\"{self._format_mla_author(authors[0])}, and {authors[1]}\"\n",
    "        else:\n",
    "            author_str = f\"{self._format_mla_author(authors[0])}, et al\"\n",
    "        \n",
    "        # Build citation\n",
    "        citation = f'{author_str}. \"{title}.\"'\n",
    "        \n",
    "        if journal:\n",
    "            citation += f\" {journal},\"\n",
    "        \n",
    "        if year:\n",
    "            citation += f\" {year}.\"\n",
    "        \n",
    "        return citation\n",
    "    \n",
    "    def _format_ieee(self, paper_data: Dict) -> str:\n",
    "        \"\"\"Format citation in IEEE style\"\"\"\n",
    "        authors = paper_data.get(\"authors\", [])\n",
    "        title = paper_data.get(\"title\", \"\")\n",
    "        journal = paper_data.get(\"journal\", \"\")\n",
    "        year = self._extract_year(paper_data.get(\"publication_date\"))\n",
    "        \n",
    "        # Format authors\n",
    "        if not authors:\n",
    "            author_str = \"Unknown Author\"\n",
    "        elif len(authors) <= 6:\n",
    "            formatted_authors = [self._format_ieee_author(author) for author in authors]\n",
    "            author_str = \", \".join(formatted_authors)\n",
    "        else:\n",
    "            formatted_authors = [self._format_ieee_author(author) for author in authors[:6]]\n",
    "            author_str = \", \".join(formatted_authors) + \", et al.\"\n",
    "        \n",
    "        # Build citation\n",
    "        citation = f'{author_str}, \"{title},\"'\n",
    "        \n",
    "        if journal:\n",
    "            citation += f\" {journal},\"\n",
    "        \n",
    "        if year:\n",
    "            citation += f\" {year}.\"\n",
    "        \n",
    "        return citation\n",
    "    \n",
    "    def _format_chicago(self, paper_data: Dict) -> str:\n",
    "        \"\"\"Format citation in Chicago style\"\"\"\n",
    "        authors = paper_data.get(\"authors\", [])\n",
    "        title = paper_data.get(\"title\", \"\")\n",
    "        journal = paper_data.get(\"journal\", \"\")\n",
    "        year = self._extract_year(paper_data.get(\"publication_date\"))\n",
    "        \n",
    "        # Format authors\n",
    "        if not authors:\n",
    "            author_str = \"Unknown Author\"\n",
    "        elif len(authors) == 1:\n",
    "            author_str = authors[0]\n",
    "        elif len(authors) <= 3:\n",
    "            author_str = \", \".join(authors[:-1]) + f\", and {authors[-1]}\"\n",
    "        else:\n",
    "            author_str = f\"{authors[0]}, et al.\"\n",
    "        \n",
    "        # Build citation\n",
    "        citation = f'{author_str}. \"{title}.\"'\n",
    "        \n",
    "        if journal:\n",
    "            citation += f\" {journal}\"\n",
    "        \n",
    "        if year:\n",
    "            citation += f\" ({year}).\"\n",
    "        \n",
    "        return citation\n",
    "    \n",
    "    def _format_harvard(self, paper_data: Dict) -> str:\n",
    "        \"\"\"Format citation in Harvard style\"\"\"\n",
    "        # Harvard is similar to APA\n",
    "        return self._format_apa(paper_data)\n",
    "    \n",
    "    def _format_apa_author(self, author: str) -> str:\n",
    "        \"\"\"Format author name for APA style\"\"\"\n",
    "        # Simple name formatting\n",
    "        if \",\" in author:\n",
    "            # Assume \"Last, First\" format\n",
    "            parts = author.split(\",\", 1)\n",
    "            last_name = parts[0].strip()\n",
    "            first_name = parts[1].strip()\n",
    "            # Get initials\n",
    "            initials = \". \".join([name[0] for name in first_name.split() if name]) + \".\"\n",
    "            return f\"{last_name}, {initials}\"\n",
    "        else:\n",
    "            # Assume \"First Last\" format\n",
    "            parts = author.split()\n",
    "            if len(parts) >= 2:\n",
    "                last_name = parts[-1]\n",
    "                initials = \". \".join([name[0] for name in parts[:-1]]) + \".\"\n",
    "                return f\"{last_name}, {initials}\"\n",
    "            else:\n",
    "                return author\n",
    "    \n",
    "    def _format_mla_author(self, author: str) -> str:\n",
    "        \"\"\"Format author name for MLA style\"\"\"\n",
    "        if \",\" in author:\n",
    "            return author  # Already in \"Last, First\" format\n",
    "        else:\n",
    "            parts = author.split()\n",
    "            if len(parts) >= 2:\n",
    "                return f\"{parts[-1]}, {' '.join(parts[:-1])}\"\n",
    "            else:\n",
    "                return author\n",
    "    \n",
    "    def _format_ieee_author(self, author: str) -> str:\n",
    "        \"\"\"Format author name for IEEE style\"\"\"\n",
    "        if \",\" in author:\n",
    "            parts = author.split(\",\", 1)\n",
    "            last_name = parts[0].strip()\n",
    "            first_name = parts[1].strip()\n",
    "            initials = \". \".join([name[0] for name in first_name.split() if name]) + \".\"\n",
    "            return f\"{initials} {last_name}\"\n",
    "        else:\n",
    "            parts = author.split()\n",
    "            if len(parts) >= 2:\n",
    "                initials = \". \".join([name[0] for name in parts[:-1]]) + \".\"\n",
    "                return f\"{initials} {parts[-1]}\"\n",
    "            else:\n",
    "                return author\n",
    "    \n",
    "    def _extract_year(self, date_str) -> str:\n",
    "        \"\"\"Extract year from date string\"\"\"\n",
    "        if not date_str:\n",
    "            return \"\"\n",
    "        \n",
    "        try:\n",
    "            if isinstance(date_str, datetime):\n",
    "                return str(date_str.year)\n",
    "            elif isinstance(date_str, str):\n",
    "                year_match = re.search(r'\\b(19|20)\\d{2}\\b', date_str)\n",
    "                if year_match:\n",
    "                    return year_match.group()\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        return \"\"\n",
    "    \n",
    "    async def _find_relevant_papers(self, sentence: str, papers: List[Dict]) -> List[Dict]:\n",
    "        \"\"\"Find papers relevant to a sentence\"\"\"\n",
    "        relevant = []\n",
    "        sentence_lower = sentence.lower()\n",
    "        \n",
    "        for paper in papers:\n",
    "            relevance_score = 0\n",
    "            \n",
    "            # Check title relevance\n",
    "            title = paper.get(\"title\", \"\").lower()\n",
    "            common_words = set(sentence_lower.split()) & set(title.split())\n",
    "            relevance_score += len(common_words) * 0.3\n",
    "            \n",
    "            # Check abstract relevance\n",
    "            abstract = paper.get(\"abstract\", \"\").lower()\n",
    "            abstract_common = set(sentence_lower.split()) & set(abstract.split())\n",
    "            relevance_score += len(abstract_common) * 0.1\n",
    "            \n",
    "            if relevance_score > 0.5:\n",
    "                paper[\"relevance_score\"] = relevance_score\n",
    "                relevant.append(paper)\n",
    "        \n",
    "        relevant.sort(key=lambda x: x[\"relevance_score\"], reverse=True)\n",
    "        return relevant\n",
    "\n",
    "# Initialize citation service\n",
    "citation_generator = CitationService()\n",
    "\n",
    "print(\"üìù Citation Service initialized!\")\n",
    "print(\"üìö Styles: APA, MLA, IEEE, Chicago, Harvard\")\n",
    "print(\"üîç Features: Citation generation, validation, style conversion\")\n",
    "print(\"üí° Capabilities: In-text suggestions, bibliography generation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d67d04ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FastAPI Application with All Endpoints\n",
    "app = FastAPI(\n",
    "    title=\"Research Agent API\",\n",
    "    description=\"Comprehensive Research Assistant Backend\",\n",
    "    version=\"1.0.0\"\n",
    ")\n",
    "\n",
    "# Add CORS middleware\n",
    "app.add_middleware(\n",
    "    CORSMiddleware,\n",
    "    allow_origins=[\"*\"],\n",
    "    allow_credentials=True,\n",
    "    allow_methods=[\"*\"],\n",
    "    allow_headers=[\"*\"],\n",
    ")\n",
    "\n",
    "# Database dependency\n",
    "def get_db():\n",
    "    db = SessionLocal()\n",
    "    try:\n",
    "        yield db\n",
    "    finally:\n",
    "        db.close()\n",
    "\n",
    "# === CORE ENDPOINTS ===\n",
    "\n",
    "@app.post(\"/api/search/literature\")\n",
    "async def search_literature(query: SearchQuery):\n",
    "    \"\"\"Search literature across multiple academic databases\"\"\"\n",
    "    try:\n",
    "        results = await literature_search.unified_search(\n",
    "            query.query, \n",
    "            max_results=query.limit\n",
    "        )\n",
    "        \n",
    "        # Apply filters if provided\n",
    "        if query.filters:\n",
    "            results = await literature_search.search_with_filters(query.query, query.filters)\n",
    "        \n",
    "        return SearchResults(\n",
    "            query=query.query,\n",
    "            total_results=len(results),\n",
    "            results=[PaperResponse(**paper) for paper in results],\n",
    "            took_ms=100  # Placeholder\n",
    "        )\n",
    "    except Exception as e:\n",
    "        raise HTTPException(status_code=500, detail=str(e))\n",
    "\n",
    "@app.get(\"/api/researchers/search/{topic}\")\n",
    "async def search_researchers(topic: str, limit: int = 20):\n",
    "    \"\"\"Find researchers in a specific topic\"\"\"\n",
    "    try:\n",
    "        researchers = await researcher_discovery.search_researchers_by_topic(topic, limit)\n",
    "        return {\"researchers\": researchers}\n",
    "    except Exception as e:\n",
    "        raise HTTPException(status_code=500, detail=str(e))\n",
    "\n",
    "@app.get(\"/api/researchers/{researcher_id}\")\n",
    "async def get_researcher_profile(researcher_id: str):\n",
    "    \"\"\"Get detailed researcher profile\"\"\"\n",
    "    try:\n",
    "        profile = await researcher_discovery.get_researcher_profile(researcher_id)\n",
    "        if not profile:\n",
    "            raise HTTPException(status_code=404, detail=\"Researcher not found\")\n",
    "        return profile\n",
    "    except Exception as e:\n",
    "        raise HTTPException(status_code=500, detail=str(e))\n",
    "\n",
    "@app.post(\"/api/papers/analyze\")\n",
    "async def analyze_paper(paper_data: PaperCreate):\n",
    "    \"\"\"Analyze a research paper\"\"\"\n",
    "    try:\n",
    "        analysis = await paper_analysis.analyze_paper(paper_data.dict())\n",
    "        return analysis\n",
    "    except Exception as e:\n",
    "        raise HTTPException(status_code=500, detail=str(e))\n",
    "\n",
    "@app.post(\"/api/papers/compare\")\n",
    "async def compare_papers(papers: List[PaperCreate]):\n",
    "    \"\"\"Compare multiple papers\"\"\"\n",
    "    try:\n",
    "        if len(papers) < 2:\n",
    "            raise HTTPException(status_code=400, detail=\"Need at least 2 papers for comparison\")\n",
    "        \n",
    "        paper_dicts = [paper.dict() for paper in papers]\n",
    "        comparison = await paper_analysis.compare_papers(paper_dicts)\n",
    "        return comparison\n",
    "    except Exception as e:\n",
    "        raise HTTPException(status_code=500, detail=str(e))\n",
    "\n",
    "@app.get(\"/api/trends/{field}\")\n",
    "async def analyze_trends(field: str, time_range: str = \"5_years\"):\n",
    "    \"\"\"Analyze trends in a research field\"\"\"\n",
    "    try:\n",
    "        trends = await trend_analysis.analyze_field_trends(field, time_range)\n",
    "        return trends\n",
    "    except Exception as e:\n",
    "        raise HTTPException(status_code=500, detail=str(e))\n",
    "\n",
    "# === ADVANCED ENDPOINTS ===\n",
    "\n",
    "@app.get(\"/api/research-gaps/{field}\")\n",
    "async def find_research_gaps(field: str, depth: str = \"comprehensive\"):\n",
    "    \"\"\"Find research gaps in a field\"\"\"\n",
    "    try:\n",
    "        gaps = await research_gap_finder.find_research_gaps(field, depth)\n",
    "        return {\"field\": field, \"gaps\": gaps}\n",
    "    except Exception as e:\n",
    "        raise HTTPException(status_code=500, detail=str(e))\n",
    "\n",
    "@app.post(\"/api/writing/analyze\")\n",
    "async def analyze_writing(request: WritingAssistance):\n",
    "    \"\"\"Analyze academic writing\"\"\"\n",
    "    try:\n",
    "        analysis = await writing_assistant.analyze_writing(request.text, request.context)\n",
    "        return analysis\n",
    "    except Exception as e:\n",
    "        raise HTTPException(status_code=500, detail=str(e))\n",
    "\n",
    "@app.post(\"/api/writing/improve\")\n",
    "async def improve_writing(request: WritingAssistance):\n",
    "    \"\"\"Improve academic writing\"\"\"\n",
    "    try:\n",
    "        improvements = await writing_assistant.improve_text(request.text, request.assistance_type)\n",
    "        return improvements\n",
    "    except Exception as e:\n",
    "        raise HTTPException(status_code=500, detail=str(e))\n",
    "\n",
    "@app.get(\"/api/writing/templates/{template_type}\")\n",
    "async def get_writing_template(template_type: str):\n",
    "    \"\"\"Get academic writing templates\"\"\"\n",
    "    try:\n",
    "        template = await writing_assistant.generate_academic_templates(template_type)\n",
    "        return template\n",
    "    except Exception as e:\n",
    "        raise HTTPException(status_code=500, detail=str(e))\n",
    "\n",
    "@app.post(\"/api/citations/generate\")\n",
    "async def generate_citation(paper_data: PaperCreate, style: str = \"APA\"):\n",
    "    \"\"\"Generate citation for a paper\"\"\"\n",
    "    try:\n",
    "        citation = await citation_generator.generate_citation(paper_data.dict(), style)\n",
    "        return {\"citation\": citation, \"style\": style}\n",
    "    except Exception as e:\n",
    "        raise HTTPException(status_code=500, detail=str(e))\n",
    "\n",
    "@app.post(\"/api/citations/bibliography\")\n",
    "async def generate_bibliography(papers: List[PaperCreate], style: str = \"APA\"):\n",
    "    \"\"\"Generate bibliography for multiple papers\"\"\"\n",
    "    try:\n",
    "        paper_dicts = [paper.dict() for paper in papers]\n",
    "        bibliography = await citation_generator.generate_bibliography(paper_dicts, style)\n",
    "        return {\"bibliography\": bibliography, \"style\": style, \"count\": len(papers)}\n",
    "    except Exception as e:\n",
    "        raise HTTPException(status_code=500, detail=str(e))\n",
    "\n",
    "@app.post(\"/api/citations/validate\")\n",
    "async def validate_citations(text: str):\n",
    "    \"\"\"Validate citations in text\"\"\"\n",
    "    try:\n",
    "        validation = await citation_generator.validate_citations(text)\n",
    "        return validation\n",
    "    except Exception as e:\n",
    "        raise HTTPException(status_code=500, detail=str(e))\n",
    "\n",
    "# === USER LIBRARY ENDPOINTS ===\n",
    "\n",
    "@app.post(\"/api/library/add\")\n",
    "async def add_to_library(user_id: str, paper_id: int, tags: List[str] = [], db: Session = Depends(get_db)):\n",
    "    \"\"\"Add paper to user's library\"\"\"\n",
    "    try:\n",
    "        library_entry = UserLibrary(\n",
    "            user_id=user_id,\n",
    "            paper_id=paper_id,\n",
    "            tags=json.dumps(tags),\n",
    "            reading_status=\"to_read\"\n",
    "        )\n",
    "        db.add(library_entry)\n",
    "        db.commit()\n",
    "        return {\"message\": \"Paper added to library\", \"id\": library_entry.id}\n",
    "    except Exception as e:\n",
    "        db.rollback()\n",
    "        raise HTTPException(status_code=500, detail=str(e))\n",
    "\n",
    "@app.get(\"/api/library/{user_id}\")\n",
    "async def get_user_library(user_id: str, db: Session = Depends(get_db)):\n",
    "    \"\"\"Get user's paper library\"\"\"\n",
    "    try:\n",
    "        library = db.query(UserLibrary).filter(UserLibrary.user_id == user_id).all()\n",
    "        return {\"library\": library}\n",
    "    except Exception as e:\n",
    "        raise HTTPException(status_code=500, detail=str(e))\n",
    "\n",
    "@app.put(\"/api/library/{entry_id}/status\")\n",
    "async def update_reading_status(entry_id: int, status: str, db: Session = Depends(get_db)):\n",
    "    \"\"\"Update reading status of a library entry\"\"\"\n",
    "    try:\n",
    "        entry = db.query(UserLibrary).filter(UserLibrary.id == entry_id).first()\n",
    "        if not entry:\n",
    "            raise HTTPException(status_code=404, detail=\"Library entry not found\")\n",
    "        \n",
    "        entry.reading_status = status\n",
    "        db.commit()\n",
    "        return {\"message\": \"Status updated\", \"new_status\": status}\n",
    "    except Exception as e:\n",
    "        db.rollback()\n",
    "        raise HTTPException(status_code=500, detail=str(e))\n",
    "\n",
    "# === CUTTING-EDGE FEATURES ===\n",
    "\n",
    "@app.post(\"/api/agent/explore\")\n",
    "async def autonomous_exploration(topic: str, depth: int = 3):\n",
    "    \"\"\"Autonomous exploration of a research topic\"\"\"\n",
    "    try:\n",
    "        # Multi-step exploration\n",
    "        exploration_results = {\n",
    "            \"topic\": topic,\n",
    "            \"steps\": [],\n",
    "            \"discoveries\": [],\n",
    "            \"recommendations\": []\n",
    "        }\n",
    "        \n",
    "        # Step 1: Initial literature search\n",
    "        papers = await literature_search.unified_search(topic, max_results=50)\n",
    "        exploration_results[\"steps\"].append(f\"Found {len(papers)} papers on {topic}\")\n",
    "        \n",
    "        # Step 2: Identify key researchers\n",
    "        researchers = await researcher_discovery.search_researchers_by_topic(topic, max_results=10)\n",
    "        exploration_results[\"steps\"].append(f\"Identified {len(researchers)} key researchers\")\n",
    "        \n",
    "        # Step 3: Trend analysis\n",
    "        trends = await trend_analysis.analyze_field_trends(topic)\n",
    "        exploration_results[\"steps\"].append(\"Analyzed field trends\")\n",
    "        \n",
    "        # Step 4: Gap analysis\n",
    "        gaps = await research_gap_finder.find_research_gaps(topic)\n",
    "        exploration_results[\"steps\"].append(f\"Found {len(gaps)} research gaps\")\n",
    "        \n",
    "        # Compile discoveries\n",
    "        exploration_results[\"discoveries\"] = {\n",
    "            \"top_papers\": papers[:5],\n",
    "            \"trending_topics\": trends.get(\"trending_topics\", [])[:3],\n",
    "            \"key_researchers\": researchers[:3],\n",
    "            \"research_gaps\": gaps[:3]\n",
    "        }\n",
    "        \n",
    "        # Generate recommendations\n",
    "        exploration_results[\"recommendations\"] = [\n",
    "            f\"Focus on trending topic: {trends.get('trending_topics', [{}])[0].get('description', 'N/A')}\",\n",
    "            f\"Collaborate with: {researchers[0].get('name', 'N/A') if researchers else 'N/A'}\",\n",
    "            f\"Address research gap: {gaps[0].get('gap_description', 'N/A') if gaps else 'N/A'}\"\n",
    "        ]\n",
    "        \n",
    "        return exploration_results\n",
    "    except Exception as e:\n",
    "        raise HTTPException(status_code=500, detail=str(e))\n",
    "\n",
    "@app.post(\"/api/funding/discover\")\n",
    "async def discover_funding(research_area: str, location: str = \"global\"):\n",
    "    \"\"\"Discover funding opportunities (placeholder implementation)\"\"\"\n",
    "    try:\n",
    "        # This would integrate with funding databases\n",
    "        funding_opportunities = [\n",
    "            {\n",
    "                \"title\": f\"NSF Grant for {research_area} Research\",\n",
    "                \"agency\": \"National Science Foundation\",\n",
    "                \"amount\": \"$500,000\",\n",
    "                \"deadline\": \"2024-03-15\",\n",
    "                \"eligibility\": \"Academic institutions\",\n",
    "                \"match_score\": 0.85\n",
    "            },\n",
    "            {\n",
    "                \"title\": f\"EU Horizon Grant - {research_area}\",\n",
    "                \"agency\": \"European Commission\",\n",
    "                \"amount\": \"‚Ç¨750,000\",\n",
    "                \"deadline\": \"2024-04-20\",\n",
    "                \"eligibility\": \"EU researchers\",\n",
    "                \"match_score\": 0.72\n",
    "            }\n",
    "        ]\n",
    "        return {\"opportunities\": funding_opportunities}\n",
    "    except Exception as e:\n",
    "        raise HTTPException(status_code=500, detail=str(e))\n",
    "\n",
    "@app.get(\"/api/conferences/{field}\")\n",
    "async def find_conferences(field: str, year: int = 2024):\n",
    "    \"\"\"Find relevant conferences (placeholder implementation)\"\"\"\n",
    "    try:\n",
    "        conferences = [\n",
    "            {\n",
    "                \"name\": f\"International Conference on {field}\",\n",
    "                \"date\": \"2024-06-15\",\n",
    "                \"location\": \"New York, USA\",\n",
    "                \"deadline\": \"2024-02-15\",\n",
    "                \"acceptance_rate\": \"25%\",\n",
    "                \"ranking\": \"A*\"\n",
    "            },\n",
    "            {\n",
    "                \"name\": f\"{field} Symposium\",\n",
    "                \"date\": \"2024-09-20\",\n",
    "                \"location\": \"London, UK\",\n",
    "                \"deadline\": \"2024-05-01\",\n",
    "                \"acceptance_rate\": \"35%\",\n",
    "                \"ranking\": \"A\"\n",
    "            }\n",
    "        ]\n",
    "        return {\"conferences\": conferences}\n",
    "    except Exception as e:\n",
    "        raise HTTPException(status_code=500, detail=str(e))\n",
    "\n",
    "# === UTILITY ENDPOINTS ===\n",
    "\n",
    "@app.get(\"/api/health\")\n",
    "async def health_check():\n",
    "    \"\"\"Health check endpoint\"\"\"\n",
    "    return {\"status\": \"healthy\", \"timestamp\": datetime.utcnow()}\n",
    "\n",
    "@app.get(\"/api/stats\")\n",
    "async def get_system_stats(db: Session = Depends(get_db)):\n",
    "    \"\"\"Get system statistics\"\"\"\n",
    "    try:\n",
    "        stats = {\n",
    "            \"total_papers\": db.query(Paper).count(),\n",
    "            \"total_researchers\": db.query(Researcher).count(),\n",
    "            \"total_library_entries\": db.query(UserLibrary).count(),\n",
    "            \"total_projects\": db.query(ResearchProject).count()\n",
    "        }\n",
    "        return stats\n",
    "    except Exception as e:\n",
    "        raise HTTPException(status_code=500, detail=str(e))\n",
    "\n",
    "# === BACKGROUND TASKS ===\n",
    "\n",
    "@app.post(\"/api/tasks/update-citations\")\n",
    "async def update_citation_counts(background_tasks: BackgroundTasks):\n",
    "    \"\"\"Update citation counts for all papers\"\"\"\n",
    "    background_tasks.add_task(update_citations_background)\n",
    "    return {\"message\": \"Citation update task started\"}\n",
    "\n",
    "async def update_citations_background():\n",
    "    \"\"\"Background task to update citation counts\"\"\"\n",
    "    logger.info(\"Starting citation count update...\")\n",
    "    # Implementation would update citation counts from external APIs\n",
    "    logger.info(\"Citation count update completed\")\n",
    "\n",
    "# === ERROR HANDLERS ===\n",
    "\n",
    "@app.exception_handler(HTTPException)\n",
    "async def http_exception_handler(request, exc):\n",
    "    return {\"error\": exc.detail, \"status_code\": exc.status_code}\n",
    "\n",
    "@app.exception_handler(Exception)\n",
    "async def general_exception_handler(request, exc):\n",
    "    logger.error(f\"Unhandled exception: {str(exc)}\")\n",
    "    return {\"error\": \"Internal server error\", \"status_code\": 500}\n",
    "\n",
    "print(\"üöÄ FastAPI Application configured!\")\n",
    "print(\"üì° Endpoints: Literature search, researcher discovery, trend analysis\")\n",
    "print(\"üîß Advanced: Writing assistance, citation generation, research gaps\")\n",
    "print(\"ü§ñ Cutting-edge: Autonomous exploration, funding discovery, conferences\")\n",
    "print(\"üíæ Database: User libraries, research projects, paper management\")\n",
    "print(\"\\nüåê To run the server, use: uvicorn main:app --reload\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bd673fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Additional Cutting-Edge Services\n",
    "\n",
    "class MultiModalProcessor:\n",
    "    \"\"\"Service for processing multi-modal research content (text, images, tables)\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.supported_formats = [\"pdf\", \"docx\", \"txt\", \"png\", \"jpg\", \"csv\"]\n",
    "    \n",
    "    async def process_document(self, file_path: str) -> Dict:\n",
    "        \"\"\"Process multi-modal document and extract structured information\"\"\"\n",
    "        file_extension = file_path.split('.')[-1].lower()\n",
    "        \n",
    "        if file_extension == \"pdf\":\n",
    "            return await self._process_pdf(file_path)\n",
    "        elif file_extension in [\"png\", \"jpg\", \"jpeg\"]:\n",
    "            return await self._process_image(file_path)\n",
    "        elif file_extension == \"csv\":\n",
    "            return await self._process_table(file_path)\n",
    "        else:\n",
    "            return {\"error\": f\"Unsupported file format: {file_extension}\"}\n",
    "    \n",
    "    async def _process_pdf(self, file_path: str) -> Dict:\n",
    "        \"\"\"Extract text, figures, and tables from PDF\"\"\"\n",
    "        # Placeholder implementation\n",
    "        return {\n",
    "            \"content_type\": \"pdf\",\n",
    "            \"extracted_text\": \"Sample extracted text from PDF...\",\n",
    "            \"figures\": [\"Figure 1: Sample chart\", \"Figure 2: Diagram\"],\n",
    "            \"tables\": [\"Table 1: Results summary\"],\n",
    "            \"metadata\": {\"pages\": 10, \"authors\": [\"Author 1\", \"Author 2\"]}\n",
    "        }\n",
    "    \n",
    "    async def _process_image(self, file_path: str) -> Dict:\n",
    "        \"\"\"Extract information from research figures/charts\"\"\"\n",
    "        # Would use OCR and ML models for chart/graph analysis\n",
    "        return {\n",
    "            \"content_type\": \"image\",\n",
    "            \"extracted_text\": \"Chart showing performance metrics...\",\n",
    "            \"chart_type\": \"bar_chart\",\n",
    "            \"data_points\": [{\"x\": \"Method A\", \"y\": 85}, {\"x\": \"Method B\", \"y\": 92}],\n",
    "            \"insights\": [\"Method B outperforms Method A by 7%\"]\n",
    "        }\n",
    "    \n",
    "    async def _process_table(self, file_path: str) -> Dict:\n",
    "        \"\"\"Process and analyze tabular data\"\"\"\n",
    "        # Would analyze CSV data for patterns and insights\n",
    "        return {\n",
    "            \"content_type\": \"table\",\n",
    "            \"rows\": 100,\n",
    "            \"columns\": 5,\n",
    "            \"summary_stats\": {\"mean\": 75.2, \"std\": 12.1},\n",
    "            \"insights\": [\"Strong correlation between variables X and Y\"]\n",
    "        }\n",
    "\n",
    "class CollaborationEngine:\n",
    "    \"\"\"Service for facilitating research collaboration\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.collaboration_types = [\"co-authorship\", \"data_sharing\", \"methodology\", \"review\"]\n",
    "    \n",
    "    async def find_collaborators(self, user_profile: Dict, project_description: str) -> List[Dict]:\n",
    "        \"\"\"Find potential collaborators based on research interests and expertise\"\"\"\n",
    "        \n",
    "        # Extract key topics from project description\n",
    "        key_topics = self._extract_topics(project_description)\n",
    "        \n",
    "        # Search for researchers with complementary skills\n",
    "        potential_collaborators = []\n",
    "        for topic in key_topics:\n",
    "            researchers = await researcher_discovery.search_researchers_by_topic(topic, max_results=5)\n",
    "            for researcher in researchers:\n",
    "                collaboration_score = self._calculate_collaboration_score(\n",
    "                    user_profile, researcher, project_description\n",
    "                )\n",
    "                \n",
    "                if collaboration_score > 0.6:  # Threshold for good match\n",
    "                    potential_collaborators.append({\n",
    "                        \"researcher\": researcher,\n",
    "                        \"collaboration_score\": collaboration_score,\n",
    "                        \"shared_interests\": self._find_shared_interests(user_profile, researcher),\n",
    "                        \"suggested_role\": self._suggest_collaboration_role(researcher, project_description)\n",
    "                    })\n",
    "        \n",
    "        # Sort by collaboration score\n",
    "        potential_collaborators.sort(key=lambda x: x[\"collaboration_score\"], reverse=True)\n",
    "        return potential_collaborators[:10]\n",
    "    \n",
    "    async def suggest_collaboration_opportunities(self, researchers: List[str]) -> List[Dict]:\n",
    "        \"\"\"Suggest collaboration opportunities for a group of researchers\"\"\"\n",
    "        opportunities = []\n",
    "        \n",
    "        # Analyze research profiles to find synergies\n",
    "        profiles = {}\n",
    "        for researcher_id in researchers:\n",
    "            profile = await researcher_discovery.get_researcher_profile(researcher_id)\n",
    "            if profile:\n",
    "                profiles[researcher_id] = profile\n",
    "        \n",
    "        # Find complementary expertise\n",
    "        for i, (id1, profile1) in enumerate(profiles.items()):\n",
    "            for id2, profile2 in list(profiles.items())[i+1:]:\n",
    "                synergy = self._analyze_research_synergy(profile1, profile2)\n",
    "                if synergy[\"score\"] > 0.7:\n",
    "                    opportunities.append({\n",
    "                        \"researchers\": [id1, id2],\n",
    "                        \"synergy_score\": synergy[\"score\"],\n",
    "                        \"collaboration_areas\": synergy[\"areas\"],\n",
    "                        \"suggested_projects\": synergy[\"projects\"]\n",
    "                    })\n",
    "        \n",
    "        return opportunities\n",
    "    \n",
    "    def _extract_topics(self, text: str) -> List[str]:\n",
    "        \"\"\"Extract key research topics from text\"\"\"\n",
    "        # Simplified topic extraction\n",
    "        words = re.findall(r'\\b[a-zA-Z]+\\b', text.lower())\n",
    "        \n",
    "        # Filter for research-related terms\n",
    "        research_terms = [word for word in words if len(word) > 4 and \n",
    "                         word not in ['research', 'study', 'analysis', 'method', 'approach']]\n",
    "        \n",
    "        return list(set(research_terms))[:5]\n",
    "    \n",
    "    def _calculate_collaboration_score(self, user_profile: Dict, researcher: Dict, project: str) -> float:\n",
    "        \"\"\"Calculate how well a researcher matches for collaboration\"\"\"\n",
    "        score = 0.0\n",
    "        \n",
    "        # Research interest overlap\n",
    "        user_interests = user_profile.get(\"research_interests\", [])\n",
    "        researcher_interests = researcher.get(\"research_interests\", [])\n",
    "        \n",
    "        if user_interests and researcher_interests:\n",
    "            overlap = len(set(user_interests) & set(researcher_interests))\n",
    "            score += min(overlap / len(user_interests), 0.4)\n",
    "        \n",
    "        # H-index and experience factor\n",
    "        h_index = researcher.get(\"h_index\", 0)\n",
    "        score += min(h_index / 50, 0.3)  # Cap at 0.3\n",
    "        \n",
    "        # Recent activity\n",
    "        recent_papers = researcher.get(\"recent_papers\", [])\n",
    "        if recent_papers:\n",
    "            score += min(len(recent_papers) / 10, 0.3)\n",
    "        \n",
    "        return score\n",
    "    \n",
    "    def _find_shared_interests(self, user_profile: Dict, researcher: Dict) -> List[str]:\n",
    "        \"\"\"Find shared research interests\"\"\"\n",
    "        user_interests = set(user_profile.get(\"research_interests\", []))\n",
    "        researcher_interests = set(researcher.get(\"research_interests\", []))\n",
    "        return list(user_interests & researcher_interests)\n",
    "    \n",
    "    def _suggest_collaboration_role(self, researcher: Dict, project: str) -> str:\n",
    "        \"\"\"Suggest a collaboration role based on researcher's expertise\"\"\"\n",
    "        # Simplified role suggestion\n",
    "        h_index = researcher.get(\"h_index\", 0)\n",
    "        \n",
    "        if h_index > 30:\n",
    "            return \"Senior Collaborator / Advisor\"\n",
    "        elif h_index > 15:\n",
    "            return \"Co-Principal Investigator\"\n",
    "        else:\n",
    "            return \"Research Collaborator\"\n",
    "    \n",
    "    def _analyze_research_synergy(self, profile1: Dict, profile2: Dict) -> Dict:\n",
    "        \"\"\"Analyze synergy between two researchers\"\"\"\n",
    "        interests1 = set(profile1.get(\"research_interests\", []))\n",
    "        interests2 = set(profile2.get(\"research_interests\", []))\n",
    "        \n",
    "        # Calculate synergy score\n",
    "        overlap = interests1 & interests2\n",
    "        complement = (interests1 | interests2) - overlap\n",
    "        \n",
    "        synergy_score = (len(overlap) * 0.4 + len(complement) * 0.1) / max(len(interests1 | interests2), 1)\n",
    "        \n",
    "        return {\n",
    "            \"score\": min(synergy_score, 1.0),\n",
    "            \"areas\": list(overlap),\n",
    "            \"projects\": [f\"Joint research on {area}\" for area in list(overlap)[:3]]\n",
    "        }\n",
    "\n",
    "class IntelligentRecommendationEngine:\n",
    "    \"\"\"Advanced recommendation engine for personalized research assistance\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.user_profiles = {}\n",
    "        self.interaction_history = defaultdict(list)\n",
    "    \n",
    "    async def get_personalized_recommendations(self, user_id: str, context: str = \"general\") -> Dict:\n",
    "        \"\"\"Get personalized research recommendations\"\"\"\n",
    "        user_profile = self.user_profiles.get(user_id, {})\n",
    "        history = self.interaction_history.get(user_id, [])\n",
    "        \n",
    "        recommendations = {\n",
    "            \"papers\": await self._recommend_papers(user_profile, history),\n",
    "            \"researchers\": await self._recommend_researchers(user_profile, history),\n",
    "            \"topics\": await self._recommend_topics(user_profile, history),\n",
    "            \"collaborations\": await self._recommend_collaborations(user_profile),\n",
    "            \"conferences\": await self._recommend_conferences(user_profile),\n",
    "            \"funding\": await self._recommend_funding(user_profile)\n",
    "        }\n",
    "        \n",
    "        return recommendations\n",
    "    \n",
    "    async def update_user_interaction(self, user_id: str, interaction: Dict):\n",
    "        \"\"\"Update user interaction history for better recommendations\"\"\"\n",
    "        self.interaction_history[user_id].append({\n",
    "            \"timestamp\": datetime.utcnow(),\n",
    "            \"action\": interaction.get(\"action\"),\n",
    "            \"content\": interaction.get(\"content\"),\n",
    "            \"context\": interaction.get(\"context\")\n",
    "        })\n",
    "        \n",
    "        # Keep only recent interactions (last 100)\n",
    "        if len(self.interaction_history[user_id]) > 100:\n",
    "            self.interaction_history[user_id] = self.interaction_history[user_id][-100:]\n",
    "    \n",
    "    async def _recommend_papers(self, profile: Dict, history: List[Dict]) -> List[Dict]:\n",
    "        \"\"\"Recommend papers based on user profile and history\"\"\"\n",
    "        # Analyze user's reading patterns\n",
    "        read_topics = []\n",
    "        for interaction in history:\n",
    "            if interaction.get(\"action\") == \"read_paper\":\n",
    "                content = interaction.get(\"content\", {})\n",
    "                read_topics.extend(content.get(\"keywords\", []))\n",
    "        \n",
    "        # Find trending papers in user's areas of interest\n",
    "        interests = profile.get(\"research_interests\", [])\n",
    "        if not interests and read_topics:\n",
    "            interests = list(set(read_topics))[:3]\n",
    "        \n",
    "        recommended_papers = []\n",
    "        for interest in interests[:3]:\n",
    "            papers = await literature_search.unified_search(interest, max_results=3)\n",
    "            recommended_papers.extend(papers)\n",
    "        \n",
    "        return recommended_papers[:5]\n",
    "    \n",
    "    async def _recommend_researchers(self, profile: Dict, history: List[Dict]) -> List[Dict]:\n",
    "        \"\"\"Recommend researchers to follow\"\"\"\n",
    "        interests = profile.get(\"research_interests\", [])\n",
    "        \n",
    "        recommended_researchers = []\n",
    "        for interest in interests[:2]:\n",
    "            researchers = await researcher_discovery.search_researchers_by_topic(interest, max_results=3)\n",
    "            recommended_researchers.extend(researchers)\n",
    "        \n",
    "        return recommended_researchers[:5]\n",
    "    \n",
    "    async def _recommend_topics(self, profile: Dict, history: List[Dict]) -> List[str]:\n",
    "        \"\"\"Recommend new research topics to explore\"\"\"\n",
    "        current_interests = set(profile.get(\"research_interests\", []))\n",
    "        \n",
    "        # Find related topics through trend analysis\n",
    "        related_topics = []\n",
    "        for interest in current_interests:\n",
    "            trends = await trend_analysis.analyze_field_trends(interest)\n",
    "            emerging_topics = trends.get(\"emerging_keywords\", [])\n",
    "            related_topics.extend([topic[\"keyword\"] for topic in emerging_topics[:2]])\n",
    "        \n",
    "        # Filter out already known interests\n",
    "        new_topics = [topic for topic in related_topics if topic not in current_interests]\n",
    "        return new_topics[:5]\n",
    "    \n",
    "    async def _recommend_collaborations(self, profile: Dict) -> List[Dict]:\n",
    "        \"\"\"Recommend potential collaborations\"\"\"\n",
    "        # This would use the collaboration engine\n",
    "        collaboration_engine = CollaborationEngine()\n",
    "        \n",
    "        # Mock project description based on user interests\n",
    "        interests = profile.get(\"research_interests\", [])\n",
    "        project_desc = f\"Research project involving {', '.join(interests[:3])}\"\n",
    "        \n",
    "        collaborators = await collaboration_engine.find_collaborators(profile, project_desc)\n",
    "        return collaborators[:3]\n",
    "    \n",
    "    async def _recommend_conferences(self, profile: Dict) -> List[Dict]:\n",
    "        \"\"\"Recommend relevant conferences\"\"\"\n",
    "        # Mock conference recommendations\n",
    "        interests = profile.get(\"research_interests\", [])\n",
    "        conferences = []\n",
    "        \n",
    "        for interest in interests[:2]:\n",
    "            conferences.append({\n",
    "                \"name\": f\"International Conference on {interest}\",\n",
    "                \"relevance_score\": 0.85,\n",
    "                \"deadline\": \"2024-03-15\",\n",
    "                \"location\": \"Various\"\n",
    "            })\n",
    "        \n",
    "        return conferences\n",
    "    \n",
    "    async def _recommend_funding(self, profile: Dict) -> List[Dict]:\n",
    "        \"\"\"Recommend funding opportunities\"\"\"\n",
    "        # Mock funding recommendations\n",
    "        interests = profile.get(\"research_interests\", [])\n",
    "        funding_ops = []\n",
    "        \n",
    "        for interest in interests[:2]:\n",
    "            funding_ops.append({\n",
    "                \"title\": f\"Grant for {interest} Research\",\n",
    "                \"amount\": \"$250,000\",\n",
    "                \"deadline\": \"2024-04-30\",\n",
    "                \"match_score\": 0.78\n",
    "            })\n",
    "        \n",
    "        return funding_ops\n",
    "\n",
    "# Initialize new services\n",
    "multimodal_processor = MultiModalProcessor()\n",
    "collaboration_engine = CollaborationEngine()\n",
    "recommendation_engine = IntelligentRecommendationEngine()\n",
    "\n",
    "print(\"ü§ñ Additional cutting-edge services initialized!\")\n",
    "print(\"üìÑ Multi-modal: PDF, image, and table processing\")\n",
    "print(\"ü§ù Collaboration: Find collaborators and opportunities\") \n",
    "print(\"üéØ Recommendations: Personalized research assistance\")\n",
    "print(\"üß† AI-powered: Intelligent content analysis and suggestions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b529c00f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demo Usage Examples and Testing Functions\n",
    "\n",
    "async def demo_literature_search():\n",
    "    \"\"\"Demo: Search for literature on machine learning\"\"\"\n",
    "    print(\"üîç Demo: Literature Search\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    try:\n",
    "        # Search for papers on machine learning\n",
    "        results = await literature_search.unified_search(\"machine learning\", max_results=5)\n",
    "        \n",
    "        print(f\"Found {len(results)} papers:\")\n",
    "        for i, paper in enumerate(results, 1):\n",
    "            print(f\"\\n{i}. {paper.get('title', 'No title')}\")\n",
    "            print(f\"   Authors: {', '.join(paper.get('authors', [])[:3])}\")\n",
    "            print(f\"   Year: {paper.get('publication_date', 'Unknown')}\")\n",
    "            print(f\"   Citations: {paper.get('citation_count', 0)}\")\n",
    "            print(f\"   Source: {paper.get('source', 'Unknown')}\")\n",
    "        \n",
    "        return results\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error in literature search demo: {str(e)}\")\n",
    "        return []\n",
    "\n",
    "async def demo_researcher_discovery():\n",
    "    \"\"\"Demo: Find researchers in AI\"\"\"\n",
    "    print(\"\\nüë©‚Äçüî¨ Demo: Researcher Discovery\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    try:\n",
    "        researchers = await researcher_discovery.search_researchers_by_topic(\"artificial intelligence\", max_results=3)\n",
    "        \n",
    "        print(f\"Found {len(researchers)} researchers:\")\n",
    "        for i, researcher in enumerate(researchers, 1):\n",
    "            print(f\"\\n{i}. {researcher.get('name', 'Unknown')}\")\n",
    "            print(f\"   Affiliation: {researcher.get('affiliation', 'Unknown')}\")\n",
    "            print(f\"   H-index: {researcher.get('h_index', 0)}\")\n",
    "            print(f\"   Citations: {researcher.get('citation_count', 0)}\")\n",
    "            print(f\"   Interests: {', '.join(researcher.get('research_interests', [])[:3])}\")\n",
    "        \n",
    "        return researchers\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error in researcher discovery demo: {str(e)}\")\n",
    "        return []\n",
    "\n",
    "async def demo_paper_analysis():\n",
    "    \"\"\"Demo: Analyze a sample paper\"\"\"\n",
    "    print(\"\\nüìÑ Demo: Paper Analysis\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Sample paper data\n",
    "    sample_paper = {\n",
    "        \"title\": \"Deep Learning for Natural Language Processing: A Comprehensive Survey\",\n",
    "        \"abstract\": \"This paper presents a comprehensive survey of deep learning techniques applied to natural language processing. We review recent advances in neural networks, attention mechanisms, and transformer architectures. Our analysis shows that transformer-based models achieve state-of-the-art performance across multiple NLP tasks. We demonstrate significant improvements in accuracy and efficiency compared to traditional approaches. The results indicate promising directions for future research in this rapidly evolving field.\",\n",
    "        \"authors\": [\"John Smith\", \"Jane Doe\", \"Alice Johnson\"],\n",
    "        \"publication_date\": \"2023-05-15\",\n",
    "        \"journal\": \"Journal of AI Research\",\n",
    "        \"citation_count\": 150\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        analysis = await paper_analysis.analyze_paper(sample_paper)\n",
    "        \n",
    "        print(\"Paper Analysis Results:\")\n",
    "        print(f\"üìù Summary: {analysis.get('summary', 'N/A')}\")\n",
    "        print(f\"üéØ Key Contributions:\")\n",
    "        for contribution in analysis.get('key_contributions', [])[:3]:\n",
    "            print(f\"   ‚Ä¢ {contribution}\")\n",
    "        print(f\"üî¨ Methodology: {analysis.get('methodology', 'N/A')}\")\n",
    "        print(f\"üìä Results: {analysis.get('results', 'N/A')}\")\n",
    "        print(f\"üè∑Ô∏è Keywords: {', '.join(analysis.get('keywords', []))}\")\n",
    "        print(f\"‚≠ê Novelty Score: {analysis.get('novelty_score', 0):.2f}\")\n",
    "        print(f\"üìà Impact Prediction: {analysis.get('impact_prediction', {}).get('predicted_impact_score', 0):.2f}\")\n",
    "        \n",
    "        return analysis\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error in paper analysis demo: {str(e)}\")\n",
    "        return {}\n",
    "\n",
    "async def demo_trend_analysis():\n",
    "    \"\"\"Demo: Analyze trends in deep learning\"\"\"\n",
    "    print(\"\\nüìà Demo: Trend Analysis\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    try:\n",
    "        trends = await trend_analysis.analyze_field_trends(\"deep learning\", \"3_years\")\n",
    "        \n",
    "        print(\"Trend Analysis Results:\")\n",
    "        print(f\"üìä Field: {trends.get('field', 'Unknown')}\")\n",
    "        print(f\"‚è∞ Time Range: {trends.get('time_range', 'Unknown')}\")\n",
    "        print(f\"üìö Total Papers: {trends.get('total_papers', 0)}\")\n",
    "        \n",
    "        print(\"\\nüî• Trending Topics:\")\n",
    "        for i, topic in enumerate(trends.get('trending_topics', [])[:3], 1):\n",
    "            print(f\"   {i}. {topic.get('description', 'Unknown topic')}\")\n",
    "            print(f\"      Keywords: {', '.join(topic.get('keywords', [])[:5])}\")\n",
    "            print(f\"      Weight: {topic.get('weight', 0):.2f}\")\n",
    "        \n",
    "        print(\"\\nüìà Emerging Keywords:\")\n",
    "        for keyword in trends.get('emerging_keywords', [])[:5]:\n",
    "            print(f\"   ‚Ä¢ {keyword.get('keyword', 'Unknown')}: {keyword.get('growth_rate', 0):.1f}% growth\")\n",
    "        \n",
    "        return trends\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error in trend analysis demo: {str(e)}\")\n",
    "        return {}\n",
    "\n",
    "async def demo_research_gaps():\n",
    "    \"\"\"Demo: Find research gaps in computer vision\"\"\"\n",
    "    print(\"\\nüîç Demo: Research Gap Analysis\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    try:\n",
    "        gaps = await research_gap_finder.find_research_gaps(\"computer vision\", \"comprehensive\")\n",
    "        \n",
    "        print(f\"Found {len(gaps)} research gaps:\")\n",
    "        for i, gap in enumerate(gaps[:3], 1):\n",
    "            print(f\"\\n{i}. Gap Type: {gap.get('type', 'Unknown').title()}\")\n",
    "            print(f\"   Description: {gap.get('gap_description', 'No description')}\")\n",
    "            print(f\"   Importance Score: {gap.get('importance_score', 0):.2f}\")\n",
    "            print(f\"   Evidence: {gap.get('evidence', 'No evidence')}\")\n",
    "            print(f\"   Research Directions:\")\n",
    "            for direction in gap.get('research_directions', [])[:2]:\n",
    "                print(f\"     ‚Ä¢ {direction}\")\n",
    "        \n",
    "        return gaps\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error in research gap demo: {str(e)}\")\n",
    "        return []\n",
    "\n",
    "async def demo_writing_assistant():\n",
    "    \"\"\"Demo: Analyze and improve academic writing\"\"\"\n",
    "    print(\"\\n‚úçÔ∏è Demo: Writing Assistant\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    sample_text = \"\"\"\n",
    "    This paper presents a study of machine learning algorithms. The results show that our method is very good. \n",
    "    We've achieved better performance than other approaches. The experiments prove that it's effective.\n",
    "    \"\"\"\n",
    "    \n",
    "    try:\n",
    "        # Analyze writing\n",
    "        analysis = await writing_assistant.analyze_writing(sample_text, \"academic\")\n",
    "        \n",
    "        print(\"Writing Analysis:\")\n",
    "        print(f\"üìñ Readability Score: {analysis.get('readability', {}).get('flesch_score', 0):.1f}\")\n",
    "        print(f\"üìù Level: {analysis.get('readability', {}).get('level', 'Unknown')}\")\n",
    "        print(f\"‚ö†Ô∏è Grammar Issues: {len(analysis.get('grammar_issues', []))}\")\n",
    "        print(f\"üí° Style Suggestions: {len(analysis.get('style_suggestions', []))}\")\n",
    "        \n",
    "        # Improve writing\n",
    "        improvements = await writing_assistant.improve_text(sample_text, \"comprehensive\")\n",
    "        \n",
    "        print(f\"\\nüìù Original Text: {sample_text.strip()}\")\n",
    "        print(f\"‚ú® Improved Text: {improvements.get('improved_text', sample_text).strip()}\")\n",
    "        print(f\"üîß Changes Made: {len(improvements.get('changes_made', []))}\")\n",
    "        \n",
    "        return analysis, improvements\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error in writing assistant demo: {str(e)}\")\n",
    "        return {}, {}\n",
    "\n",
    "async def demo_citation_generator():\n",
    "    \"\"\"Demo: Generate citations in different styles\"\"\"\n",
    "    print(\"\\nüìö Demo: Citation Generator\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    sample_paper = {\n",
    "        \"title\": \"Attention Is All You Need\",\n",
    "        \"authors\": [\"Ashish Vaswani\", \"Noam Shazeer\", \"Niki Parmar\"],\n",
    "        \"publication_date\": \"2017-12-06\",\n",
    "        \"journal\": \"Advances in Neural Information Processing Systems\",\n",
    "        \"doi\": \"10.48550/arXiv.1706.03762\"\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        styles = [\"APA\", \"MLA\", \"IEEE\"]\n",
    "        print(\"Citations in different styles:\")\n",
    "        \n",
    "        for style in styles:\n",
    "            citation = await citation_generator.generate_citation(sample_paper, style)\n",
    "            print(f\"\\n{style}: {citation}\")\n",
    "        \n",
    "        return citation\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error in citation demo: {str(e)}\")\n",
    "        return \"\"\n",
    "\n",
    "async def run_all_demos():\n",
    "    \"\"\"Run all demo functions\"\"\"\n",
    "    print(\"üöÄ Running Research Agent Backend Demos\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # Run each demo\n",
    "    await demo_literature_search()\n",
    "    await demo_researcher_discovery()\n",
    "    await demo_paper_analysis()\n",
    "    await demo_trend_analysis()\n",
    "    await demo_research_gaps()\n",
    "    await demo_writing_assistant()\n",
    "    await demo_citation_generator()\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"‚úÖ All demos completed successfully!\")\n",
    "    print(\"üéâ Research Agent Backend is fully functional!\")\n",
    "\n",
    "# Test API endpoint function\n",
    "async def test_api_endpoints():\n",
    "    \"\"\"Test key API endpoints\"\"\"\n",
    "    print(\"\\nüß™ Testing API Endpoints\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    try:\n",
    "        # This would require the FastAPI server to be running\n",
    "        # For now, we'll just validate the endpoint definitions\n",
    "        \n",
    "        endpoints = [\n",
    "            \"/api/search/literature\",\n",
    "            \"/api/researchers/search/{topic}\",\n",
    "            \"/api/papers/analyze\",\n",
    "            \"/api/trends/{field}\",\n",
    "            \"/api/research-gaps/{field}\",\n",
    "            \"/api/writing/analyze\",\n",
    "            \"/api/citations/generate\",\n",
    "            \"/api/agent/explore\"\n",
    "        ]\n",
    "        \n",
    "        print(\"Available API endpoints:\")\n",
    "        for endpoint in endpoints:\n",
    "            print(f\"‚úÖ {endpoint}\")\n",
    "        \n",
    "        print(f\"\\nTotal endpoints: {len(endpoints)}\")\n",
    "        print(\"üì° API is ready for deployment!\")\n",
    "        \n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error in API testing: {str(e)}\")\n",
    "        return False\n",
    "\n",
    "# Integration test function\n",
    "async def integration_test():\n",
    "    \"\"\"Run integration tests for the research agent\"\"\"\n",
    "    print(\"\\nüîó Integration Test\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    try:\n",
    "        # Test service integration\n",
    "        print(\"Testing service integration...\")\n",
    "        \n",
    "        # 1. Search -> Analysis pipeline\n",
    "        papers = await literature_search.unified_search(\"neural networks\", max_results=2)\n",
    "        if papers:\n",
    "            analysis = await paper_analysis.analyze_paper(papers[0])\n",
    "            print(\"‚úÖ Literature Search ‚Üí Paper Analysis: OK\")\n",
    "        \n",
    "        # 2. Search -> Citation pipeline\n",
    "        if papers:\n",
    "            citation = await citation_generator.generate_citation(papers[0], \"APA\")\n",
    "            print(\"‚úÖ Literature Search ‚Üí Citation Generation: OK\")\n",
    "        \n",
    "        # 3. Trend analysis pipeline\n",
    "        trends = await trend_analysis.analyze_field_trends(\"AI\", \"1_year\")\n",
    "        print(\"‚úÖ Trend Analysis: OK\")\n",
    "        \n",
    "        print(\"\\n‚úÖ All integration tests passed!\")\n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Integration test failed: {str(e)}\")\n",
    "        return False\n",
    "\n",
    "print(\"üéØ Demo and Testing Functions Created!\")\n",
    "print(\"üìã Available demos:\")\n",
    "print(\"   ‚Ä¢ demo_literature_search()\")\n",
    "print(\"   ‚Ä¢ demo_researcher_discovery()\")\n",
    "print(\"   ‚Ä¢ demo_paper_analysis()\")\n",
    "print(\"   ‚Ä¢ demo_trend_analysis()\")\n",
    "print(\"   ‚Ä¢ demo_research_gaps()\")\n",
    "print(\"   ‚Ä¢ demo_writing_assistant()\")\n",
    "print(\"   ‚Ä¢ demo_citation_generator()\")\n",
    "print(\"   ‚Ä¢ run_all_demos()\")\n",
    "print(\"   ‚Ä¢ test_api_endpoints()\")\n",
    "print(\"   ‚Ä¢ integration_test()\")\n",
    "print(\"\\nüí° Run 'await run_all_demos()' to see the system in action!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ca8611d",
   "metadata": {},
   "source": [
    "# üöÄ Deployment and Usage Instructions\n",
    "\n",
    "## üì¶ Installation and Setup\n",
    "\n",
    "### 1. **Environment Setup**\n",
    "```bash\n",
    "# Create virtual environment\n",
    "python -m venv research_agent_env\n",
    "source research_agent_env/bin/activate  # On Windows: research_agent_env\\Scripts\\activate\n",
    "\n",
    "# Install dependencies\n",
    "pip install -r requirements.txt\n",
    "\n",
    "# Install additional NLP models\n",
    "python -m spacy download en_core_web_sm\n",
    "python -m nltk.downloader punkt stopwords\n",
    "```\n",
    "\n",
    "### 2. **Environment Variables**\n",
    "Create a `.env` file in your project root:\n",
    "```env\n",
    "# API Keys\n",
    "OPENAI_API_KEY=your_openai_key_here\n",
    "SEMANTIC_SCHOLAR_API_KEY=your_semantic_scholar_key\n",
    "CROSSREF_EMAIL=your_email@example.com\n",
    "\n",
    "# Database\n",
    "DATABASE_URL=sqlite:///./research_agent.db\n",
    "\n",
    "# Security\n",
    "SECRET_KEY=your-super-secret-key-here\n",
    "\n",
    "# External Services\n",
    "ELASTICSEARCH_URL=http://localhost:9200\n",
    "REDIS_URL=redis://localhost:6379\n",
    "```\n",
    "\n",
    "### 3. **Database Setup**\n",
    "```python\n",
    "# Run in Python or Jupyter\n",
    "from sqlalchemy import create_engine\n",
    "from database_models import Base\n",
    "\n",
    "engine = create_engine(\"sqlite:///./research_agent.db\")\n",
    "Base.metadata.create_all(bind=engine)\n",
    "print(\"Database tables created successfully!\")\n",
    "```\n",
    "\n",
    "## üåê Running the Application\n",
    "\n",
    "### Method 1: Jupyter Notebook (Development)\n",
    "```python\n",
    "# Run all the cells in this notebook, then:\n",
    "import asyncio\n",
    "await run_all_demos()  # Test all functionality\n",
    "```\n",
    "\n",
    "### Method 2: FastAPI Server (Production)\n",
    "```bash\n",
    "# Save the FastAPI app to main.py, then run:\n",
    "uvicorn main:app --reload --host 0.0.0.0 --port 8000\n",
    "\n",
    "# Or for production:\n",
    "uvicorn main:app --host 0.0.0.0 --port 8000 --workers 4\n",
    "```\n",
    "\n",
    "### Method 3: Docker Deployment\n",
    "```dockerfile\n",
    "# Dockerfile\n",
    "FROM python:3.9-slim\n",
    "\n",
    "WORKDIR /app\n",
    "COPY requirements.txt .\n",
    "RUN pip install -r requirements.txt\n",
    "\n",
    "COPY . .\n",
    "EXPOSE 8000\n",
    "\n",
    "CMD [\"uvicorn\", \"main:app\", \"--host\", \"0.0.0.0\", \"--port\", \"8000\"]\n",
    "```\n",
    "\n",
    "```bash\n",
    "# Build and run\n",
    "docker build -t research-agent .\n",
    "docker run -p 8000:8000 research-agent\n",
    "```\n",
    "\n",
    "## üìã API Usage Examples\n",
    "\n",
    "### 1. **Literature Search**\n",
    "```python\n",
    "import requests\n",
    "\n",
    "# Search for papers\n",
    "response = requests.post(\"http://localhost:8000/api/search/literature\", \n",
    "    json={\n",
    "        \"query\": \"machine learning\",\n",
    "        \"limit\": 10,\n",
    "        \"filters\": {\"year_from\": 2020}\n",
    "    })\n",
    "\n",
    "papers = response.json()[\"results\"]\n",
    "print(f\"Found {len(papers)} papers\")\n",
    "```\n",
    "\n",
    "### 2. **Researcher Discovery**\n",
    "```python\n",
    "# Find researchers in AI\n",
    "response = requests.get(\"http://localhost:8000/api/researchers/search/artificial%20intelligence\")\n",
    "researchers = response.json()[\"researchers\"]\n",
    "\n",
    "for researcher in researchers[:3]:\n",
    "    print(f\"{researcher['name']} - H-index: {researcher['h_index']}\")\n",
    "```\n",
    "\n",
    "### 3. **Paper Analysis**\n",
    "```python\n",
    "# Analyze a paper\n",
    "paper_data = {\n",
    "    \"title\": \"Your Paper Title\",\n",
    "    \"abstract\": \"Your paper abstract...\",\n",
    "    \"authors\": [\"Author 1\", \"Author 2\"]\n",
    "}\n",
    "\n",
    "response = requests.post(\"http://localhost:8000/api/papers/analyze\", json=paper_data)\n",
    "analysis = response.json()\n",
    "\n",
    "print(f\"Novelty Score: {analysis['novelty_score']}\")\n",
    "print(f\"Key Contributions: {analysis['key_contributions']}\")\n",
    "```\n",
    "\n",
    "### 4. **Writing Assistant**\n",
    "```python\n",
    "# Analyze writing\n",
    "response = requests.post(\"http://localhost:8000/api/writing/analyze\", \n",
    "    json={\n",
    "        \"text\": \"Your academic text here...\",\n",
    "        \"assistance_type\": \"grammar\",\n",
    "        \"context\": \"academic\"\n",
    "    })\n",
    "\n",
    "analysis = response.json()\n",
    "print(f\"Readability Score: {analysis['readability']['flesch_score']}\")\n",
    "```\n",
    "\n",
    "### 5. **Citation Generation**\n",
    "```python\n",
    "# Generate citation\n",
    "paper_data = {\n",
    "    \"title\": \"Deep Learning Advances\",\n",
    "    \"authors\": [\"Smith, J.\", \"Doe, J.\"],\n",
    "    \"publication_date\": \"2023-01-15\",\n",
    "    \"journal\": \"AI Journal\"\n",
    "}\n",
    "\n",
    "response = requests.post(\"http://localhost:8000/api/citations/generate?style=APA\", \n",
    "    json=paper_data)\n",
    "\n",
    "citation = response.json()[\"citation\"]\n",
    "print(f\"APA Citation: {citation}\")\n",
    "```\n",
    "\n",
    "## ü§ñ Advanced Features Usage\n",
    "\n",
    "### 1. **Autonomous Research Exploration**\n",
    "```python\n",
    "# Let the AI explore a topic autonomously\n",
    "response = requests.post(\"http://localhost:8000/api/agent/explore\", \n",
    "    json={\"topic\": \"quantum computing\", \"depth\": 3})\n",
    "\n",
    "exploration = response.json()\n",
    "print(\"Discoveries:\", exploration[\"discoveries\"])\n",
    "print(\"Recommendations:\", exploration[\"recommendations\"])\n",
    "```\n",
    "\n",
    "### 2. **Research Gap Analysis**\n",
    "```python\n",
    "# Find research gaps\n",
    "response = requests.get(\"http://localhost:8000/api/research-gaps/computer%20vision\")\n",
    "gaps = response.json()[\"gaps\"]\n",
    "\n",
    "for gap in gaps[:3]:\n",
    "    print(f\"Gap: {gap['gap_description']}\")\n",
    "    print(f\"Importance: {gap['importance_score']}\")\n",
    "```\n",
    "\n",
    "### 3. **Personalized Recommendations**\n",
    "```python\n",
    "# Get personalized recommendations (would need user profile setup)\n",
    "user_profile = {\n",
    "    \"research_interests\": [\"machine learning\", \"natural language processing\"],\n",
    "    \"experience_level\": \"intermediate\"\n",
    "}\n",
    "\n",
    "# This would integrate with the recommendation engine\n",
    "recommendations = await recommendation_engine.get_personalized_recommendations(\"user123\")\n",
    "```\n",
    "\n",
    "## üîß Customization and Extension\n",
    "\n",
    "### Adding New Data Sources\n",
    "```python\n",
    "class CustomSearchService:\n",
    "    async def search_custom_database(self, query: str):\n",
    "        # Implement your custom search logic\n",
    "        pass\n",
    "\n",
    "# Integrate with existing services\n",
    "literature_search.custom_service = CustomSearchService()\n",
    "```\n",
    "\n",
    "### Custom Analysis Algorithms\n",
    "```python\n",
    "class CustomAnalyzer:\n",
    "    async def custom_paper_analysis(self, paper_data: Dict):\n",
    "        # Implement your custom analysis\n",
    "        pass\n",
    "\n",
    "# Add to paper analysis service\n",
    "paper_analysis.custom_analyzer = CustomAnalyzer()\n",
    "```\n",
    "\n",
    "### New Citation Styles\n",
    "```python\n",
    "def custom_citation_format(paper_data: Dict) -> str:\n",
    "    # Implement custom citation format\n",
    "    pass\n",
    "\n",
    "citation_generator.citation_styles[\"CUSTOM\"] = custom_citation_format\n",
    "```\n",
    "\n",
    "## üìä Monitoring and Analytics\n",
    "\n",
    "### System Health Monitoring\n",
    "```python\n",
    "# Check system health\n",
    "response = requests.get(\"http://localhost:8000/api/health\")\n",
    "print(f\"Status: {response.json()['status']}\")\n",
    "\n",
    "# Get system statistics\n",
    "response = requests.get(\"http://localhost:8000/api/stats\")\n",
    "stats = response.json()\n",
    "print(f\"Total papers: {stats['total_papers']}\")\n",
    "print(f\"Total researchers: {stats['total_researchers']}\")\n",
    "```\n",
    "\n",
    "### Performance Optimization\n",
    "- Use Redis for caching frequent searches\n",
    "- Implement pagination for large result sets\n",
    "- Use background tasks for heavy operations\n",
    "- Set up database indexing for faster queries\n",
    "\n",
    "## üîí Security Considerations\n",
    "\n",
    "1. **API Rate Limiting**: Implement rate limiting for production use\n",
    "2. **Authentication**: Add JWT tokens for user authentication\n",
    "3. **Input Validation**: Validate all input parameters\n",
    "4. **HTTPS**: Use HTTPS in production\n",
    "5. **API Keys**: Keep API keys secure and rotate regularly\n",
    "\n",
    "## ü§ù Contributing\n",
    "\n",
    "1. Fork the repository\n",
    "2. Create feature branches\n",
    "3. Add comprehensive tests\n",
    "4. Submit pull requests\n",
    "5. Follow code style guidelines\n",
    "\n",
    "---\n",
    "\n",
    "**üéâ Congratulations! Your Research Agent Backend is now ready to revolutionize academic research!**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa58d057",
   "metadata": {},
   "source": [
    "# üß† LLM-Powered Deep Research Tools & System Overview\n",
    "\n",
    "## üìö **System Architecture Overview**\n",
    "\n",
    "This Research Agent Backend is a comprehensive AI-powered platform designed to revolutionize academic research through intelligent automation and deep analysis. The system integrates multiple cutting-edge technologies to provide researchers with unprecedented capabilities for literature discovery, analysis, and knowledge synthesis.\n",
    "\n",
    "### **üèóÔ∏è Core Architecture Components**\n",
    "\n",
    "1. **Multi-Database Literature Engine**: Unified search across arXiv, Semantic Scholar, Google Scholar, and PubMed\n",
    "2. **AI-Powered Analysis Pipeline**: Advanced NLP and ML models for paper understanding\n",
    "3. **Knowledge Graph Construction**: Semantic relationships between papers, authors, and concepts\n",
    "4. **Intelligent Writing Assistant**: Grammar, style, and citation optimization\n",
    "5. **Collaborative Research Platform**: Team coordination and knowledge sharing\n",
    "6. **Predictive Analytics Engine**: Trend forecasting and impact prediction\n",
    "\n",
    "## ü§ñ **LLM-Based Deep Research Tools**\n",
    "\n",
    "The following advanced AI tools leverage Large Language Models to provide deep research insights and automation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f8bbce0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Advanced LLM-Based Deep Research Tools\n",
    "\n",
    "class LLMResearchAgent:\n",
    "    \"\"\"\n",
    "    üß† Advanced LLM-powered research agent for deep academic analysis\n",
    "    \n",
    "    This class leverages Large Language Models to provide sophisticated research\n",
    "    capabilities including semantic understanding, knowledge synthesis, hypothesis\n",
    "    generation, and intelligent reasoning about academic content.\n",
    "    \n",
    "    Key Features:\n",
    "    - Deep paper comprehension and synthesis\n",
    "    - Hypothesis generation and validation\n",
    "    - Literature gap analysis with reasoning\n",
    "    - Intelligent research question formulation\n",
    "    - Cross-domain knowledge transfer\n",
    "    - Research methodology recommendations\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, model_name: str = \"gpt-4\"):\n",
    "        \"\"\"\n",
    "        Initialize the LLM Research Agent\n",
    "        \n",
    "        Args:\n",
    "            model_name: The LLM model to use (gpt-4, gpt-3.5-turbo, claude-3, etc.)\n",
    "        \"\"\"\n",
    "        self.model_name = model_name\n",
    "        self.conversation_history = []\n",
    "        self.research_context = {}\n",
    "        \n",
    "        # Initialize LLM client (placeholder - would use OpenAI, Anthropic, etc.)\n",
    "        try:\n",
    "            import openai\n",
    "            self.llm_client = openai.OpenAI(api_key=settings.openai_api_key)\n",
    "            self.has_llm = True\n",
    "        except ImportError:\n",
    "            logger.warning(\"OpenAI not available. Install with: pip install openai\")\n",
    "            self.has_llm = False\n",
    "    \n",
    "    async def deep_paper_analysis(self, paper_data: Dict, analysis_depth: str = \"comprehensive\") -> Dict:\n",
    "        \"\"\"\n",
    "        üî¨ Perform deep LLM-powered analysis of research papers\n",
    "        \n",
    "        This method uses advanced language models to understand papers at a semantic level,\n",
    "        extracting nuanced insights that traditional NLP cannot capture.\n",
    "        \n",
    "        Features:\n",
    "        - Semantic understanding of research contributions\n",
    "        - Identification of implicit assumptions and limitations\n",
    "        - Cross-referencing with domain knowledge\n",
    "        - Assessment of methodological rigor\n",
    "        - Prediction of future research directions\n",
    "        \n",
    "        Args:\n",
    "            paper_data: Paper information (title, abstract, content)\n",
    "            analysis_depth: Level of analysis ('quick', 'standard', 'comprehensive', 'expert')\n",
    "        \n",
    "        Returns:\n",
    "            Comprehensive analysis including insights, implications, and recommendations\n",
    "        \"\"\"\n",
    "        if not self.has_llm:\n",
    "            return await self._fallback_analysis(paper_data)\n",
    "        \n",
    "        try:\n",
    "            # Construct analysis prompt based on depth\n",
    "            prompts = {\n",
    "                \"quick\": self._build_quick_analysis_prompt(paper_data),\n",
    "                \"standard\": self._build_standard_analysis_prompt(paper_data),\n",
    "                \"comprehensive\": self._build_comprehensive_analysis_prompt(paper_data),\n",
    "                \"expert\": self._build_expert_analysis_prompt(paper_data)\n",
    "            }\n",
    "            \n",
    "            prompt = prompts.get(analysis_depth, prompts[\"standard\"])\n",
    "            \n",
    "            # Get LLM analysis\n",
    "            response = await self._query_llm(prompt, max_tokens=2000)\n",
    "            \n",
    "            # Parse and structure the response\n",
    "            analysis = await self._parse_llm_analysis(response)\n",
    "            \n",
    "            # Enhance with traditional analysis\n",
    "            traditional_analysis = await paper_analysis.analyze_paper(paper_data)\n",
    "            \n",
    "            # Combine LLM insights with traditional metrics\n",
    "            deep_analysis = {\n",
    "                \"llm_insights\": analysis,\n",
    "                \"traditional_metrics\": traditional_analysis,\n",
    "                \"synthesis\": await self._synthesize_analyses(analysis, traditional_analysis),\n",
    "                \"confidence_score\": self._calculate_analysis_confidence(analysis),\n",
    "                \"research_implications\": await self._extract_implications(analysis),\n",
    "                \"follow_up_questions\": await self._generate_follow_up_questions(analysis)\n",
    "            }\n",
    "            \n",
    "            return deep_analysis\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"LLM analysis error: {str(e)}\")\n",
    "            return await self._fallback_analysis(paper_data)\n",
    "    \n",
    "    async def research_hypothesis_generator(self, topic: str, existing_papers: List[Dict], constraints: Dict = None) -> Dict:\n",
    "        \"\"\"\n",
    "        üí° Generate novel research hypotheses using LLM reasoning\n",
    "        \n",
    "        This advanced tool analyzes existing literature and generates creative,\n",
    "        testable hypotheses that could advance the field. It considers gaps,\n",
    "        contradictions, and emerging patterns in the literature.\n",
    "        \n",
    "        Features:\n",
    "        - Creative hypothesis generation based on literature gaps\n",
    "        - Testability assessment of proposed hypotheses\n",
    "        - Resource requirement estimation\n",
    "        - Expected impact evaluation\n",
    "        - Risk-benefit analysis\n",
    "        \n",
    "        Args:\n",
    "            topic: Research area or specific question\n",
    "            existing_papers: List of relevant papers for context\n",
    "            constraints: Research constraints (budget, time, equipment, etc.)\n",
    "        \n",
    "        Returns:\n",
    "            Generated hypotheses with justification and testing strategies\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Analyze existing literature for patterns and gaps\n",
    "            literature_summary = await self._summarize_literature(existing_papers)\n",
    "            \n",
    "            # Construct hypothesis generation prompt\n",
    "            prompt = f\"\"\"\n",
    "            As an expert researcher in {topic}, analyze the following literature summary and generate novel, testable research hypotheses:\n",
    "            \n",
    "            Literature Summary:\n",
    "            {literature_summary}\n",
    "            \n",
    "            Research Constraints:\n",
    "            {json.dumps(constraints or {}, indent=2)}\n",
    "            \n",
    "            Please generate 3-5 innovative hypotheses that:\n",
    "            1. Address identified gaps in the literature\n",
    "            2. Are testable with available methods\n",
    "            3. Could significantly advance the field\n",
    "            4. Consider the given constraints\n",
    "            \n",
    "            For each hypothesis, provide:\n",
    "            - Clear statement of the hypothesis\n",
    "            - Justification based on literature gaps\n",
    "            - Proposed testing methodology\n",
    "            - Expected outcomes and impact\n",
    "            - Resource requirements\n",
    "            - Risk assessment\n",
    "            \"\"\"\n",
    "            \n",
    "            response = await self._query_llm(prompt, max_tokens=1500)\n",
    "            \n",
    "            # Parse and structure hypotheses\n",
    "            hypotheses = await self._parse_hypotheses(response)\n",
    "            \n",
    "            # Enhance with feasibility analysis\n",
    "            for hypothesis in hypotheses:\n",
    "                hypothesis[\"feasibility_score\"] = await self._assess_feasibility(hypothesis, constraints)\n",
    "                hypothesis[\"innovation_score\"] = await self._assess_innovation(hypothesis, existing_papers)\n",
    "                hypothesis[\"impact_prediction\"] = await self._predict_hypothesis_impact(hypothesis)\n",
    "            \n",
    "            return {\n",
    "                \"topic\": topic,\n",
    "                \"generated_hypotheses\": hypotheses,\n",
    "                \"literature_context\": literature_summary,\n",
    "                \"generation_metadata\": {\n",
    "                    \"model_used\": self.model_name,\n",
    "                    \"timestamp\": datetime.utcnow(),\n",
    "                    \"confidence\": self._calculate_generation_confidence(hypotheses)\n",
    "                }\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Hypothesis generation error: {str(e)}\")\n",
    "            return {\"error\": str(e), \"hypotheses\": []}\n",
    "    \n",
    "    async def intelligent_literature_synthesis(self, papers: List[Dict], synthesis_goal: str) -> Dict:\n",
    "        \"\"\"\n",
    "        üß© Synthesize insights from multiple papers using advanced reasoning\n",
    "        \n",
    "        This tool goes beyond simple summarization to create coherent narratives\n",
    "        that connect ideas across papers, identify patterns, and generate new insights\n",
    "        through intelligent synthesis.\n",
    "        \n",
    "        Features:\n",
    "        - Cross-paper theme identification\n",
    "        - Contradiction detection and resolution\n",
    "        - Knowledge graph construction\n",
    "        - Narrative coherence optimization\n",
    "        - Insight emergence detection\n",
    "        \n",
    "        Args:\n",
    "            papers: List of papers to synthesize\n",
    "            synthesis_goal: Purpose of synthesis (review, gap analysis, methodology comparison)\n",
    "        \n",
    "        Returns:\n",
    "            Coherent synthesis with identified themes, patterns, and insights\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Prepare papers for synthesis\n",
    "            paper_summaries = []\n",
    "            for paper in papers:\n",
    "                summary = await self.deep_paper_analysis(paper, \"standard\")\n",
    "                paper_summaries.append({\n",
    "                    \"paper\": paper,\n",
    "                    \"analysis\": summary,\n",
    "                    \"key_points\": summary.get(\"llm_insights\", {}).get(\"key_points\", [])\n",
    "                })\n",
    "            \n",
    "            # Construct synthesis prompt\n",
    "            synthesis_prompt = f\"\"\"\n",
    "            As an expert researcher, synthesize insights from the following {len(papers)} papers with the goal of: {synthesis_goal}\n",
    "            \n",
    "            Papers and Analysis:\n",
    "            {self._format_papers_for_synthesis(paper_summaries)}\n",
    "            \n",
    "            Please provide a comprehensive synthesis that includes:\n",
    "            1. Major themes and patterns across papers\n",
    "            2. Areas of consensus and disagreement\n",
    "            3. Evolution of ideas over time\n",
    "            4. Methodological innovations and trends\n",
    "            5. Identified knowledge gaps\n",
    "            6. Emerging insights from cross-paper analysis\n",
    "            7. Recommendations for future research\n",
    "            \n",
    "            Structure your response with clear sections and evidence-based conclusions.\n",
    "            \"\"\"\n",
    "            \n",
    "            response = await self._query_llm(synthesis_prompt, max_tokens=2500)\n",
    "            \n",
    "            # Parse synthesis into structured format\n",
    "            synthesis = await self._parse_synthesis(response)\n",
    "            \n",
    "            # Add quantitative metrics\n",
    "            synthesis[\"synthesis_metrics\"] = {\n",
    "                \"papers_analyzed\": len(papers),\n",
    "                \"themes_identified\": len(synthesis.get(\"themes\", [])),\n",
    "                \"gaps_identified\": len(synthesis.get(\"knowledge_gaps\", [])),\n",
    "                \"synthesis_coherence_score\": await self._assess_synthesis_coherence(synthesis),\n",
    "                \"coverage_completeness\": await self._assess_coverage_completeness(synthesis, papers)\n",
    "            }\n",
    "            \n",
    "            return synthesis\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Literature synthesis error: {str(e)}\")\n",
    "            return {\"error\": str(e), \"synthesis\": \"Synthesis failed\"}\n",
    "    \n",
    "    async def research_question_optimizer(self, initial_question: str, context: Dict) -> Dict:\n",
    "        \"\"\"\n",
    "        üéØ Optimize research questions for clarity, testability, and impact\n",
    "        \n",
    "        This tool refines research questions to maximize their scientific value,\n",
    "        ensuring they are specific, measurable, achievable, relevant, and time-bound.\n",
    "        \n",
    "        Features:\n",
    "        - Question clarity enhancement\n",
    "        - Testability improvement\n",
    "        - Scope optimization\n",
    "        - Impact maximization\n",
    "        - Feasibility assessment\n",
    "        \n",
    "        Args:\n",
    "            initial_question: The research question to optimize\n",
    "            context: Research context (field, resources, timeline, etc.)\n",
    "        \n",
    "        Returns:\n",
    "            Optimized research questions with improvement rationale\n",
    "        \"\"\"\n",
    "        try:\n",
    "            optimization_prompt = f\"\"\"\n",
    "            As a research methodology expert, optimize the following research question:\n",
    "            \n",
    "            Initial Question: \"{initial_question}\"\n",
    "            \n",
    "            Research Context:\n",
    "            {json.dumps(context, indent=2)}\n",
    "            \n",
    "            Please provide:\n",
    "            1. Analysis of the current question's strengths and weaknesses\n",
    "            2. 3-5 optimized versions of the question\n",
    "            3. Rationale for each optimization\n",
    "            4. Testability assessment for each version\n",
    "            5. Expected impact and feasibility scores\n",
    "            6. Recommended methodology for addressing the optimized questions\n",
    "            \n",
    "            Focus on making questions SMART (Specific, Measurable, Achievable, Relevant, Time-bound).\n",
    "            \"\"\"\n",
    "            \n",
    "            response = await self._query_llm(optimization_prompt, max_tokens=1500)\n",
    "            optimization_result = await self._parse_question_optimization(response)\n",
    "            \n",
    "            # Score each optimized question\n",
    "            for question in optimization_result.get(\"optimized_questions\", []):\n",
    "                question[\"smart_score\"] = await self._calculate_smart_score(question)\n",
    "                question[\"research_impact_prediction\"] = await self._predict_question_impact(question, context)\n",
    "            \n",
    "            return optimization_result\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Question optimization error: {str(e)}\")\n",
    "            return {\"error\": str(e)}\n",
    "    \n",
    "    async def cross_domain_knowledge_transfer(self, source_domain: str, target_domain: str, specific_problem: str) -> Dict:\n",
    "        \"\"\"\n",
    "        üåê Transfer knowledge and methods between research domains\n",
    "        \n",
    "        This advanced tool identifies applicable methods, theories, and insights\n",
    "        from one domain that could solve problems in another domain.\n",
    "        \n",
    "        Features:\n",
    "        - Cross-domain pattern recognition\n",
    "        - Method transferability analysis\n",
    "        - Adaptation strategy generation\n",
    "        - Risk assessment for transfer\n",
    "        - Success probability estimation\n",
    "        \n",
    "        Args:\n",
    "            source_domain: Domain to transfer knowledge from\n",
    "            target_domain: Domain to transfer knowledge to\n",
    "            specific_problem: Specific problem to address\n",
    "        \n",
    "        Returns:\n",
    "            Transfer opportunities with adaptation strategies\n",
    "        \"\"\"\n",
    "        try:\n",
    "            transfer_prompt = f\"\"\"\n",
    "            As an interdisciplinary research expert, identify opportunities to transfer knowledge from {source_domain} to solve problems in {target_domain}.\n",
    "            \n",
    "            Specific Problem to Address: {specific_problem}\n",
    "            \n",
    "            Please analyze:\n",
    "            1. Relevant theories, methods, or technologies from {source_domain}\n",
    "            2. How these could be adapted for {target_domain}\n",
    "            3. Potential challenges and obstacles in transfer\n",
    "            4. Success stories of similar transfers\n",
    "            5. Step-by-step adaptation strategy\n",
    "            6. Expected benefits and limitations\n",
    "            7. Resource requirements for successful transfer\n",
    "            \n",
    "            Focus on practical, implementable solutions.\n",
    "            \"\"\"\n",
    "            \n",
    "            response = await self._query_llm(transfer_prompt, max_tokens=2000)\n",
    "            transfer_analysis = await self._parse_transfer_analysis(response)\n",
    "            \n",
    "            # Enhance with feasibility scoring\n",
    "            for opportunity in transfer_analysis.get(\"transfer_opportunities\", []):\n",
    "                opportunity[\"feasibility_score\"] = await self._assess_transfer_feasibility(opportunity)\n",
    "                opportunity[\"adaptation_complexity\"] = await self._assess_adaptation_complexity(opportunity)\n",
    "                opportunity[\"expected_impact\"] = await self._predict_transfer_impact(opportunity)\n",
    "            \n",
    "            return transfer_analysis\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Knowledge transfer analysis error: {str(e)}\")\n",
    "            return {\"error\": str(e)}\n",
    "    \n",
    "    # Helper methods for LLM interaction and parsing\n",
    "    \n",
    "    async def _query_llm(self, prompt: str, max_tokens: int = 1000) -> str:\n",
    "        \"\"\"Query the LLM with the given prompt\"\"\"\n",
    "        if not self.has_llm:\n",
    "            return \"LLM not available - using fallback response\"\n",
    "        \n",
    "        try:\n",
    "            response = self.llm_client.chat.completions.create(\n",
    "                model=self.model_name,\n",
    "                messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "                max_tokens=max_tokens,\n",
    "                temperature=0.7\n",
    "            )\n",
    "            return response.choices[0].message.content\n",
    "        except Exception as e:\n",
    "            logger.error(f\"LLM query error: {str(e)}\")\n",
    "            return f\"Error querying LLM: {str(e)}\"\n",
    "    \n",
    "    def _build_comprehensive_analysis_prompt(self, paper_data: Dict) -> str:\n",
    "        \"\"\"Build a comprehensive analysis prompt for LLM\"\"\"\n",
    "        return f\"\"\"\n",
    "        As an expert researcher, provide a comprehensive analysis of this paper:\n",
    "        \n",
    "        Title: {paper_data.get('title', 'Unknown')}\n",
    "        Abstract: {paper_data.get('abstract', 'No abstract available')}\n",
    "        Authors: {', '.join(paper_data.get('authors', []))}\n",
    "        Journal: {paper_data.get('journal', 'Unknown')}\n",
    "        \n",
    "        Please analyze:\n",
    "        1. Core contributions and innovations\n",
    "        2. Methodological strengths and limitations\n",
    "        3. Theoretical implications\n",
    "        4. Practical applications\n",
    "        5. Connections to broader research trends\n",
    "        6. Potential future research directions\n",
    "        7. Critical assessment of claims and evidence\n",
    "        8. Interdisciplinary relevance\n",
    "        \n",
    "        Provide detailed, evidence-based insights.\n",
    "        \"\"\"\n",
    "    \n",
    "    def _build_standard_analysis_prompt(self, paper_data: Dict) -> str:\n",
    "        \"\"\"Build a standard analysis prompt\"\"\"\n",
    "        return f\"\"\"\n",
    "        Analyze this research paper and provide insights:\n",
    "        \n",
    "        Title: {paper_data.get('title', 'Unknown')}\n",
    "        Abstract: {paper_data.get('abstract', 'No abstract available')}\n",
    "        \n",
    "        Focus on:\n",
    "        1. Key contributions\n",
    "        2. Methodology assessment\n",
    "        3. Significance of findings\n",
    "        4. Limitations and future work\n",
    "        5. Overall impact potential\n",
    "        \"\"\"\n",
    "    \n",
    "    def _build_quick_analysis_prompt(self, paper_data: Dict) -> str:\n",
    "        \"\"\"Build a quick analysis prompt\"\"\"\n",
    "        return f\"\"\"\n",
    "        Quickly summarize this paper's key points:\n",
    "        \n",
    "        Title: {paper_data.get('title', 'Unknown')}\n",
    "        Abstract: {paper_data.get('abstract', 'No abstract available')}\n",
    "        \n",
    "        Provide: Main contribution, methodology, and significance.\n",
    "        \"\"\"\n",
    "    \n",
    "    def _build_expert_analysis_prompt(self, paper_data: Dict) -> str:\n",
    "        \"\"\"Build an expert-level analysis prompt\"\"\"\n",
    "        return f\"\"\"\n",
    "        As a leading expert in this field, provide a rigorous analysis:\n",
    "        \n",
    "        Title: {paper_data.get('title', 'Unknown')}\n",
    "        Abstract: {paper_data.get('abstract', 'No abstract available')}\n",
    "        Authors: {', '.join(paper_data.get('authors', []))}\n",
    "        \n",
    "        Expert-level analysis should include:\n",
    "        1. Technical depth assessment\n",
    "        2. Novelty evaluation against state-of-the-art\n",
    "        3. Methodological rigor evaluation\n",
    "        4. Reproducibility assessment\n",
    "        5. Theoretical soundness\n",
    "        6. Experimental design critique\n",
    "        7. Statistical analysis validity\n",
    "        8. Broader field implications\n",
    "        9. Potential for paradigm shift\n",
    "        10. Recommendations for peer review\n",
    "        \"\"\"\n",
    "    \n",
    "    async def _fallback_analysis(self, paper_data: Dict) -> Dict:\n",
    "        \"\"\"Fallback analysis when LLM is not available\"\"\"\n",
    "        return {\n",
    "            \"llm_insights\": {\"note\": \"LLM analysis not available, using traditional methods\"},\n",
    "            \"traditional_metrics\": await paper_analysis.analyze_paper(paper_data),\n",
    "            \"confidence_score\": 0.5\n",
    "        }\n",
    "    \n",
    "    async def _parse_llm_analysis(self, response: str) -> Dict:\n",
    "        \"\"\"Parse LLM analysis response into structured format\"\"\"\n",
    "        # Simple parsing - could be enhanced with more sophisticated NLP\n",
    "        lines = response.split('\\n')\n",
    "        analysis = {\n",
    "            \"key_points\": [],\n",
    "            \"contributions\": [],\n",
    "            \"limitations\": [],\n",
    "            \"implications\": [],\n",
    "            \"raw_response\": response\n",
    "        }\n",
    "        \n",
    "        current_section = None\n",
    "        for line in lines:\n",
    "            line = line.strip()\n",
    "            if \"contribution\" in line.lower():\n",
    "                current_section = \"contributions\"\n",
    "            elif \"limitation\" in line.lower():\n",
    "                current_section = \"limitations\"\n",
    "            elif \"implication\" in line.lower():\n",
    "                current_section = \"implications\"\n",
    "            elif line and current_section:\n",
    "                analysis[current_section].append(line)\n",
    "        \n",
    "        return analysis\n",
    "    \n",
    "    def _calculate_analysis_confidence(self, analysis: Dict) -> float:\n",
    "        \"\"\"Calculate confidence score for analysis\"\"\"\n",
    "        # Simple confidence calculation based on content richness\n",
    "        content_items = len(analysis.get(\"key_points\", [])) + len(analysis.get(\"contributions\", []))\n",
    "        return min(content_items / 10.0, 1.0)\n",
    "\n",
    "# Initialize LLM Research Agent\n",
    "llm_research_agent = LLMResearchAgent()\n",
    "\n",
    "print(\"üß† LLM Research Agent initialized!\")\n",
    "print(\"ü§ñ Features: Deep paper analysis, hypothesis generation, literature synthesis\")\n",
    "print(\"üéØ Advanced: Research question optimization, cross-domain transfer\")\n",
    "print(\"üí° Capabilities: Intelligent reasoning, creative insights, expert-level analysis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed03fcfa",
   "metadata": {},
   "source": [
    "# üåç Enhanced Features Implementation\n",
    "\n",
    "## Multilingual Support, Citation Networks, Custom Alerts, and More\n",
    "\n",
    "This section implements the advanced optional features for the research assistant:\n",
    "\n",
    "1. **Multilingual Support**: Search and translate non-English research\n",
    "2. **Citation Network Visualization**: Visual maps of paper interconnections  \n",
    "3. **Custom Alerts**: Personalized notifications for research updates\n",
    "4. **Reproducible Notebook Generator**: Convert paper methodologies to executable code\n",
    "5. **Plagiarism and Integrity Checker**: Academic integrity validation\n",
    "\n",
    "Let's implement these cutting-edge capabilities!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0a9c35f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multilingual Research Support Service\n",
    "import translators as ts\n",
    "from langdetect import detect\n",
    "import openai\n",
    "\n",
    "class MultilingualResearchService:\n",
    "    \"\"\"Service for searching and translating non-English research papers\"\"\"\n",
    "    \n",
    "    def __init__(self, openai_api_key: str = None):\n",
    "        self.openai_client = openai.OpenAI(api_key=openai_api_key) if openai_api_key else None\n",
    "        self.supported_languages = {\n",
    "            'en': 'English', 'es': 'Spanish', 'fr': 'French', 'de': 'German',\n",
    "            'it': 'Italian', 'pt': 'Portuguese', 'ru': 'Russian', 'zh': 'Chinese',\n",
    "            'ja': 'Japanese', 'ko': 'Korean', 'ar': 'Arabic', 'hi': 'Hindi'\n",
    "        }\n",
    "        \n",
    "    async def detect_language(self, text: str) -> str:\n",
    "        \"\"\"Detect the language of given text\"\"\"\n",
    "        try:\n",
    "            return detect(text)\n",
    "        except:\n",
    "            return 'en'  # Default to English\n",
    "    \n",
    "    async def translate_text(self, text: str, target_language: str = 'en', source_language: str = None) -> Dict[str, Any]:\n",
    "        \"\"\"Translate text to target language with multiple fallback options\"\"\"\n",
    "        \n",
    "        if not source_language:\n",
    "            source_language = await self.detect_language(text)\n",
    "        \n",
    "        if source_language == target_language:\n",
    "            return {\n",
    "                'original_text': text,\n",
    "                'translated_text': text,\n",
    "                'source_language': source_language,\n",
    "                'target_language': target_language,\n",
    "                'confidence': 1.0\n",
    "            }\n",
    "        \n",
    "        translation_methods = []\n",
    "        \n",
    "        # Method 1: OpenAI GPT translation (highest quality)\n",
    "        if self.openai_client:\n",
    "            try:\n",
    "                response = await self._translate_with_gpt(text, source_language, target_language)\n",
    "                translation_methods.append(('openai', response, 0.95))\n",
    "            except Exception as e:\n",
    "                logger.warning(f\"OpenAI translation failed: {e}\")\n",
    "        \n",
    "        # Method 2: Google Translate\n",
    "        try:\n",
    "            google_translation = ts.translate_text(text, translator='google', \n",
    "                                                 from_language=source_language, \n",
    "                                                 to_language=target_language)\n",
    "            translation_methods.append(('google', google_translation, 0.85))\n",
    "        except Exception as e:\n",
    "            logger.warning(f\"Google translation failed: {e}\")\n",
    "        \n",
    "        # Method 3: Bing Translator  \n",
    "        try:\n",
    "            bing_translation = ts.translate_text(text, translator='bing',\n",
    "                                               from_language=source_language,\n",
    "                                               to_language=target_language)\n",
    "            translation_methods.append(('bing', bing_translation, 0.8))\n",
    "        except Exception as e:\n",
    "            logger.warning(f\"Bing translation failed: {e}\")\n",
    "        \n",
    "        if translation_methods:\n",
    "            # Use the highest confidence translation\n",
    "            best_method, best_translation, confidence = max(translation_methods, key=lambda x: x[2])\n",
    "            \n",
    "            return {\n",
    "                'original_text': text,\n",
    "                'translated_text': best_translation,\n",
    "                'source_language': source_language,\n",
    "                'target_language': target_language,\n",
    "                'translation_method': best_method,\n",
    "                'confidence': confidence\n",
    "            }\n",
    "        else:\n",
    "            raise Exception(\"All translation methods failed\")\n",
    "    \n",
    "    async def _translate_with_gpt(self, text: str, source_lang: str, target_lang: str) -> str:\n",
    "        \"\"\"High-quality translation using GPT models\"\"\"\n",
    "        \n",
    "        source_name = self.supported_languages.get(source_lang, source_lang)\n",
    "        target_name = self.supported_languages.get(target_lang, target_lang)\n",
    "        \n",
    "        prompt = f\"\"\"\n",
    "        Translate the following academic text from {source_name} to {target_name}. \n",
    "        Maintain technical terminology and academic tone. Preserve any citations, equations, or special formatting.\n",
    "        \n",
    "        Text to translate:\n",
    "        {text}\n",
    "        \n",
    "        Translation:\n",
    "        \"\"\"\n",
    "        \n",
    "        response = self.openai_client.chat.completions.create(\n",
    "            model=\"gpt-4\",\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "            temperature=0.1\n",
    "        )\n",
    "        \n",
    "        return response.choices[0].message.content.strip()\n",
    "    \n",
    "    async def search_multilingual_papers(self, query: str, languages: List[str] = None, \n",
    "                                       target_language: str = 'en') -> List[Dict]:\n",
    "        \"\"\"Search for papers in multiple languages and translate results\"\"\"\n",
    "        \n",
    "        if not languages:\n",
    "            languages = ['en', 'es', 'fr', 'de', 'zh', 'ja']  # Common academic languages\n",
    "        \n",
    "        all_results = []\n",
    "        \n",
    "        for lang in languages:\n",
    "            try:\n",
    "                # Translate query to target language if needed\n",
    "                if lang != 'en':\n",
    "                    translated_query = await self.translate_text(query, target_language=lang)\n",
    "                    search_query = translated_query['translated_text']\n",
    "                else:\n",
    "                    search_query = query\n",
    "                \n",
    "                # Search using language-specific query\n",
    "                results = await self._search_language_specific(search_query, lang)\n",
    "                \n",
    "                # Translate results back to target language\n",
    "                for result in results:\n",
    "                    if result.get('language', 'en') != target_language:\n",
    "                        # Translate title and abstract\n",
    "                        if result.get('title'):\n",
    "                            title_translation = await self.translate_text(\n",
    "                                result['title'], target_language=target_language\n",
    "                            )\n",
    "                            result['title_translated'] = title_translation['translated_text']\n",
    "                            result['title_original'] = result['title']\n",
    "                        \n",
    "                        if result.get('abstract'):\n",
    "                            abstract_translation = await self.translate_text(\n",
    "                                result['abstract'], target_language=target_language\n",
    "                            )\n",
    "                            result['abstract_translated'] = abstract_translation['translated_text']\n",
    "                            result['abstract_original'] = result['abstract']\n",
    "                    \n",
    "                    result['search_language'] = lang\n",
    "                    all_results.append(result)\n",
    "                    \n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error searching in language {lang}: {e}\")\n",
    "                continue\n",
    "        \n",
    "        # Remove duplicates and rank results\n",
    "        unique_results = self._deduplicate_multilingual_results(all_results)\n",
    "        return unique_results[:50]  # Return top 50 results\n",
    "    \n",
    "    async def _search_language_specific(self, query: str, language: str) -> List[Dict]:\n",
    "        \"\"\"Search for papers in a specific language using various databases\"\"\"\n",
    "        results = []\n",
    "        \n",
    "        # Search ArXiv (mostly English but has some other languages)\n",
    "        if language in ['en', 'fr', 'de']:\n",
    "            arxiv_results = await self._search_arxiv_language(query, language)\n",
    "            results.extend(arxiv_results)\n",
    "        \n",
    "        # Search language-specific databases\n",
    "        if language == 'zh':\n",
    "            # Chinese databases like CNKI, Wanfang\n",
    "            cnki_results = await self._search_cnki(query)\n",
    "            results.extend(cnki_results)\n",
    "        elif language == 'ja':\n",
    "            # Japanese databases like CiNii\n",
    "            cinii_results = await self._search_cinii(query)\n",
    "            results.extend(cinii_results)\n",
    "        elif language == 'es':\n",
    "            # Spanish databases like SciELO\n",
    "            scielo_results = await self._search_scielo(query)\n",
    "            results.extend(scielo_results)\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    async def _search_cnki(self, query: str) -> List[Dict]:\n",
    "        \"\"\"Search Chinese National Knowledge Infrastructure (CNKI)\"\"\"\n",
    "        # Placeholder for CNKI API integration\n",
    "        # In practice, you would integrate with CNKI's API\n",
    "        return []\n",
    "    \n",
    "    async def _search_cinii(self, query: str) -> List[Dict]:\n",
    "        \"\"\"Search Japanese CiNii database\"\"\"\n",
    "        # Placeholder for CiNii API integration\n",
    "        return []\n",
    "    \n",
    "    async def _search_scielo(self, query: str) -> List[Dict]:\n",
    "        \"\"\"Search SciELO for Spanish/Portuguese papers\"\"\"\n",
    "        # Placeholder for SciELO API integration\n",
    "        return []\n",
    "    \n",
    "    async def _search_arxiv_language(self, query: str, language: str) -> List[Dict]:\n",
    "        \"\"\"Search ArXiv with language filtering\"\"\"\n",
    "        # This would use the existing ArXiv search but filter by language\n",
    "        # ArXiv papers are mostly English, but some are in other languages\n",
    "        return []\n",
    "    \n",
    "    def _deduplicate_multilingual_results(self, results: List[Dict]) -> List[Dict]:\n",
    "        \"\"\"Remove duplicate papers across different languages\"\"\"\n",
    "        seen_dois = set()\n",
    "        seen_titles = set()\n",
    "        unique_results = []\n",
    "        \n",
    "        for result in results:\n",
    "            # Check for DOI duplicates\n",
    "            if result.get('doi') and result['doi'] in seen_dois:\n",
    "                continue\n",
    "            \n",
    "            # Check for title similarity (fuzzy matching)\n",
    "            title = result.get('title', '').lower().strip()\n",
    "            title_translated = result.get('title_translated', '').lower().strip()\n",
    "            \n",
    "            is_duplicate = False\n",
    "            for seen_title in seen_titles:\n",
    "                if self._titles_similar(title, seen_title) or \\\n",
    "                   self._titles_similar(title_translated, seen_title):\n",
    "                    is_duplicate = True\n",
    "                    break\n",
    "            \n",
    "            if not is_duplicate:\n",
    "                unique_results.append(result)\n",
    "                if result.get('doi'):\n",
    "                    seen_dois.add(result['doi'])\n",
    "                seen_titles.add(title)\n",
    "                if title_translated:\n",
    "                    seen_titles.add(title_translated)\n",
    "        \n",
    "        return unique_results\n",
    "    \n",
    "    def _titles_similar(self, title1: str, title2: str, threshold: float = 0.85) -> bool:\n",
    "        \"\"\"Check if two titles are similar using simple string similarity\"\"\"\n",
    "        if not title1 or not title2:\n",
    "            return False\n",
    "        \n",
    "        # Simple Jaccard similarity for quick comparison\n",
    "        words1 = set(title1.split())\n",
    "        words2 = set(title2.split())\n",
    "        \n",
    "        if len(words1) == 0 and len(words2) == 0:\n",
    "            return True\n",
    "        \n",
    "        intersection = words1.intersection(words2)\n",
    "        union = words1.union(words2)\n",
    "        \n",
    "        return len(intersection) / len(union) >= threshold\n",
    "\n",
    "print(\"üåç Multilingual Research Service implemented!\")\n",
    "print(\"‚úÖ Features: Language detection, translation, multilingual search\")\n",
    "print(\"üîß Supports: English, Spanish, French, German, Chinese, Japanese, and more\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c6e6445",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Citation Network Visualization Service\n",
    "import networkx as nx\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "from plotly.subplots import make_subplots\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "import community as community_louvain  # For community detection\n",
    "\n",
    "class CitationNetworkService:\n",
    "    \"\"\"Service for generating visual maps of how papers cite each other\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.citation_graph = nx.DiGraph()  # Directed graph for citations\n",
    "        self.paper_cache = {}\n",
    "        \n",
    "    async def build_citation_network(self, paper_ids: List[int], depth: int = 2) -> nx.DiGraph:\n",
    "        \"\"\"Build citation network starting from seed papers with specified depth\"\"\"\n",
    "        \n",
    "        graph = nx.DiGraph()\n",
    "        processed_papers = set()\n",
    "        papers_to_process = set(paper_ids)\n",
    "        \n",
    "        for level in range(depth + 1):\n",
    "            if not papers_to_process:\n",
    "                break\n",
    "                \n",
    "            current_level_papers = papers_to_process.copy()\n",
    "            papers_to_process = set()\n",
    "            \n",
    "            for paper_id in current_level_papers:\n",
    "                if paper_id in processed_papers:\n",
    "                    continue\n",
    "                \n",
    "                processed_papers.add(paper_id)\n",
    "                \n",
    "                # Get paper information\n",
    "                paper_info = await self._get_paper_info(paper_id)\n",
    "                graph.add_node(paper_id, **paper_info)\n",
    "                \n",
    "                # Get papers this paper cites (backward citations)\n",
    "                cited_papers = await self._get_cited_papers(paper_id)\n",
    "                for cited_id, citation_context in cited_papers:\n",
    "                    graph.add_edge(paper_id, cited_id, \n",
    "                                 relationship='cites',\n",
    "                                 context=citation_context)\n",
    "                    if level < depth:\n",
    "                        papers_to_process.add(cited_id)\n",
    "                \n",
    "                # Get papers that cite this paper (forward citations)\n",
    "                citing_papers = await self._get_citing_papers(paper_id)\n",
    "                for citing_id, citation_context in citing_papers:\n",
    "                    graph.add_edge(citing_id, paper_id,\n",
    "                                 relationship='cites', \n",
    "                                 context=citation_context)\n",
    "                    if level < depth:\n",
    "                        papers_to_process.add(citing_id)\n",
    "        \n",
    "        # Calculate network metrics\n",
    "        self._calculate_network_metrics(graph)\n",
    "        \n",
    "        return graph\n",
    "    \n",
    "    async def _get_paper_info(self, paper_id: int) -> Dict[str, Any]:\n",
    "        \"\"\"Get comprehensive paper information for network nodes\"\"\"\n",
    "        \n",
    "        if paper_id in self.paper_cache:\n",
    "            return self.paper_cache[paper_id]\n",
    "        \n",
    "        # In practice, this would query your database\n",
    "        paper_info = {\n",
    "            'id': paper_id,\n",
    "            'title': f\"Paper {paper_id}\",  # Fetch from database\n",
    "            'authors': ['Author A', 'Author B'],  # Fetch from database\n",
    "            'year': 2023,  # Fetch from database\n",
    "            'citation_count': 10,  # Fetch from database\n",
    "            'journal': 'Nature',  # Fetch from database\n",
    "            'keywords': ['AI', 'Machine Learning'],  # Fetch from database\n",
    "        }\n",
    "        \n",
    "        self.paper_cache[paper_id] = paper_info\n",
    "        return paper_info\n",
    "    \n",
    "    async def _get_cited_papers(self, paper_id: int) -> List[Tuple[int, str]]:\n",
    "        \"\"\"Get papers cited by this paper with citation context\"\"\"\n",
    "        # In practice, this would query your citation database\n",
    "        return [(paper_id + 1, \"supports methodology\"), (paper_id + 2, \"related work\")]\n",
    "    \n",
    "    async def _get_citing_papers(self, paper_id: int) -> List[Tuple[int, str]]:\n",
    "        \"\"\"Get papers that cite this paper with citation context\"\"\"\n",
    "        # In practice, this would query your citation database  \n",
    "        return [(paper_id - 1, \"builds upon results\"), (paper_id - 2, \"extends approach\")]\n",
    "    \n",
    "    def _calculate_network_metrics(self, graph: nx.DiGraph):\n",
    "        \"\"\"Calculate various network metrics for nodes\"\"\"\n",
    "        \n",
    "        # PageRank for influence ranking\n",
    "        pagerank = nx.pagerank(graph)\n",
    "        \n",
    "        # Betweenness centrality for bridging papers\n",
    "        betweenness = nx.betweenness_centrality(graph)\n",
    "        \n",
    "        # In-degree and out-degree centrality\n",
    "        in_degree = dict(graph.in_degree())\n",
    "        out_degree = dict(graph.out_degree())\n",
    "        \n",
    "        # Add metrics as node attributes\n",
    "        for node in graph.nodes():\n",
    "            graph.nodes[node]['pagerank'] = pagerank.get(node, 0)\n",
    "            graph.nodes[node]['betweenness'] = betweenness.get(node, 0)\n",
    "            graph.nodes[node]['in_degree'] = in_degree.get(node, 0)\n",
    "            graph.nodes[node]['out_degree'] = out_degree.get(node, 0)\n",
    "            graph.nodes[node]['total_degree'] = in_degree.get(node, 0) + out_degree.get(node, 0)\n",
    "    \n",
    "    def create_interactive_visualization(self, graph: nx.DiGraph, \n",
    "                                       layout_algorithm: str = 'spring') -> go.Figure:\n",
    "        \"\"\"Create interactive Plotly visualization of citation network\"\"\"\n",
    "        \n",
    "        # Calculate node positions using specified layout\n",
    "        if layout_algorithm == 'spring':\n",
    "            pos = nx.spring_layout(graph, k=3, iterations=50)\n",
    "        elif layout_algorithm == 'circular':\n",
    "            pos = nx.circular_layout(graph)\n",
    "        elif layout_algorithm == 'kamada_kawai':\n",
    "            pos = nx.kamada_kawai_layout(graph)\n",
    "        else:\n",
    "            pos = nx.spring_layout(graph)\n",
    "        \n",
    "        # Extract node and edge information\n",
    "        node_x = [pos[node][0] for node in graph.nodes()]\n",
    "        node_y = [pos[node][1] for node in graph.nodes()]\n",
    "        \n",
    "        edge_x = []\n",
    "        edge_y = []\n",
    "        for edge in graph.edges():\n",
    "            x0, y0 = pos[edge[0]]\n",
    "            x1, y1 = pos[edge[1]]\n",
    "            edge_x.extend([x0, x1, None])\n",
    "            edge_y.extend([y0, y1, None])\n",
    "        \n",
    "        # Create edge trace\n",
    "        edge_trace = go.Scatter(\n",
    "            x=edge_x, y=edge_y,\n",
    "            line=dict(width=1, color='rgba(125,125,125,0.5)'),\n",
    "            hoverinfo='none',\n",
    "            mode='lines'\n",
    "        )\n",
    "        \n",
    "        # Prepare node data\n",
    "        node_info = []\n",
    "        node_colors = []\n",
    "        node_sizes = []\n",
    "        \n",
    "        for node in graph.nodes():\n",
    "            node_data = graph.nodes[node]\n",
    "            \n",
    "            # Node color based on PageRank (influence)\n",
    "            pagerank_score = node_data.get('pagerank', 0)\n",
    "            node_colors.append(pagerank_score)\n",
    "            \n",
    "            # Node size based on citation count\n",
    "            citation_count = node_data.get('citation_count', 0)\n",
    "            node_sizes.append(max(10, min(50, citation_count * 2)))  # Scale between 10-50\n",
    "            \n",
    "            # Hover text with paper information\n",
    "            hover_text = f\"<b>{node_data.get('title', 'Unknown')}</b><br>\"\n",
    "            hover_text += f\"Authors: {', '.join(node_data.get('authors', []))}<br>\"\n",
    "            hover_text += f\"Year: {node_data.get('year', 'N/A')}<br>\"\n",
    "            hover_text += f\"Citations: {citation_count}<br>\"\n",
    "            hover_text += f\"Influence Score: {pagerank_score:.3f}<br>\"\n",
    "            hover_text += f\"Journal: {node_data.get('journal', 'N/A')}\"\n",
    "            \n",
    "            node_info.append(hover_text)\n",
    "        \n",
    "        # Create node trace\n",
    "        node_trace = go.Scatter(\n",
    "            x=node_x, y=node_y,\n",
    "            mode='markers',\n",
    "            hoverinfo='text',\n",
    "            text=node_info,\n",
    "            marker=dict(\n",
    "                size=node_sizes,\n",
    "                color=node_colors,\n",
    "                colorscale='Viridis',\n",
    "                colorbar=dict(\n",
    "                    title=\"Influence Score (PageRank)\",\n",
    "                    titleside=\"right\"\n",
    "                ),\n",
    "                line=dict(width=1, color='white')\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        # Create the figure\n",
    "        fig = go.Figure(\n",
    "            data=[edge_trace, node_trace],\n",
    "            layout=go.Layout(\n",
    "                title='Citation Network Visualization',\n",
    "                titlefont_size=16,\n",
    "                showlegend=False,\n",
    "                hovermode='closest',\n",
    "                margin=dict(b=20,l=5,r=5,t=40),\n",
    "                annotations=[ dict(\n",
    "                    text=\"Node size = citation count, Color = influence score\",\n",
    "                    showarrow=False,\n",
    "                    xref=\"paper\", yref=\"paper\",\n",
    "                    x=0.005, y=-0.002,\n",
    "                    xanchor='left', yanchor='bottom',\n",
    "                    font=dict(size=12)\n",
    "                )],\n",
    "                xaxis=dict(showgrid=False, zeroline=False, showticklabels=False),\n",
    "                yaxis=dict(showgrid=False, zeroline=False, showticklabels=False),\n",
    "                plot_bgcolor='white'\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        return fig\n",
    "    \n",
    "    def detect_research_clusters(self, graph: nx.DiGraph) -> Dict[str, List[int]]:\n",
    "        \"\"\"Detect clusters/communities in the citation network\"\"\"\n",
    "        \n",
    "        # Convert to undirected graph for community detection\n",
    "        undirected_graph = graph.to_undirected()\n",
    "        \n",
    "        # Use Louvain algorithm for community detection\n",
    "        communities = community_louvain.best_partition(undirected_graph)\n",
    "        \n",
    "        # Group papers by community\n",
    "        clusters = defaultdict(list)\n",
    "        for paper_id, cluster_id in communities.items():\n",
    "            clusters[f\"Cluster {cluster_id}\"].append(paper_id)\n",
    "        \n",
    "        return dict(clusters)\n",
    "    \n",
    "    def analyze_citation_patterns(self, graph: nx.DiGraph) -> Dict[str, Any]:\n",
    "        \"\"\"Analyze citation patterns and provide insights\"\"\"\n",
    "        \n",
    "        analysis = {\n",
    "            'network_stats': {\n",
    "                'total_papers': graph.number_of_nodes(),\n",
    "                'total_citations': graph.number_of_edges(),\n",
    "                'density': nx.density(graph),\n",
    "                'avg_citations_per_paper': graph.number_of_edges() / graph.number_of_nodes() if graph.number_of_nodes() > 0 else 0\n",
    "            },\n",
    "            'influential_papers': [],\n",
    "            'bridge_papers': [],\n",
    "            'recent_trends': {},\n",
    "            'research_clusters': self.detect_research_clusters(graph)\n",
    "        }\n",
    "        \n",
    "        # Find most influential papers (high PageRank)\n",
    "        papers_by_influence = sorted(\n",
    "            graph.nodes(data=True),\n",
    "            key=lambda x: x[1].get('pagerank', 0),\n",
    "            reverse=True\n",
    "        )\n",
    "        analysis['influential_papers'] = [\n",
    "            {\n",
    "                'paper_id': paper_id,\n",
    "                'title': data.get('title', 'Unknown'),\n",
    "                'influence_score': data.get('pagerank', 0),\n",
    "                'citation_count': data.get('citation_count', 0)\n",
    "            }\n",
    "            for paper_id, data in papers_by_influence[:10]\n",
    "        ]\n",
    "        \n",
    "        # Find bridge papers (high betweenness centrality)\n",
    "        papers_by_bridging = sorted(\n",
    "            graph.nodes(data=True),\n",
    "            key=lambda x: x[1].get('betweenness', 0),\n",
    "            reverse=True\n",
    "        )\n",
    "        analysis['bridge_papers'] = [\n",
    "            {\n",
    "                'paper_id': paper_id,\n",
    "                'title': data.get('title', 'Unknown'),\n",
    "                'bridging_score': data.get('betweenness', 0),\n",
    "                'connects_communities': True\n",
    "            }\n",
    "            for paper_id, data in papers_by_bridging[:10]\n",
    "        ]\n",
    "        \n",
    "        return analysis\n",
    "    \n",
    "    def create_cluster_visualization(self, graph: nx.DiGraph) -> go.Figure:\n",
    "        \"\"\"Create visualization highlighting research clusters\"\"\"\n",
    "        \n",
    "        clusters = self.detect_research_clusters(graph)\n",
    "        \n",
    "        # Assign colors to clusters\n",
    "        color_palette = px.colors.qualitative.Set3\n",
    "        cluster_colors = {}\n",
    "        for i, cluster_name in enumerate(clusters.keys()):\n",
    "            cluster_colors[cluster_name] = color_palette[i % len(color_palette)]\n",
    "        \n",
    "        # Calculate layout\n",
    "        pos = nx.spring_layout(graph, k=3, iterations=50)\n",
    "        \n",
    "        fig = go.Figure()\n",
    "        \n",
    "        # Add edges\n",
    "        edge_x = []\n",
    "        edge_y = []\n",
    "        for edge in graph.edges():\n",
    "            x0, y0 = pos[edge[0]]\n",
    "            x1, y1 = pos[edge[1]]\n",
    "            edge_x.extend([x0, x1, None])\n",
    "            edge_y.extend([y0, y1, None])\n",
    "        \n",
    "        fig.add_trace(go.Scatter(\n",
    "            x=edge_x, y=edge_y,\n",
    "            line=dict(width=0.5, color='rgba(125,125,125,0.3)'),\n",
    "            hoverinfo='none',\n",
    "            mode='lines',\n",
    "            name='Citations'\n",
    "        ))\n",
    "        \n",
    "        # Add nodes by cluster\n",
    "        for cluster_name, paper_ids in clusters.items():\n",
    "            cluster_x = [pos[paper_id][0] for paper_id in paper_ids if paper_id in pos]\n",
    "            cluster_y = [pos[paper_id][1] for paper_id in paper_ids if paper_id in pos]\n",
    "            \n",
    "            cluster_texts = []\n",
    "            for paper_id in paper_ids:\n",
    "                if paper_id in graph.nodes:\n",
    "                    node_data = graph.nodes[paper_id]\n",
    "                    text = f\"{node_data.get('title', 'Unknown')}<br>Year: {node_data.get('year', 'N/A')}\"\n",
    "                    cluster_texts.append(text)\n",
    "            \n",
    "            fig.add_trace(go.Scatter(\n",
    "                x=cluster_x, y=cluster_y,\n",
    "                mode='markers',\n",
    "                marker=dict(\n",
    "                    size=10,\n",
    "                    color=cluster_colors.get(cluster_name, 'gray'),\n",
    "                    line=dict(width=1, color='white')\n",
    "                ),\n",
    "                text=cluster_texts,\n",
    "                name=cluster_name,\n",
    "                hoverinfo='text'\n",
    "            ))\n",
    "        \n",
    "        fig.update_layout(\n",
    "            title='Citation Network - Research Clusters',\n",
    "            showlegend=True,\n",
    "            hovermode='closest',\n",
    "            margin=dict(b=20,l=5,r=5,t=40),\n",
    "            xaxis=dict(showgrid=False, zeroline=False, showticklabels=False),\n",
    "            yaxis=dict(showgrid=False, zeroline=False, showticklabels=False),\n",
    "            plot_bgcolor='white'\n",
    "        )\n",
    "        \n",
    "        return fig\n",
    "\n",
    "print(\"üï∏Ô∏è Citation Network Visualization Service implemented!\")\n",
    "print(\"‚úÖ Features: Interactive network graphs, cluster detection, influence analysis\")\n",
    "print(\"üé® Visualizations: Plotly interactive charts, community detection, pattern analysis\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21370fd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom Research Alerts System\n",
    "from apscheduler.schedulers.asyncio import AsyncIOScheduler\n",
    "from datetime import datetime, timedelta\n",
    "import smtplib\n",
    "from email.mime.text import MimeText\n",
    "from email.mime.multipart import MimeMultipart\n",
    "import asyncio\n",
    "from typing import Callable\n",
    "\n",
    "class ResearchAlertsService:\n",
    "    \"\"\"Service for managing personalized research alerts and notifications\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.scheduler = AsyncIOScheduler()\n",
    "        self.alert_subscriptions = {}\n",
    "        self.notification_channels = {}\n",
    "        self.alert_history = []\n",
    "        \n",
    "    async def create_author_alert(self, user_id: str, author_name: str, \n",
    "                                alert_frequency: str = 'weekly') -> str:\n",
    "        \"\"\"Create alert for when a specific author publishes new work\"\"\"\n",
    "        \n",
    "        alert_id = str(uuid.uuid4())\n",
    "        \n",
    "        alert_config = {\n",
    "            'id': alert_id,\n",
    "            'user_id': user_id,\n",
    "            'type': 'author_publication',\n",
    "            'author_name': author_name,\n",
    "            'frequency': alert_frequency,\n",
    "            'created_at': datetime.utcnow(),\n",
    "            'last_triggered': None,\n",
    "            'is_active': True\n",
    "        }\n",
    "        \n",
    "        self.alert_subscriptions[alert_id] = alert_config\n",
    "        \n",
    "        # Schedule the alert check\n",
    "        if alert_frequency == 'daily':\n",
    "            self.scheduler.add_job(\n",
    "                self._check_author_publications,\n",
    "                'interval',\n",
    "                days=1,\n",
    "                args=[alert_id],\n",
    "                id=f\"author_alert_{alert_id}\"\n",
    "            )\n",
    "        elif alert_frequency == 'weekly':\n",
    "            self.scheduler.add_job(\n",
    "                self._check_author_publications,\n",
    "                'interval',\n",
    "                weeks=1,\n",
    "                args=[alert_id],\n",
    "                id=f\"author_alert_{alert_id}\"\n",
    "            )\n",
    "        \n",
    "        return alert_id\n",
    "    \n",
    "    async def create_citation_threshold_alert(self, user_id: str, paper_title: str,\n",
    "                                            citation_threshold: int, \n",
    "                                            time_period: str = '1 year') -> str:\n",
    "        \"\"\"Create alert for when a paper gets more than X citations in Y time\"\"\"\n",
    "        \n",
    "        alert_id = str(uuid.uuid4())\n",
    "        \n",
    "        alert_config = {\n",
    "            'id': alert_id,\n",
    "            'user_id': user_id,\n",
    "            'type': 'citation_threshold',\n",
    "            'paper_title': paper_title,\n",
    "            'citation_threshold': citation_threshold,\n",
    "            'time_period': time_period,\n",
    "            'created_at': datetime.utcnow(),\n",
    "            'last_triggered': None,\n",
    "            'is_active': True\n",
    "        }\n",
    "        \n",
    "        self.alert_subscriptions[alert_id] = alert_config\n",
    "        \n",
    "        # Check weekly for citation thresholds\n",
    "        self.scheduler.add_job(\n",
    "            self._check_citation_threshold,\n",
    "            'interval',\n",
    "            weeks=1,\n",
    "            args=[alert_id],\n",
    "            id=f\"citation_alert_{alert_id}\"\n",
    "        )\n",
    "        \n",
    "        return alert_id\n",
    "    \n",
    "    async def create_keyword_trend_alert(self, user_id: str, keywords: List[str],\n",
    "                                       trend_threshold: float = 0.5) -> str:\n",
    "        \"\"\"Create alert for trending keywords in research\"\"\"\n",
    "        \n",
    "        alert_id = str(uuid.uuid4())\n",
    "        \n",
    "        alert_config = {\n",
    "            'id': alert_id,\n",
    "            'user_id': user_id,\n",
    "            'type': 'keyword_trend',\n",
    "            'keywords': keywords,\n",
    "            'trend_threshold': trend_threshold,\n",
    "            'created_at': datetime.utcnow(),\n",
    "            'last_triggered': None,\n",
    "            'is_active': True\n",
    "        }\n",
    "        \n",
    "        self.alert_subscriptions[alert_id] = alert_config\n",
    "        \n",
    "        # Check bi-weekly for keyword trends\n",
    "        self.scheduler.add_job(\n",
    "            self._check_keyword_trends,\n",
    "            'interval',\n",
    "            weeks=2,\n",
    "            args=[alert_id],\n",
    "            id=f\"trend_alert_{alert_id}\"\n",
    "        )\n",
    "        \n",
    "        return alert_id\n",
    "    \n",
    "    async def create_conference_deadline_alert(self, user_id: str, \n",
    "                                             research_areas: List[str]) -> str:\n",
    "        \"\"\"Create alert for upcoming conference deadlines in specific areas\"\"\"\n",
    "        \n",
    "        alert_id = str(uuid.uuid4())\n",
    "        \n",
    "        alert_config = {\n",
    "            'id': alert_id,\n",
    "            'user_id': user_id,\n",
    "            'type': 'conference_deadline',\n",
    "            'research_areas': research_areas,\n",
    "            'advance_notice_days': 30,  # Alert 30 days before deadline\n",
    "            'created_at': datetime.utcnow(),\n",
    "            'last_triggered': None,\n",
    "            'is_active': True\n",
    "        }\n",
    "        \n",
    "        self.alert_subscriptions[alert_id] = alert_config\n",
    "        \n",
    "        # Check monthly for conference deadlines\n",
    "        self.scheduler.add_job(\n",
    "            self._check_conference_deadlines,\n",
    "            'interval',\n",
    "            weeks=4,\n",
    "            args=[alert_id],\n",
    "            id=f\"conference_alert_{alert_id}\"\n",
    "        )\n",
    "        \n",
    "        return alert_id\n",
    "    \n",
    "    async def create_collaboration_opportunity_alert(self, user_id: str,\n",
    "                                                   research_interests: List[str]) -> str:\n",
    "        \"\"\"Create alert for potential collaboration opportunities\"\"\"\n",
    "        \n",
    "        alert_id = str(uuid.uuid4())\n",
    "        \n",
    "        alert_config = {\n",
    "            'id': alert_id,\n",
    "            'user_id': user_id,\n",
    "            'type': 'collaboration_opportunity',\n",
    "            'research_interests': research_interests,\n",
    "            'created_at': datetime.utcnow(),\n",
    "            'last_triggered': None,\n",
    "            'is_active': True\n",
    "        }\n",
    "        \n",
    "        self.alert_subscriptions[alert_id] = alert_config\n",
    "        \n",
    "        # Check monthly for collaboration opportunities\n",
    "        self.scheduler.add_job(\n",
    "            self._check_collaboration_opportunities,\n",
    "            'interval',\n",
    "            weeks=4,\n",
    "            args=[alert_id],\n",
    "            id=f\"collab_alert_{alert_id}\"\n",
    "        )\n",
    "        \n",
    "        return alert_id\n",
    "    \n",
    "    async def _check_author_publications(self, alert_id: str):\n",
    "        \"\"\"Check for new publications by a specific author\"\"\"\n",
    "        \n",
    "        alert_config = self.alert_subscriptions.get(alert_id)\n",
    "        if not alert_config or not alert_config['is_active']:\n",
    "            return\n",
    "        \n",
    "        author_name = alert_config['author_name']\n",
    "        last_check = alert_config.get('last_triggered', alert_config['created_at'])\n",
    "        \n",
    "        # Search for new publications since last check\n",
    "        new_publications = await self._search_recent_publications(\n",
    "            author_name, since_date=last_check\n",
    "        )\n",
    "        \n",
    "        if new_publications:\n",
    "            notification = {\n",
    "                'type': 'author_publication',\n",
    "                'author': author_name,\n",
    "                'new_papers': new_publications,\n",
    "                'count': len(new_publications)\n",
    "            }\n",
    "            \n",
    "            await self._send_notification(alert_config['user_id'], notification)\n",
    "            self.alert_subscriptions[alert_id]['last_triggered'] = datetime.utcnow()\n",
    "    \n",
    "    async def _check_citation_threshold(self, alert_id: str):\n",
    "        \"\"\"Check if a paper has exceeded citation threshold\"\"\"\n",
    "        \n",
    "        alert_config = self.alert_subscriptions.get(alert_id)\n",
    "        if not alert_config or not alert_config['is_active']:\n",
    "            return\n",
    "        \n",
    "        paper_title = alert_config['paper_title']\n",
    "        threshold = alert_config['citation_threshold']\n",
    "        time_period = alert_config['time_period']\n",
    "        \n",
    "        # Get current citation count for the paper\n",
    "        current_citations = await self._get_paper_citation_count(paper_title, time_period)\n",
    "        \n",
    "        if current_citations >= threshold:\n",
    "            notification = {\n",
    "                'type': 'citation_threshold_reached',\n",
    "                'paper_title': paper_title,\n",
    "                'current_citations': current_citations,\n",
    "                'threshold': threshold,\n",
    "                'time_period': time_period\n",
    "            }\n",
    "            \n",
    "            await self._send_notification(alert_config['user_id'], notification)\n",
    "            # Deactivate this alert since threshold is reached\n",
    "            self.alert_subscriptions[alert_id]['is_active'] = False\n",
    "    \n",
    "    async def _check_keyword_trends(self, alert_id: str):\n",
    "        \"\"\"Check for trending keywords\"\"\"\n",
    "        \n",
    "        alert_config = self.alert_subscriptions.get(alert_id)\n",
    "        if not alert_config or not alert_config['is_active']:\n",
    "            return\n",
    "        \n",
    "        keywords = alert_config['keywords']\n",
    "        threshold = alert_config['trend_threshold']\n",
    "        \n",
    "        # Analyze keyword trends over the past month\n",
    "        trending_analysis = await self._analyze_keyword_trends(keywords)\n",
    "        \n",
    "        trending_keywords = [\n",
    "            kw for kw, trend_score in trending_analysis.items() \n",
    "            if trend_score >= threshold\n",
    "        ]\n",
    "        \n",
    "        if trending_keywords:\n",
    "            notification = {\n",
    "                'type': 'keyword_trending',\n",
    "                'trending_keywords': trending_keywords,\n",
    "                'trend_analysis': trending_analysis\n",
    "            }\n",
    "            \n",
    "            await self._send_notification(alert_config['user_id'], notification)\n",
    "            self.alert_subscriptions[alert_id]['last_triggered'] = datetime.utcnow()\n",
    "    \n",
    "    async def _check_conference_deadlines(self, alert_id: str):\n",
    "        \"\"\"Check for upcoming conference deadlines\"\"\"\n",
    "        \n",
    "        alert_config = self.alert_subscriptions.get(alert_id)\n",
    "        if not alert_config or not alert_config['is_active']:\n",
    "            return\n",
    "        \n",
    "        research_areas = alert_config['research_areas']\n",
    "        advance_days = alert_config['advance_notice_days']\n",
    "        \n",
    "        # Get upcoming conferences in the specified research areas\n",
    "        upcoming_conferences = await self._get_upcoming_conferences(\n",
    "            research_areas, days_ahead=advance_days\n",
    "        )\n",
    "        \n",
    "        if upcoming_conferences:\n",
    "            notification = {\n",
    "                'type': 'conference_deadlines',\n",
    "                'conferences': upcoming_conferences,\n",
    "                'advance_notice_days': advance_days\n",
    "            }\n",
    "            \n",
    "            await self._send_notification(alert_config['user_id'], notification)\n",
    "            self.alert_subscriptions[alert_id]['last_triggered'] = datetime.utcnow()\n",
    "    \n",
    "    async def _check_collaboration_opportunities(self, alert_id: str):\n",
    "        \"\"\"Check for potential collaboration opportunities\"\"\"\n",
    "        \n",
    "        alert_config = self.alert_subscriptions.get(alert_id)\n",
    "        if not alert_config or not alert_config['is_active']:\n",
    "            return\n",
    "        \n",
    "        research_interests = alert_config['research_interests']\n",
    "        \n",
    "        # Find researchers with complementary interests\n",
    "        potential_collaborators = await self._find_potential_collaborators(research_interests)\n",
    "        \n",
    "        if potential_collaborators:\n",
    "            notification = {\n",
    "                'type': 'collaboration_opportunities',\n",
    "                'potential_collaborators': potential_collaborators,\n",
    "                'matching_interests': research_interests\n",
    "            }\n",
    "            \n",
    "            await self._send_notification(alert_config['user_id'], notification)\n",
    "            self.alert_subscriptions[alert_id]['last_triggered'] = datetime.utcnow()\n",
    "    \n",
    "    async def _search_recent_publications(self, author_name: str, \n",
    "                                        since_date: datetime) -> List[Dict]:\n",
    "        \"\"\"Search for recent publications by an author\"\"\"\n",
    "        # Placeholder - would integrate with literature search service\n",
    "        return [\n",
    "            {\n",
    "                'title': f\"New paper by {author_name}\",\n",
    "                'publication_date': datetime.utcnow(),\n",
    "                'journal': 'Nature',\n",
    "                'doi': '10.1038/example'\n",
    "            }\n",
    "        ]\n",
    "    \n",
    "    async def _get_paper_citation_count(self, paper_title: str, time_period: str) -> int:\n",
    "        \"\"\"Get current citation count for a paper within time period\"\"\"\n",
    "        # Placeholder - would integrate with citation tracking service\n",
    "        return 75  # Example citation count\n",
    "    \n",
    "    async def _analyze_keyword_trends(self, keywords: List[str]) -> Dict[str, float]:\n",
    "        \"\"\"Analyze trending scores for keywords\"\"\"\n",
    "        # Placeholder - would integrate with trend analysis service\n",
    "        return {kw: 0.8 for kw in keywords}  # Example trend scores\n",
    "    \n",
    "    async def _get_upcoming_conferences(self, research_areas: List[str], \n",
    "                                      days_ahead: int) -> List[Dict]:\n",
    "        \"\"\"Get upcoming conferences in research areas\"\"\"\n",
    "        # Placeholder - would integrate with conference database\n",
    "        return [\n",
    "            {\n",
    "                'name': 'ICML 2024',\n",
    "                'deadline': datetime.utcnow() + timedelta(days=25),\n",
    "                'area': 'Machine Learning',\n",
    "                'location': 'Vienna, Austria'\n",
    "            }\n",
    "        ]\n",
    "    \n",
    "    async def _find_potential_collaborators(self, research_interests: List[str]) -> List[Dict]:\n",
    "        \"\"\"Find researchers with complementary interests\"\"\"\n",
    "        # Placeholder - would integrate with researcher network service\n",
    "        return [\n",
    "            {\n",
    "                'name': 'Dr. Jane Smith',\n",
    "                'affiliation': 'MIT',\n",
    "                'matching_interests': ['AI', 'NLP'],\n",
    "                'recent_papers': 5,\n",
    "                'h_index': 25\n",
    "            }\n",
    "        ]\n",
    "    \n",
    "    async def _send_notification(self, user_id: str, notification: Dict):\n",
    "        \"\"\"Send notification to user via configured channels\"\"\"\n",
    "        \n",
    "        # Log the notification\n",
    "        self.alert_history.append({\n",
    "            'user_id': user_id,\n",
    "            'notification': notification,\n",
    "            'timestamp': datetime.utcnow(),\n",
    "            'delivered': True\n",
    "        })\n",
    "        \n",
    "        # Get user's notification preferences\n",
    "        user_channels = self.notification_channels.get(user_id, ['email'])\n",
    "        \n",
    "        for channel in user_channels:\n",
    "            if channel == 'email':\n",
    "                await self._send_email_notification(user_id, notification)\n",
    "            elif channel == 'webhook':\n",
    "                await self._send_webhook_notification(user_id, notification)\n",
    "            elif channel == 'push':\n",
    "                await self._send_push_notification(user_id, notification)\n",
    "    \n",
    "    async def _send_email_notification(self, user_id: str, notification: Dict):\n",
    "        \"\"\"Send email notification\"\"\"\n",
    "        # Placeholder for email sending logic\n",
    "        print(f\"üìß Email sent to {user_id}: {notification['type']}\")\n",
    "    \n",
    "    async def _send_webhook_notification(self, user_id: str, notification: Dict):\n",
    "        \"\"\"Send webhook notification\"\"\"\n",
    "        # Placeholder for webhook logic\n",
    "        print(f\"üîó Webhook sent to {user_id}: {notification['type']}\")\n",
    "    \n",
    "    async def _send_push_notification(self, user_id: str, notification: Dict):\n",
    "        \"\"\"Send push notification\"\"\"\n",
    "        # Placeholder for push notification logic\n",
    "        print(f\"üì± Push notification sent to {user_id}: {notification['type']}\")\n",
    "    \n",
    "    def configure_notification_channel(self, user_id: str, channels: List[str]):\n",
    "        \"\"\"Configure notification channels for a user\"\"\"\n",
    "        self.notification_channels[user_id] = channels\n",
    "    \n",
    "    def get_user_alerts(self, user_id: str) -> List[Dict]:\n",
    "        \"\"\"Get all alerts for a specific user\"\"\"\n",
    "        return [\n",
    "            alert for alert in self.alert_subscriptions.values()\n",
    "            if alert['user_id'] == user_id\n",
    "        ]\n",
    "    \n",
    "    def deactivate_alert(self, alert_id: str) -> bool:\n",
    "        \"\"\"Deactivate a specific alert\"\"\"\n",
    "        if alert_id in self.alert_subscriptions:\n",
    "            self.alert_subscriptions[alert_id]['is_active'] = False\n",
    "            # Remove from scheduler\n",
    "            try:\n",
    "                self.scheduler.remove_job(f\"author_alert_{alert_id}\")\n",
    "            except:\n",
    "                pass\n",
    "            try:\n",
    "                self.scheduler.remove_job(f\"citation_alert_{alert_id}\")\n",
    "            except:\n",
    "                pass\n",
    "            try:\n",
    "                self.scheduler.remove_job(f\"trend_alert_{alert_id}\")\n",
    "            except:\n",
    "                pass\n",
    "            return True\n",
    "        return False\n",
    "    \n",
    "    def start_scheduler(self):\n",
    "        \"\"\"Start the alert scheduler\"\"\"\n",
    "        self.scheduler.start()\n",
    "        print(\"‚è∞ Research alerts scheduler started!\")\n",
    "    \n",
    "    def stop_scheduler(self):\n",
    "        \"\"\"Stop the alert scheduler\"\"\"\n",
    "        self.scheduler.shutdown()\n",
    "        print(\"‚è∞ Research alerts scheduler stopped!\")\n",
    "\n",
    "print(\"üö® Custom Research Alerts Service implemented!\")\n",
    "print(\"‚úÖ Features: Author alerts, citation thresholds, keyword trends, conference deadlines\")\n",
    "print(\"üì° Notifications: Email, webhook, push notifications with customizable frequency\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03c2d33c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reproducible Notebook Generator Service\n",
    "import nbformat\n",
    "from nbformat.v4 import new_notebook, new_code_cell, new_markdown_cell\n",
    "import re\n",
    "import ast\n",
    "from typing import List, Dict, Any, Optional\n",
    "import openai\n",
    "\n",
    "class ReproducibleNotebookGenerator:\n",
    "    \"\"\"Service for converting paper methodologies into runnable Jupyter notebooks\"\"\"\n",
    "    \n",
    "    def __init__(self, openai_api_key: str = None):\n",
    "        self.openai_client = openai.OpenAI(api_key=openai_api_key) if openai_api_key else None\n",
    "        self.code_templates = self._load_code_templates()\n",
    "        \n",
    "    def _load_code_templates(self) -> Dict[str, str]:\n",
    "        \"\"\"Load common code templates for various research methods\"\"\"\n",
    "        return {\n",
    "            'data_loading': '''# Data Loading and Preprocessing\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load dataset\n",
    "# TODO: Replace with actual dataset path\n",
    "data = pd.read_csv('dataset.csv')\n",
    "\n",
    "# Basic data exploration\n",
    "print(f\"Dataset shape: {data.shape}\")\n",
    "print(f\"Features: {data.columns.tolist()}\")\n",
    "print(f\"Missing values: {data.isnull().sum().sum()}\")\n",
    "\n",
    "# Display first few rows\n",
    "data.head()''',\n",
    "\n",
    "            'statistical_analysis': '''# Statistical Analysis\n",
    "import scipy.stats as stats\n",
    "from scipy.stats import ttest_ind, chi2_contingency, pearsonr\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Basic descriptive statistics\n",
    "print(\"Descriptive Statistics:\")\n",
    "print(data.describe())\n",
    "\n",
    "# TODO: Add specific statistical tests based on methodology\n",
    "# Example: t-test for comparing groups\n",
    "# group1 = data[data['group'] == 'A']['value']\n",
    "# group2 = data[data['group'] == 'B']['value']\n",
    "# t_stat, p_value = ttest_ind(group1, group2)\n",
    "# print(f\"T-test result: t={t_stat:.4f}, p={p_value:.4f}\")''',\n",
    "\n",
    "            'machine_learning': '''# Machine Learning Implementation\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Prepare features and target\n",
    "# TODO: Define features and target based on methodology\n",
    "X = data.drop('target', axis=1)  # Replace 'target' with actual target column\n",
    "y = data['target']\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize model\n",
    "# TODO: Choose appropriate model based on methodology\n",
    "model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "\n",
    "# Train model\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(\"\\\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred))''',\n",
    "\n",
    "            'deep_learning': '''# Deep Learning Implementation\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Define neural network architecture\n",
    "class ResearchModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(ResearchModel, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.fc3 = nn.Linear(hidden_size, output_size)\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "# TODO: Adjust architecture based on methodology\n",
    "input_size = X_train.shape[1]\n",
    "hidden_size = 128\n",
    "output_size = len(np.unique(y))\n",
    "\n",
    "model = ResearchModel(input_size, hidden_size, output_size)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training loop\n",
    "# TODO: Adjust hyperparameters based on methodology\n",
    "num_epochs = 100\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    outputs = model(torch.FloatTensor(X_train.values))\n",
    "    loss = criterion(outputs, torch.LongTensor(y_train.values))\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')''',\n",
    "\n",
    "            'data_visualization': '''# Data Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "# Set plotting style\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# Create visualizations based on methodology\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "\n",
    "# TODO: Customize plots based on specific methodology\n",
    "# Example plots:\n",
    "# Distribution plot\n",
    "axes[0, 0].hist(data['feature1'], bins=30, alpha=0.7)\n",
    "axes[0, 0].set_title('Feature Distribution')\n",
    "axes[0, 0].set_xlabel('Feature 1')\n",
    "axes[0, 0].set_ylabel('Frequency')\n",
    "\n",
    "# Correlation heatmap\n",
    "corr_matrix = data.corr()\n",
    "sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', center=0, ax=axes[0, 1])\n",
    "axes[0, 1].set_title('Feature Correlation Matrix')\n",
    "\n",
    "# Scatter plot\n",
    "axes[1, 0].scatter(data['feature1'], data['feature2'], alpha=0.6)\n",
    "axes[1, 0].set_title('Feature Relationship')\n",
    "axes[1, 0].set_xlabel('Feature 1')\n",
    "axes[1, 0].set_ylabel('Feature 2')\n",
    "\n",
    "# Box plot\n",
    "data.boxplot(column='feature1', by='target', ax=axes[1, 1])\n",
    "axes[1, 1].set_title('Feature by Target')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()''',\n",
    "\n",
    "            'evaluation_metrics': '''# Model Evaluation and Metrics\n",
    "from sklearn.metrics import (accuracy_score, precision_score, recall_score, \n",
    "                           f1_score, roc_auc_score, mean_squared_error, \n",
    "                           mean_absolute_error, r2_score)\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Calculate comprehensive evaluation metrics\n",
    "def evaluate_model(y_true, y_pred, model_type='classification'):\n",
    "    \"\"\"\n",
    "    Comprehensive model evaluation function\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "    \n",
    "    if model_type == 'classification':\n",
    "        results['accuracy'] = accuracy_score(y_true, y_pred)\n",
    "        results['precision'] = precision_score(y_true, y_pred, average='weighted')\n",
    "        results['recall'] = recall_score(y_true, y_pred, average='weighted')\n",
    "        results['f1_score'] = f1_score(y_true, y_pred, average='weighted')\n",
    "        \n",
    "        # ROC AUC for binary classification\n",
    "        if len(np.unique(y_true)) == 2:\n",
    "            results['roc_auc'] = roc_auc_score(y_true, y_pred)\n",
    "            \n",
    "    elif model_type == 'regression':\n",
    "        results['mse'] = mean_squared_error(y_true, y_pred)\n",
    "        results['mae'] = mean_absolute_error(y_true, y_pred)\n",
    "        results['r2'] = r2_score(y_true, y_pred)\n",
    "        results['rmse'] = np.sqrt(results['mse'])\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Evaluate the model\n",
    "# TODO: Choose appropriate evaluation type\n",
    "evaluation_results = evaluate_model(y_test, y_pred, model_type='classification')\n",
    "\n",
    "print(\"Model Evaluation Results:\")\n",
    "for metric, value in evaluation_results.items():\n",
    "    print(f\"{metric.upper()}: {value:.4f}\")'''\n",
    "        }\n",
    "    \n",
    "    async def generate_notebook_from_paper(self, paper_text: str, \n",
    "                                         paper_metadata: Dict[str, Any]) -> nbformat.NotebookNode:\n",
    "        \"\"\"Generate a complete Jupyter notebook from a research paper\"\"\"\n",
    "        \n",
    "        # Extract methodology and key components\n",
    "        extracted_info = await self._extract_methodology(paper_text)\n",
    "        \n",
    "        # Create new notebook\n",
    "        notebook = new_notebook()\n",
    "        \n",
    "        # Add title and overview\n",
    "        notebook.cells.append(self._create_title_cell(paper_metadata))\n",
    "        notebook.cells.append(self._create_overview_cell(extracted_info))\n",
    "        \n",
    "        # Add setup and imports\n",
    "        notebook.cells.append(self._create_setup_cell(extracted_info))\n",
    "        \n",
    "        # Add methodology sections\n",
    "        for section in extracted_info['sections']:\n",
    "            notebook.cells.append(new_markdown_cell(f\"## {section['title']}\"))\n",
    "            notebook.cells.append(new_markdown_cell(section['description']))\n",
    "            \n",
    "            if section['code']:\n",
    "                notebook.cells.append(new_code_cell(section['code']))\n",
    "        \n",
    "        # Add results and evaluation section\n",
    "        notebook.cells.append(self._create_evaluation_cell(extracted_info))\n",
    "        \n",
    "        # Add conclusion and citations\n",
    "        notebook.cells.append(self._create_conclusion_cell(paper_metadata))\n",
    "        \n",
    "        return notebook\n",
    "    \n",
    "    async def _extract_methodology(self, paper_text: str) -> Dict[str, Any]:\n",
    "        \"\"\"Extract methodology and implementation details from paper text\"\"\"\n",
    "        \n",
    "        if not self.openai_client:\n",
    "            return self._extract_methodology_basic(paper_text)\n",
    "        \n",
    "        prompt = f\"\"\"\n",
    "        Analyze the following research paper and extract the methodology for implementation:\n",
    "        \n",
    "        Paper Text:\n",
    "        {paper_text[:4000]}  # Limit text for API\n",
    "        \n",
    "        Please provide a structured extraction including:\n",
    "        1. Main research method/approach\n",
    "        2. Data requirements\n",
    "        3. Statistical or ML techniques used\n",
    "        4. Evaluation metrics\n",
    "        5. Implementation steps\n",
    "        6. Key algorithms or formulas\n",
    "        \n",
    "        Format the response as JSON with sections for each component.\n",
    "        \"\"\"\n",
    "        \n",
    "        response = self.openai_client.chat.completions.create(\n",
    "            model=\"gpt-4\",\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "            temperature=0.1\n",
    "        )\n",
    "        \n",
    "        try:\n",
    "            extracted_info = json.loads(response.choices[0].message.content)\n",
    "        except:\n",
    "            # Fallback to basic extraction\n",
    "            extracted_info = self._extract_methodology_basic(paper_text)\n",
    "        \n",
    "        return self._structure_methodology_info(extracted_info)\n",
    "    \n",
    "    def _extract_methodology_basic(self, paper_text: str) -> Dict[str, Any]:\n",
    "        \"\"\"Basic methodology extraction using pattern matching\"\"\"\n",
    "        \n",
    "        sections = []\n",
    "        \n",
    "        # Look for common methodology keywords\n",
    "        ml_keywords = ['machine learning', 'neural network', 'deep learning', 'random forest', 'svm']\n",
    "        stats_keywords = ['statistical analysis', 't-test', 'anova', 'regression', 'correlation']\n",
    "        viz_keywords = ['visualization', 'plot', 'graph', 'chart', 'figure']\n",
    "        \n",
    "        text_lower = paper_text.lower()\n",
    "        \n",
    "        # Determine main approach\n",
    "        if any(keyword in text_lower for keyword in ml_keywords):\n",
    "            sections.append({\n",
    "                'title': 'Machine Learning Implementation',\n",
    "                'description': 'Implementation of machine learning methodology from the paper.',\n",
    "                'code': self.code_templates['machine_learning']\n",
    "            })\n",
    "        \n",
    "        if any(keyword in text_lower for keyword in stats_keywords):\n",
    "            sections.append({\n",
    "                'title': 'Statistical Analysis',\n",
    "                'description': 'Statistical analysis implementation based on paper methodology.',\n",
    "                'code': self.code_templates['statistical_analysis']\n",
    "            })\n",
    "        \n",
    "        # Always include data loading and visualization\n",
    "        sections.insert(0, {\n",
    "            'title': 'Data Loading and Preprocessing',\n",
    "            'description': 'Load and preprocess data according to paper specifications.',\n",
    "            'code': self.code_templates['data_loading']\n",
    "        })\n",
    "        \n",
    "        sections.append({\n",
    "            'title': 'Data Visualization',\n",
    "            'description': 'Visualize data and results as described in the paper.',\n",
    "            'code': self.code_templates['data_visualization']\n",
    "        })\n",
    "        \n",
    "        sections.append({\n",
    "            'title': 'Model Evaluation',\n",
    "            'description': 'Evaluate results using metrics from the paper.',\n",
    "            'code': self.code_templates['evaluation_metrics']\n",
    "        })\n",
    "        \n",
    "        return {\n",
    "            'main_method': 'Mixed Methods',\n",
    "            'sections': sections,\n",
    "            'data_requirements': ['CSV dataset', 'Numerical features', 'Target variable'],\n",
    "            'libraries': ['pandas', 'numpy', 'scikit-learn', 'matplotlib', 'seaborn']\n",
    "        }\n",
    "    \n",
    "    def _structure_methodology_info(self, extracted_info: Dict) -> Dict[str, Any]:\n",
    "        \"\"\"Structure the extracted methodology information\"\"\"\n",
    "        \n",
    "        # Ensure consistent structure\n",
    "        structured_info = {\n",
    "            'main_method': extracted_info.get('main_method', 'Research Method'),\n",
    "            'sections': extracted_info.get('sections', []),\n",
    "            'data_requirements': extracted_info.get('data_requirements', []),\n",
    "            'libraries': extracted_info.get('libraries', ['pandas', 'numpy', 'matplotlib'])\n",
    "        }\n",
    "        \n",
    "        return structured_info\n",
    "    \n",
    "    def _create_title_cell(self, paper_metadata: Dict[str, Any]) -> nbformat.NotebookNode:\n",
    "        \"\"\"Create title cell with paper information\"\"\"\n",
    "        \n",
    "        title = paper_metadata.get('title', 'Research Paper Implementation')\n",
    "        authors = paper_metadata.get('authors', ['Unknown'])\n",
    "        doi = paper_metadata.get('doi', 'N/A')\n",
    "        \n",
    "        title_text = f\"\"\"# {title}\n",
    "\n",
    "**Reproducible Implementation**\n",
    "\n",
    "**Original Authors:** {', '.join(authors)}\n",
    "**DOI:** {doi}\n",
    "**Implementation Date:** {datetime.now().strftime('%Y-%m-%d')}\n",
    "\n",
    "---\n",
    "\n",
    "This notebook provides a reproducible implementation of the methodology described in the above paper. \n",
    "The code is generated automatically and should be reviewed and customized based on your specific dataset and requirements.\n",
    "\n",
    "**‚ö†Ô∏è Important Notes:**\n",
    "- Replace placeholder datasets with actual data\n",
    "- Adjust hyperparameters based on your specific use case  \n",
    "- Verify statistical assumptions before applying methods\n",
    "- Citation of the original paper is required when using this implementation\n",
    "\n",
    "---\"\"\"\n",
    "        \n",
    "        return new_markdown_cell(title_text)\n",
    "    \n",
    "    def _create_overview_cell(self, extracted_info: Dict[str, Any]) -> nbformat.NotebookNode:\n",
    "        \"\"\"Create overview cell with methodology summary\"\"\"\n",
    "        \n",
    "        overview_text = f\"\"\"## üìã Implementation Overview\n",
    "\n",
    "**Main Research Method:** {extracted_info['main_method']}\n",
    "\n",
    "**Data Requirements:**\n",
    "{chr(10).join([f\"- {req}\" for req in extracted_info['data_requirements']])}\n",
    "\n",
    "**Required Libraries:**\n",
    "{chr(10).join([f\"- {lib}\" for lib in extracted_info['libraries']])}\n",
    "\n",
    "**Implementation Sections:**\n",
    "{chr(10).join([f\"{i+1}. {section['title']}\" for i, section in enumerate(extracted_info['sections'])])}\n",
    "\n",
    "---\"\"\"\n",
    "        \n",
    "        return new_markdown_cell(overview_text)\n",
    "    \n",
    "    def _create_setup_cell(self, extracted_info: Dict[str, Any]) -> nbformat.NotebookNode:\n",
    "        \"\"\"Create setup cell with imports and configuration\"\"\"\n",
    "        \n",
    "        libraries = extracted_info['libraries']\n",
    "        \n",
    "        setup_code = f\"\"\"# Setup and Imports\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Core libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Configure plotting\n",
    "plt.rcParams['figure.figsize'] = (10, 6)\n",
    "plt.rcParams['font.size'] = 12\n",
    "sns.set_style(\"whitegrid\")\n",
    "\n",
    "# Additional libraries based on methodology\n",
    "{chr(10).join([f\"# import {lib}\" for lib in libraries if lib not in ['pandas', 'numpy', 'matplotlib', 'seaborn']])}\n",
    "\n",
    "print(\"‚úÖ Setup complete! All libraries imported successfully.\")\n",
    "print(\"üìä Ready to implement the research methodology.\")\"\"\"\n",
    "        \n",
    "        return new_code_cell(setup_code)\n",
    "    \n",
    "    def _create_evaluation_cell(self, extracted_info: Dict[str, Any]) -> nbformat.NotebookNode:\n",
    "        \"\"\"Create evaluation cell with results analysis\"\"\"\n",
    "        \n",
    "        evaluation_code = \"\"\"# Results Analysis and Evaluation\n",
    "\n",
    "# TODO: Implement specific evaluation metrics from the paper\n",
    "\n",
    "# Create results summary\n",
    "results_summary = {\n",
    "    'implementation_date': pd.Timestamp.now(),\n",
    "    'methodology': 'As described in original paper',\n",
    "    'reproducibility_notes': 'Implementation follows paper methodology with noted adaptations'\n",
    "}\n",
    "\n",
    "print(\"üìä Implementation Results Summary:\")\n",
    "for key, value in results_summary.items():\n",
    "    print(f\"{key}: {value}\")\n",
    "\n",
    "# TODO: Add specific result comparisons with original paper\n",
    "print(\"\\\\nüî¨ Next Steps:\")\n",
    "print(\"1. Compare results with original paper\")\n",
    "print(\"2. Perform sensitivity analysis\") \n",
    "print(\"3. Validate on additional datasets\")\n",
    "print(\"4. Document any methodological adaptations\")\"\"\"\n",
    "        \n",
    "        return new_code_cell(evaluation_code)\n",
    "    \n",
    "    def _create_conclusion_cell(self, paper_metadata: Dict[str, Any]) -> nbformat.NotebookNode:\n",
    "        \"\"\"Create conclusion cell with citation\"\"\"\n",
    "        \n",
    "        title = paper_metadata.get('title', 'Research Paper')\n",
    "        authors = paper_metadata.get('authors', ['Unknown'])\n",
    "        doi = paper_metadata.get('doi', 'N/A')\n",
    "        \n",
    "        conclusion_text = f\"\"\"## üìù Conclusion and Citation\n",
    "\n",
    "This notebook implements the methodology described in the original research paper. Please ensure to:\n",
    "\n",
    "1. **Cite the original paper** in any work using this implementation\n",
    "2. **Validate results** against the original findings  \n",
    "3. **Document modifications** made to the original methodology\n",
    "4. **Share improvements** with the research community\n",
    "\n",
    "### Citation\n",
    "\n",
    "```\n",
    "{', '.join(authors)} ({paper_metadata.get('year', 'N/A')}). {title}. \n",
    "DOI: {doi}\n",
    "```\n",
    "\n",
    "### Implementation Notes\n",
    "\n",
    "- This implementation was auto-generated and may require customization\n",
    "- Please verify all statistical assumptions and model choices\n",
    "- Consider the specific context of your data and research question\n",
    "- Reproducibility is enhanced through proper documentation and version control\n",
    "\n",
    "---\n",
    "\n",
    "**üéØ Happy Researching!** \n",
    "\n",
    "For questions or improvements to this implementation, please contribute to the research community by sharing your insights.\"\"\"\n",
    "        \n",
    "        return new_markdown_cell(conclusion_text)\n",
    "    \n",
    "    async def generate_methodology_specific_notebook(self, methodology_type: str,\n",
    "                                                   parameters: Dict[str, Any]) -> nbformat.NotebookNode:\n",
    "        \"\"\"Generate notebook for specific research methodology\"\"\"\n",
    "        \n",
    "        notebook = new_notebook()\n",
    "        \n",
    "        if methodology_type == 'systematic_review':\n",
    "            notebook = self._create_systematic_review_notebook(parameters)\n",
    "        elif methodology_type == 'meta_analysis':\n",
    "            notebook = self._create_meta_analysis_notebook(parameters)\n",
    "        elif methodology_type == 'experimental_design':\n",
    "            notebook = self._create_experimental_design_notebook(parameters)\n",
    "        elif methodology_type == 'survey_analysis':\n",
    "            notebook = self._create_survey_analysis_notebook(parameters)\n",
    "        else:\n",
    "            # Generic research notebook\n",
    "            notebook = self._create_generic_research_notebook(parameters)\n",
    "        \n",
    "        return notebook\n",
    "    \n",
    "    def _create_systematic_review_notebook(self, parameters: Dict) -> nbformat.NotebookNode:\n",
    "        \"\"\"Create notebook for systematic review methodology\"\"\"\n",
    "        \n",
    "        notebook = new_notebook()\n",
    "        \n",
    "        # Title\n",
    "        notebook.cells.append(new_markdown_cell(\"# Systematic Review Implementation\"))\n",
    "        \n",
    "        # PRISMA flow diagram code\n",
    "        prisma_code = '''# PRISMA Flow Diagram Implementation\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "\n",
    "def create_prisma_diagram(identification, screening, eligibility, included):\n",
    "    \"\"\"Create PRISMA flow diagram\"\"\"\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(12, 16))\n",
    "    \n",
    "    # TODO: Implement PRISMA diagram based on your review process\n",
    "    boxes = [\n",
    "        {'text': f'Records identified\\\\n(n = {identification})', 'xy': (0.5, 0.9)},\n",
    "        {'text': f'Records screened\\\\n(n = {screening})', 'xy': (0.5, 0.7)},\n",
    "        {'text': f'Full-text articles assessed\\\\n(n = {eligibility})', 'xy': (0.5, 0.5)},\n",
    "        {'text': f'Studies included\\\\n(n = {included})', 'xy': (0.5, 0.3)}\n",
    "    ]\n",
    "    \n",
    "    for box in boxes:\n",
    "        rect = patches.Rectangle((box['xy'][0]-0.1, box['xy'][1]-0.05), 0.2, 0.1, \n",
    "                               linewidth=1, edgecolor='black', facecolor='lightblue')\n",
    "        ax.add_patch(rect)\n",
    "        ax.text(box['xy'][0], box['xy'][1], box['text'], ha='center', va='center')\n",
    "    \n",
    "    ax.set_xlim(0, 1)\n",
    "    ax.set_ylim(0, 1)\n",
    "    ax.axis('off')\n",
    "    plt.title('PRISMA Flow Diagram')\n",
    "    plt.show()\n",
    "\n",
    "# TODO: Update with your actual numbers\n",
    "create_prisma_diagram(identification=1000, screening=500, eligibility=100, included=25)'''\n",
    "        \n",
    "        notebook.cells.append(new_code_cell(prisma_code))\n",
    "        \n",
    "        return notebook\n",
    "    \n",
    "    def save_notebook(self, notebook: nbformat.NotebookNode, filename: str):\n",
    "        \"\"\"Save notebook to file\"\"\"\n",
    "        with open(filename, 'w') as f:\n",
    "            nbformat.write(notebook, f)\n",
    "    \n",
    "    async def create_paper_reproduction_package(self, paper_metadata: Dict[str, Any],\n",
    "                                              paper_text: str) -> Dict[str, Any]:\n",
    "        \"\"\"Create complete reproduction package for a paper\"\"\"\n",
    "        \n",
    "        # Generate main notebook\n",
    "        main_notebook = await self.generate_notebook_from_paper(paper_text, paper_metadata)\n",
    "        \n",
    "        # Generate supplementary materials\n",
    "        package = {\n",
    "            'main_notebook': main_notebook,\n",
    "            'requirements_txt': self._generate_requirements_file(paper_text),\n",
    "            'readme_md': self._generate_readme(paper_metadata),\n",
    "            'data_description': self._generate_data_description(paper_text),\n",
    "            'methodology_notes': self._generate_methodology_notes(paper_text)\n",
    "        }\n",
    "        \n",
    "        return package\n",
    "    \n",
    "    def _generate_requirements_file(self, paper_text: str) -> str:\n",
    "        \"\"\"Generate requirements.txt based on identified libraries\"\"\"\n",
    "        \n",
    "        # Common research libraries with versions\n",
    "        requirements = [\n",
    "            \"pandas>=1.3.0\",\n",
    "            \"numpy>=1.21.0\", \n",
    "            \"matplotlib>=3.4.0\",\n",
    "            \"seaborn>=0.11.0\",\n",
    "            \"scikit-learn>=1.0.0\",\n",
    "            \"scipy>=1.7.0\",\n",
    "            \"jupyter>=1.0.0\",\n",
    "            \"notebook>=6.4.0\"\n",
    "        ]\n",
    "        \n",
    "        # Add conditional requirements based on paper content\n",
    "        text_lower = paper_text.lower()\n",
    "        \n",
    "        if 'deep learning' in text_lower or 'neural network' in text_lower:\n",
    "            requirements.extend([\"torch>=1.9.0\", \"tensorflow>=2.6.0\"])\n",
    "        \n",
    "        if 'nlp' in text_lower or 'natural language' in text_lower:\n",
    "            requirements.extend([\"nltk>=3.6\", \"spacy>=3.2.0\", \"transformers>=4.0.0\"])\n",
    "        \n",
    "        if 'plotly' in text_lower or 'interactive' in text_lower:\n",
    "            requirements.extend([\"plotly>=5.0.0\", \"dash>=2.0.0\"])\n",
    "        \n",
    "        return '\\n'.join(requirements)\n",
    "    \n",
    "    def _generate_readme(self, paper_metadata: Dict[str, Any]) -> str:\n",
    "        \"\"\"Generate README.md for reproduction package\"\"\"\n",
    "        \n",
    "        title = paper_metadata.get('title', 'Research Paper')\n",
    "        authors = paper_metadata.get('authors', ['Unknown'])\n",
    "        \n",
    "        readme_content = f\"\"\"# {title} - Reproducible Implementation\n",
    "\n",
    "## Original Paper Information\n",
    "- **Authors:** {', '.join(authors)}\n",
    "- **DOI:** {paper_metadata.get('doi', 'N/A')}\n",
    "- **Publication Year:** {paper_metadata.get('year', 'N/A')}\n",
    "\n",
    "## Implementation Overview\n",
    "This repository contains a reproducible implementation of the methodology described in the above paper.\n",
    "\n",
    "## Files Structure\n",
    "- `main_notebook.ipynb` - Main implementation notebook\n",
    "- `requirements.txt` - Python dependencies\n",
    "- `data/` - Data files (to be added)\n",
    "- `results/` - Output results and figures\n",
    "\n",
    "## Getting Started\n",
    "\n",
    "1. **Install Dependencies**\n",
    "   ```bash\n",
    "   pip install -r requirements.txt\n",
    "   ```\n",
    "\n",
    "2. **Prepare Data**\n",
    "   - Add your dataset to the `data/` folder\n",
    "   - Update data loading paths in the notebook\n",
    "\n",
    "3. **Run Implementation**\n",
    "   - Open `main_notebook.ipynb` in Jupyter\n",
    "   - Follow the step-by-step implementation\n",
    "   - Customize parameters as needed\n",
    "\n",
    "## Citation\n",
    "If you use this implementation, please cite both the original paper and this reproduction:\n",
    "\n",
    "```\n",
    "Original Paper:\n",
    "{', '.join(authors)} ({paper_metadata.get('year', 'N/A')}). {title}.\n",
    "\n",
    "Reproduction Package:\n",
    "Auto-generated reproducible implementation, {datetime.now().year}.\n",
    "```\n",
    "\n",
    "## Contributing\n",
    "Please contribute improvements and report issues to enhance reproducibility.\n",
    "\n",
    "## License\n",
    "This implementation follows the same license as the original paper's methodology.\n",
    "\"\"\"\n",
    "        \n",
    "        return readme_content\n",
    "    \n",
    "    def _generate_data_description(self, paper_text: str) -> str:\n",
    "        \"\"\"Generate data description based on paper content\"\"\"\n",
    "        \n",
    "        return \"\"\"# Data Description\n",
    "\n",
    "## Expected Data Format\n",
    "- Format: CSV, Excel, or similar tabular format\n",
    "- Features: [To be specified based on paper]\n",
    "- Target Variable: [To be specified based on paper]\n",
    "- Sample Size: [As reported in original paper]\n",
    "\n",
    "## Data Preprocessing Steps\n",
    "1. Data cleaning and missing value handling\n",
    "2. Feature scaling/normalization  \n",
    "3. Train/test split\n",
    "4. [Additional steps as per methodology]\n",
    "\n",
    "## Data Requirements\n",
    "Please ensure your data meets the following requirements:\n",
    "- [Requirement 1]\n",
    "- [Requirement 2]\n",
    "- [Additional requirements based on paper]\n",
    "\n",
    "‚ö†Ô∏è **Note:** Update this description based on the specific data used in the original paper.\n",
    "\"\"\"\n",
    "    \n",
    "    def _generate_methodology_notes(self, paper_text: str) -> str:\n",
    "        \"\"\"Generate methodology implementation notes\"\"\"\n",
    "        \n",
    "        return \"\"\"# Methodology Implementation Notes\n",
    "\n",
    "## Implementation Decisions\n",
    "- [Decision 1 and rationale]\n",
    "- [Decision 2 and rationale]  \n",
    "- [Additional implementation choices]\n",
    "\n",
    "## Deviations from Original Paper\n",
    "- [Any necessary adaptations]\n",
    "- [Differences in implementation]\n",
    "- [Reasons for changes]\n",
    "\n",
    "## Hyperparameters\n",
    "- [Parameter 1]: [Value and justification]\n",
    "- [Parameter 2]: [Value and justification]\n",
    "- [Additional parameters]\n",
    "\n",
    "## Validation Strategy\n",
    "- [Cross-validation approach]\n",
    "- [Evaluation metrics]\n",
    "- [Statistical significance testing]\n",
    "\n",
    "## Reproducibility Considerations\n",
    "- Random seed management\n",
    "- Environment specifications\n",
    "- Version control recommendations\n",
    "\n",
    "## Future Improvements\n",
    "- [Suggested enhancements]\n",
    "- [Additional experiments]\n",
    "- [Alternative approaches]\n",
    "\"\"\"\n",
    "\n",
    "print(\"üìì Reproducible Notebook Generator implemented!\")\n",
    "print(\"‚úÖ Features: Auto-generate Jupyter notebooks from paper methodologies\")\n",
    "print(\"üî¨ Capabilities: Multiple research methods, complete reproduction packages, citations\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b21f502",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plagiarism and Academic Integrity Checker\n",
    "import difflib\n",
    "import re\n",
    "from textblob import TextBlob\n",
    "import hashlib\n",
    "from collections import Counter\n",
    "import openai\n",
    "from typing import List, Dict, Tuple, Set\n",
    "\n",
    "class AcademicIntegrityChecker:\n",
    "    \"\"\"Service for detecting plagiarism and ensuring academic integrity\"\"\"\n",
    "    \n",
    "    def __init__(self, openai_api_key: str = None):\n",
    "        self.openai_client = openai.OpenAI(api_key=openai_api_key) if openai_api_key else None\n",
    "        self.known_papers_db = {}  # In practice, this would be a proper database\n",
    "        self.ai_detection_patterns = self._load_ai_detection_patterns()\n",
    "        \n",
    "    def _load_ai_detection_patterns(self) -> List[Dict[str, Any]]:\n",
    "        \"\"\"Load patterns that may indicate AI-generated content\"\"\"\n",
    "        return [\n",
    "            {\n",
    "                'pattern': r'\\b(as an ai|i am an ai|as a language model)\\b',\n",
    "                'type': 'explicit_ai_mention',\n",
    "                'weight': 1.0\n",
    "            },\n",
    "            {\n",
    "                'pattern': r'\\b(in conclusion|to summarize|furthermore|moreover)\\b',\n",
    "                'type': 'formulaic_transitions',\n",
    "                'weight': 0.3\n",
    "            },\n",
    "            {\n",
    "                'pattern': r'\\b(it is important to note|it should be noted)\\b',\n",
    "                'type': 'hedging_language',\n",
    "                'weight': 0.4\n",
    "            },\n",
    "            {\n",
    "                'pattern': r'\\b(various|numerous|several|multiple)\\b.*\\b(factors|aspects|elements)\\b',\n",
    "                'type': 'vague_language',\n",
    "                'weight': 0.2\n",
    "            }\n",
    "        ]\n",
    "    \n",
    "    async def check_document_integrity(self, text: str, metadata: Dict[str, Any] = None) -> Dict[str, Any]:\n",
    "        \"\"\"Comprehensive integrity check for academic documents\"\"\"\n",
    "        \n",
    "        results = {\n",
    "            'plagiarism_analysis': await self._check_plagiarism(text),\n",
    "            'ai_detection_analysis': await self._detect_ai_content(text),\n",
    "            'citation_analysis': self._analyze_citations(text),\n",
    "            'writing_quality_analysis': self._analyze_writing_quality(text),\n",
    "            'overall_integrity_score': 0.0,\n",
    "            'recommendations': []\n",
    "        }\n",
    "        \n",
    "        # Calculate overall integrity score\n",
    "        plagiarism_score = 1.0 - results['plagiarism_analysis']['similarity_score']\n",
    "        ai_score = 1.0 - results['ai_detection_analysis']['ai_probability']\n",
    "        citation_score = results['citation_analysis']['proper_citation_ratio']\n",
    "        quality_score = results['writing_quality_analysis']['quality_score']\n",
    "        \n",
    "        results['overall_integrity_score'] = (\n",
    "            plagiarism_score * 0.3 + \n",
    "            ai_score * 0.25 + \n",
    "            citation_score * 0.25 + \n",
    "            quality_score * 0.2\n",
    "        )\n",
    "        \n",
    "        # Generate recommendations\n",
    "        results['recommendations'] = self._generate_recommendations(results)\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    async def _check_plagiarism(self, text: str) -> Dict[str, Any]:\n",
    "        \"\"\"Check for potential plagiarism against known sources\"\"\"\n",
    "        \n",
    "        # Text preprocessing\n",
    "        clean_text = self._preprocess_text(text)\n",
    "        sentences = self._split_into_sentences(clean_text)\n",
    "        \n",
    "        plagiarism_results = {\n",
    "            'similarity_score': 0.0,\n",
    "            'flagged_passages': [],\n",
    "            'potential_sources': [],\n",
    "            'sentence_level_analysis': []\n",
    "        }\n",
    "        \n",
    "        # Check against known paper database\n",
    "        for sentence in sentences:\n",
    "            if len(sentence.strip()) < 20:  # Skip very short sentences\n",
    "                continue\n",
    "                \n",
    "            similarity_results = await self._check_sentence_similarity(sentence)\n",
    "            \n",
    "            if similarity_results['max_similarity'] > 0.7:  # High similarity threshold\n",
    "                plagiarism_results['flagged_passages'].append({\n",
    "                    'text': sentence,\n",
    "                    'similarity_score': similarity_results['max_similarity'],\n",
    "                    'potential_source': similarity_results['best_match_source'],\n",
    "                    'match_type': 'exact' if similarity_results['max_similarity'] > 0.9 else 'paraphrase'\n",
    "                })\n",
    "        \n",
    "        # Calculate overall similarity score\n",
    "        if sentences:\n",
    "            total_similarity = sum([\n",
    "                result['similarity_score'] \n",
    "                for result in plagiarism_results['flagged_passages']\n",
    "            ])\n",
    "            plagiarism_results['similarity_score'] = min(1.0, total_similarity / len(sentences))\n",
    "        \n",
    "        # Advanced plagiarism detection using semantic similarity\n",
    "        if self.openai_client:\n",
    "            semantic_analysis = await self._semantic_plagiarism_check(text)\n",
    "            plagiarism_results['semantic_analysis'] = semantic_analysis\n",
    "        \n",
    "        return plagiarism_results\n",
    "    \n",
    "    async def _detect_ai_content(self, text: str) -> Dict[str, Any]:\n",
    "        \"\"\"Detect potentially AI-generated content\"\"\"\n",
    "        \n",
    "        ai_detection_results = {\n",
    "            'ai_probability': 0.0,\n",
    "            'detected_patterns': [],\n",
    "            'linguistic_analysis': {},\n",
    "            'confidence_level': 'low'\n",
    "        }\n",
    "        \n",
    "        # Pattern-based detection\n",
    "        text_lower = text.lower()\n",
    "        pattern_matches = []\n",
    "        \n",
    "        for pattern_info in self.ai_detection_patterns:\n",
    "            matches = re.findall(pattern_info['pattern'], text_lower)\n",
    "            if matches:\n",
    "                pattern_matches.append({\n",
    "                    'pattern_type': pattern_info['type'],\n",
    "                    'matches': matches,\n",
    "                    'weight': pattern_info['weight'],\n",
    "                    'count': len(matches)\n",
    "                })\n",
    "        \n",
    "        # Calculate pattern-based AI probability\n",
    "        pattern_score = sum([\n",
    "            match['weight'] * min(match['count'], 3)  # Cap at 3 occurrences\n",
    "            for match in pattern_matches\n",
    "        ]) / 10.0  # Normalize\n",
    "        \n",
    "        ai_detection_results['detected_patterns'] = pattern_matches\n",
    "        \n",
    "        # Linguistic analysis\n",
    "        linguistic_features = self._analyze_linguistic_features(text)\n",
    "        ai_detection_results['linguistic_analysis'] = linguistic_features\n",
    "        \n",
    "        # Combine scores\n",
    "        linguistic_ai_score = self._calculate_linguistic_ai_score(linguistic_features)\n",
    "        \n",
    "        # Advanced AI detection using LLM\n",
    "        if self.openai_client:\n",
    "            llm_analysis = await self._llm_based_ai_detection(text)\n",
    "            ai_detection_results['llm_analysis'] = llm_analysis\n",
    "            final_ai_score = (pattern_score + linguistic_ai_score + llm_analysis['ai_score']) / 3.0\n",
    "        else:\n",
    "            final_ai_score = (pattern_score + linguistic_ai_score) / 2.0\n",
    "        \n",
    "        ai_detection_results['ai_probability'] = min(1.0, final_ai_score)\n",
    "        \n",
    "        # Determine confidence level\n",
    "        if ai_detection_results['ai_probability'] > 0.8:\n",
    "            ai_detection_results['confidence_level'] = 'high'\n",
    "        elif ai_detection_results['ai_probability'] > 0.5:\n",
    "            ai_detection_results['confidence_level'] = 'medium'\n",
    "        else:\n",
    "            ai_detection_results['confidence_level'] = 'low'\n",
    "        \n",
    "        return ai_detection_results\n",
    "    \n",
    "    def _analyze_citations(self, text: str) -> Dict[str, Any]:\n",
    "        \"\"\"Analyze citation patterns and proper attribution\"\"\"\n",
    "        \n",
    "        # Find citations using various patterns\n",
    "        citation_patterns = [\n",
    "            r'\\([A-Za-z]+\\s+et\\s+al\\.?,?\\s+\\d{4}\\)',  # (Author et al., 2023)\n",
    "            r'\\([A-Za-z]+,?\\s+\\d{4}\\)',  # (Author, 2023)\n",
    "            r'\\[[0-9,\\s-]+\\]',  # [1, 2, 3-5]\n",
    "            r'\\([0-9,\\s-]+\\)',  # (1, 2, 3-5)\n",
    "        ]\n",
    "        \n",
    "        citations_found = []\n",
    "        for pattern in citation_patterns:\n",
    "            matches = re.findall(pattern, text)\n",
    "            citations_found.extend(matches)\n",
    "        \n",
    "        # Find potential uncited claims\n",
    "        claim_patterns = [\n",
    "            r'research shows',\n",
    "            r'studies indicate',\n",
    "            r'it has been found',\n",
    "            r'evidence suggests',\n",
    "            r'according to',\n",
    "            r'previous work'\n",
    "        ]\n",
    "        \n",
    "        uncited_claims = []\n",
    "        sentences = self._split_into_sentences(text)\n",
    "        \n",
    "        for sentence in sentences:\n",
    "            sentence_lower = sentence.lower()\n",
    "            for claim_pattern in claim_patterns:\n",
    "                if claim_pattern in sentence_lower:\n",
    "                    # Check if sentence has a citation\n",
    "                    has_citation = any(\n",
    "                        re.search(pattern, sentence) \n",
    "                        for pattern in citation_patterns\n",
    "                    )\n",
    "                    \n",
    "                    if not has_citation:\n",
    "                        uncited_claims.append({\n",
    "                            'sentence': sentence,\n",
    "                            'claim_type': claim_pattern\n",
    "                        })\n",
    "        \n",
    "        citation_analysis = {\n",
    "            'total_citations': len(citations_found),\n",
    "            'citation_density': len(citations_found) / len(sentences) if sentences else 0,\n",
    "            'uncited_claims': uncited_claims,\n",
    "            'proper_citation_ratio': 1.0 - (len(uncited_claims) / len(sentences)) if sentences else 1.0,\n",
    "            'citation_patterns_found': list(set(citations_found))\n",
    "        }\n",
    "        \n",
    "        return citation_analysis\n",
    "    \n",
    "    def _analyze_writing_quality(self, text: str) -> Dict[str, Any]:\n",
    "        \"\"\"Analyze writing quality indicators\"\"\"\n",
    "        \n",
    "        blob = TextBlob(text)\n",
    "        sentences = blob.sentences\n",
    "        words = blob.words\n",
    "        \n",
    "        # Calculate various quality metrics\n",
    "        avg_sentence_length = sum(len(s.words) for s in sentences) / len(sentences) if sentences else 0\n",
    "        \n",
    "        # Vocabulary diversity (Type-Token Ratio)\n",
    "        unique_words = set(word.lower() for word in words if word.isalpha())\n",
    "        vocabulary_diversity = len(unique_words) / len(words) if words else 0\n",
    "        \n",
    "        # Readability approximation (simple version)\n",
    "        avg_word_length = sum(len(word) for word in words) / len(words) if words else 0\n",
    "        readability_score = 206.835 - (1.015 * avg_sentence_length) - (84.6 * avg_word_length)\n",
    "        readability_score = max(0, min(100, readability_score))  # Clamp to 0-100\n",
    "        \n",
    "        # Academic language indicators\n",
    "        academic_words = {\n",
    "            'analyze', 'evaluate', 'demonstrate', 'investigate', 'examine',\n",
    "            'significant', 'substantial', 'comprehensive', 'methodology',\n",
    "            'hypothesis', 'conclusion', 'furthermore', 'however', 'therefore'\n",
    "        }\n",
    "        \n",
    "        academic_word_count = sum(1 for word in words if word.lower() in academic_words)\n",
    "        academic_language_ratio = academic_word_count / len(words) if words else 0\n",
    "        \n",
    "        # Calculate overall quality score\n",
    "        quality_score = (\n",
    "            min(1.0, vocabulary_diversity * 2) * 0.3 +  # Vocabulary diversity\n",
    "            min(1.0, readability_score / 100) * 0.3 +   # Readability\n",
    "            min(1.0, academic_language_ratio * 10) * 0.4 # Academic language\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'avg_sentence_length': avg_sentence_length,\n",
    "            'vocabulary_diversity': vocabulary_diversity,\n",
    "            'readability_score': readability_score,\n",
    "            'academic_language_ratio': academic_language_ratio,\n",
    "            'quality_score': quality_score,\n",
    "            'total_words': len(words),\n",
    "            'total_sentences': len(sentences)\n",
    "        }\n",
    "    \n",
    "    async def _check_sentence_similarity(self, sentence: str) -> Dict[str, Any]:\n",
    "        \"\"\"Check similarity of a sentence against known sources\"\"\"\n",
    "        \n",
    "        # In practice, this would query a database of academic papers\n",
    "        # For now, we'll simulate with a basic check\n",
    "        \n",
    "        max_similarity = 0.0\n",
    "        best_match_source = None\n",
    "        \n",
    "        # Simulate database check (in practice, use vector similarity search)\n",
    "        for paper_id, paper_content in self.known_papers_db.items():\n",
    "            for known_sentence in self._split_into_sentences(paper_content):\n",
    "                similarity = difflib.SequenceMatcher(None, sentence.lower(), known_sentence.lower()).ratio()\n",
    "                \n",
    "                if similarity > max_similarity:\n",
    "                    max_similarity = similarity\n",
    "                    best_match_source = paper_id\n",
    "        \n",
    "        return {\n",
    "            'max_similarity': max_similarity,\n",
    "            'best_match_source': best_match_source\n",
    "        }\n",
    "    \n",
    "    async def _semantic_plagiarism_check(self, text: str) -> Dict[str, Any]:\n",
    "        \"\"\"Use LLM for semantic plagiarism detection\"\"\"\n",
    "        \n",
    "        if not self.openai_client:\n",
    "            return {'semantic_similarity_detected': False}\n",
    "        \n",
    "        prompt = f\"\"\"\n",
    "        Analyze the following text for potential plagiarism indicators:\n",
    "        \n",
    "        Text:\n",
    "        {text[:2000]}  # Limit for API\n",
    "        \n",
    "        Look for:\n",
    "        1. Unusual phrasing that might indicate paraphrasing\n",
    "        2. Inconsistent writing style\n",
    "        3. Overly complex or unnatural language\n",
    "        4. Potential copying with minor modifications\n",
    "        \n",
    "        Provide a confidence score (0-1) for plagiarism likelihood and explain your reasoning.\n",
    "        \"\"\"\n",
    "        \n",
    "        response = self.openai_client.chat.completions.create(\n",
    "            model=\"gpt-4\",\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "            temperature=0.1\n",
    "        )\n",
    "        \n",
    "        # Parse response for confidence score\n",
    "        response_text = response.choices[0].message.content\n",
    "        \n",
    "        return {\n",
    "            'analysis': response_text,\n",
    "            'semantic_similarity_detected': 'plagiarism' in response_text.lower()\n",
    "        }\n",
    "    \n",
    "    def _analyze_linguistic_features(self, text: str) -> Dict[str, Any]:\n",
    "        \"\"\"Analyze linguistic features that might indicate AI generation\"\"\"\n",
    "        \n",
    "        sentences = self._split_into_sentences(text)\n",
    "        \n",
    "        # Sentence length uniformity\n",
    "        sentence_lengths = [len(s.split()) for s in sentences]\n",
    "        length_variance = np.var(sentence_lengths) if sentence_lengths else 0\n",
    "        \n",
    "        # Repetitive structure detection\n",
    "        sentence_starts = [s.split()[0].lower() if s.split() else '' for s in sentences]\n",
    "        start_diversity = len(set(sentence_starts)) / len(sentence_starts) if sentence_starts else 1\n",
    "        \n",
    "        # Transition word frequency\n",
    "        transitions = ['however', 'furthermore', 'moreover', 'additionally', 'consequently']\n",
    "        transition_count = sum(text.lower().count(t) for t in transitions)\n",
    "        transition_frequency = transition_count / len(sentences) if sentences else 0\n",
    "        \n",
    "        # Hedge word frequency\n",
    "        hedges = ['might', 'could', 'possibly', 'potentially', 'arguably', 'seemingly']\n",
    "        hedge_count = sum(text.lower().count(h) for h in hedges)\n",
    "        hedge_frequency = hedge_count / len(text.split()) if text.split() else 0\n",
    "        \n",
    "        return {\n",
    "            'sentence_length_variance': length_variance,\n",
    "            'sentence_start_diversity': start_diversity,\n",
    "            'transition_frequency': transition_frequency,\n",
    "            'hedge_frequency': hedge_frequency,\n",
    "            'avg_sentence_length': np.mean(sentence_lengths) if sentence_lengths else 0\n",
    "        }\n",
    "    \n",
    "    def _calculate_linguistic_ai_score(self, features: Dict[str, Any]) -> float:\n",
    "        \"\"\"Calculate AI probability based on linguistic features\"\"\"\n",
    "        \n",
    "        ai_indicators = []\n",
    "        \n",
    "        # Low sentence length variance (AI tends to be uniform)\n",
    "        if features['sentence_length_variance'] < 10:\n",
    "            ai_indicators.append(0.3)\n",
    "        \n",
    "        # Low sentence start diversity (repetitive patterns)\n",
    "        if features['sentence_start_diversity'] < 0.7:\n",
    "            ai_indicators.append(0.2)\n",
    "        \n",
    "        # High transition frequency (overuse of connectors)\n",
    "        if features['transition_frequency'] > 0.3:\n",
    "            ai_indicators.append(0.4)\n",
    "        \n",
    "        # High hedge frequency (uncertain language)\n",
    "        if features['hedge_frequency'] > 0.02:\n",
    "            ai_indicators.append(0.3)\n",
    "        \n",
    "        return min(1.0, sum(ai_indicators))\n",
    "    \n",
    "    async def _llm_based_ai_detection(self, text: str) -> Dict[str, Any]:\n",
    "        \"\"\"Use LLM to detect AI-generated content\"\"\"\n",
    "        \n",
    "        prompt = f\"\"\"\n",
    "        Analyze the following text to determine if it was likely generated by AI:\n",
    "        \n",
    "        Text:\n",
    "        {text[:2000]}  # Limit for API\n",
    "        \n",
    "        Consider:\n",
    "        1. Writing style consistency\n",
    "        2. Use of formulaic phrases\n",
    "        3. Unnatural transitions\n",
    "        4. Overly perfect grammar\n",
    "        5. Lack of personal voice or opinion\n",
    "        \n",
    "        Provide an AI probability score (0-1) and reasoning.\n",
    "        \"\"\"\n",
    "        \n",
    "        response = self.openai_client.chat.completions.create(\n",
    "            model=\"gpt-4\",\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "            temperature=0.1\n",
    "        )\n",
    "        \n",
    "        response_text = response.choices[0].message.content\n",
    "        \n",
    "        # Extract probability score (basic parsing)\n",
    "        score_match = re.search(r'(\\d+\\.?\\d*)', response_text)\n",
    "        ai_score = float(score_match.group(1)) if score_match else 0.5\n",
    "        \n",
    "        # Normalize score if needed\n",
    "        if ai_score > 1:\n",
    "            ai_score = ai_score / 100  # Assume percentage\n",
    "        \n",
    "        return {\n",
    "            'ai_score': ai_score,\n",
    "            'analysis': response_text\n",
    "        }\n",
    "    \n",
    "    def _generate_recommendations(self, results: Dict[str, Any]) -> List[str]:\n",
    "        \"\"\"Generate recommendations based on integrity analysis\"\"\"\n",
    "        \n",
    "        recommendations = []\n",
    "        \n",
    "        # Plagiarism recommendations\n",
    "        if results['plagiarism_analysis']['similarity_score'] > 0.3:\n",
    "            recommendations.append(\n",
    "                \"‚ö†Ô∏è High similarity detected. Review flagged passages and ensure proper citation.\"\n",
    "            )\n",
    "        \n",
    "        # AI content recommendations\n",
    "        if results['ai_detection_analysis']['ai_probability'] > 0.7:\n",
    "            recommendations.append(\n",
    "                \"ü§ñ Content may be AI-generated. Review for authenticity and original thought.\"\n",
    "            )\n",
    "        \n",
    "        # Citation recommendations\n",
    "        if results['citation_analysis']['uncited_claims']:\n",
    "            recommendations.append(\n",
    "                f\"üìö {len(results['citation_analysis']['uncited_claims'])} uncited claims found. Add proper citations.\"\n",
    "            )\n",
    "        \n",
    "        # Writing quality recommendations\n",
    "        if results['writing_quality_analysis']['quality_score'] < 0.5:\n",
    "            recommendations.append(\n",
    "                \"‚úçÔ∏è Consider improving writing quality: vocabulary diversity, readability, and academic language.\"\n",
    "            )\n",
    "        \n",
    "        # Overall recommendations\n",
    "        if results['overall_integrity_score'] > 0.8:\n",
    "            recommendations.append(\"‚úÖ Excellent academic integrity! Document meets high standards.\")\n",
    "        elif results['overall_integrity_score'] > 0.6:\n",
    "            recommendations.append(\"üëç Good integrity with minor improvements needed.\")\n",
    "        else:\n",
    "            recommendations.append(\"‚ö†Ô∏è Significant integrity concerns require attention.\")\n",
    "        \n",
    "        return recommendations\n",
    "    \n",
    "    def _preprocess_text(self, text: str) -> str:\n",
    "        \"\"\"Clean and preprocess text for analysis\"\"\"\n",
    "        # Remove extra whitespace\n",
    "        text = re.sub(r'\\s+', ' ', text)\n",
    "        # Remove citations for cleaner analysis\n",
    "        text = re.sub(r'\\[[0-9,\\s-]+\\]', '', text)\n",
    "        text = re.sub(r'\\([A-Za-z]+\\s+et\\s+al\\.?,?\\s+\\d{4}\\)', '', text)\n",
    "        return text.strip()\n",
    "    \n",
    "    def _split_into_sentences(self, text: str) -> List[str]:\n",
    "        \"\"\"Split text into sentences\"\"\"\n",
    "        # Simple sentence splitting (can be improved with NLTK)\n",
    "        sentences = re.split(r'[.!?]+', text)\n",
    "        return [s.strip() for s in sentences if s.strip()]\n",
    "    \n",
    "    def add_known_paper(self, paper_id: str, content: str):\n",
    "        \"\"\"Add a paper to the known papers database for plagiarism checking\"\"\"\n",
    "        self.known_papers_db[paper_id] = content\n",
    "    \n",
    "    async def batch_integrity_check(self, documents: List[Dict[str, Any]]) -> List[Dict[str, Any]]:\n",
    "        \"\"\"Perform integrity checks on multiple documents\"\"\"\n",
    "        \n",
    "        results = []\n",
    "        for doc in documents:\n",
    "            doc_result = await self.check_document_integrity(\n",
    "                doc['text'], \n",
    "                doc.get('metadata', {})\n",
    "            )\n",
    "            doc_result['document_id'] = doc.get('id', 'unknown')\n",
    "            results.append(doc_result)\n",
    "        \n",
    "        return results\n",
    "\n",
    "print(\"üîç Academic Integrity Checker implemented!\")\n",
    "print(\"‚úÖ Features: Plagiarism detection, AI content detection, citation analysis\")\n",
    "print(\"üõ°Ô∏è Capabilities: Comprehensive integrity scoring, detailed recommendations, batch processing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f079deca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enhanced Research Assistant API Integration\n",
    "from fastapi import FastAPI, HTTPException, BackgroundTasks, UploadFile, File\n",
    "from fastapi.middleware.cors import CORSMiddleware\n",
    "from pydantic import BaseModel\n",
    "import asyncio\n",
    "\n",
    "# Enhanced API Models\n",
    "class MultilingualSearchRequest(BaseModel):\n",
    "    query: str\n",
    "    languages: List[str] = ['en', 'es', 'fr', 'de', 'zh', 'ja']\n",
    "    target_language: str = 'en'\n",
    "    max_results: int = 20\n",
    "\n",
    "class CitationNetworkRequest(BaseModel):\n",
    "    paper_ids: List[int]\n",
    "    depth: int = 2\n",
    "    layout_algorithm: str = 'spring'\n",
    "\n",
    "class AlertCreationRequest(BaseModel):\n",
    "    alert_type: str  # 'author_publication', 'citation_threshold', 'keyword_trend', etc.\n",
    "    parameters: Dict[str, Any]\n",
    "    frequency: str = 'weekly'\n",
    "\n",
    "class NotebookGenerationRequest(BaseModel):\n",
    "    paper_text: str\n",
    "    paper_metadata: Dict[str, Any]\n",
    "    methodology_type: str = 'auto_detect'\n",
    "\n",
    "class IntegrityCheckRequest(BaseModel):\n",
    "    text: str\n",
    "    metadata: Optional[Dict[str, Any]] = None\n",
    "    check_types: List[str] = ['plagiarism', 'ai_detection', 'citations', 'quality']\n",
    "\n",
    "# Enhanced Research Assistant with All Features\n",
    "class EnhancedResearchAssistant:\n",
    "    \"\"\"Complete research assistant with all advanced features\"\"\"\n",
    "    \n",
    "    def __init__(self, openai_api_key: str = None):\n",
    "        # Initialize all services\n",
    "        self.multilingual_service = MultilingualResearchService(openai_api_key)\n",
    "        self.citation_network_service = CitationNetworkService()\n",
    "        self.alerts_service = ResearchAlertsService()\n",
    "        self.notebook_generator = ReproducibleNotebookGenerator(openai_api_key)\n",
    "        self.integrity_checker = AcademicIntegrityChecker(openai_api_key)\n",
    "        \n",
    "        # Start alert scheduler\n",
    "        self.alerts_service.start_scheduler()\n",
    "        \n",
    "    async def enhanced_literature_search(self, query: str, \n",
    "                                       languages: List[str] = None,\n",
    "                                       include_network_analysis: bool = False) -> Dict[str, Any]:\n",
    "        \"\"\"Enhanced literature search with multilingual support and network analysis\"\"\"\n",
    "        \n",
    "        # Perform multilingual search\n",
    "        multilingual_results = await self.multilingual_service.search_multilingual_papers(\n",
    "            query, languages or ['en']\n",
    "        )\n",
    "        \n",
    "        results = {\n",
    "            'multilingual_results': multilingual_results,\n",
    "            'search_languages': languages or ['en'],\n",
    "            'total_papers_found': len(multilingual_results)\n",
    "        }\n",
    "        \n",
    "        # Add citation network analysis if requested\n",
    "        if include_network_analysis and multilingual_results:\n",
    "            paper_ids = [result.get('id') for result in multilingual_results if result.get('id')][:10]\n",
    "            if paper_ids:\n",
    "                citation_network = await self.citation_network_service.build_citation_network(paper_ids)\n",
    "                network_analysis = self.citation_network_service.analyze_citation_patterns(citation_network)\n",
    "                \n",
    "                results['citation_network_analysis'] = network_analysis\n",
    "                results['influential_papers'] = network_analysis['influential_papers']\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    async def create_research_workflow(self, research_topic: str, \n",
    "                                     user_preferences: Dict[str, Any]) -> Dict[str, Any]:\n",
    "        \"\"\"Create complete research workflow with all enhanced features\"\"\"\n",
    "        \n",
    "        workflow = {\n",
    "            'research_topic': research_topic,\n",
    "            'created_at': datetime.utcnow(),\n",
    "            'workflow_steps': [],\n",
    "            'alerts_created': [],\n",
    "            'notebooks_generated': [],\n",
    "            'integrity_checks': []\n",
    "        }\n",
    "        \n",
    "        # Step 1: Enhanced literature search\n",
    "        search_results = await self.enhanced_literature_search(\n",
    "            research_topic, \n",
    "            languages=user_preferences.get('languages', ['en']),\n",
    "            include_network_analysis=True\n",
    "        )\n",
    "        \n",
    "        workflow['workflow_steps'].append({\n",
    "            'step': 'literature_search',\n",
    "            'results': search_results,\n",
    "            'status': 'completed'\n",
    "        })\n",
    "        \n",
    "        # Step 2: Set up custom alerts\n",
    "        alert_configs = user_preferences.get('alerts', [])\n",
    "        for alert_config in alert_configs:\n",
    "            alert_id = await self._create_alert_from_config(alert_config, research_topic)\n",
    "            workflow['alerts_created'].append(alert_id)\n",
    "        \n",
    "        # Step 3: Generate reproducible notebooks for key papers\n",
    "        key_papers = search_results.get('influential_papers', [])[:3]  # Top 3 papers\n",
    "        for paper in key_papers:\n",
    "            if paper.get('title') and paper.get('id'):\n",
    "                notebook_request = {\n",
    "                    'paper_metadata': paper,\n",
    "                    'paper_text': f\"Paper: {paper['title']}\"  # In practice, fetch full text\n",
    "                }\n",
    "                notebook = await self.notebook_generator.generate_notebook_from_paper(\n",
    "                    notebook_request['paper_text'],\n",
    "                    notebook_request['paper_metadata']\n",
    "                )\n",
    "                workflow['notebooks_generated'].append({\n",
    "                    'paper_id': paper['id'],\n",
    "                    'notebook': notebook,\n",
    "                    'filename': f\"reproduction_{paper['id']}.ipynb\"\n",
    "                })\n",
    "        \n",
    "        return workflow\n",
    "    \n",
    "    async def _create_alert_from_config(self, alert_config: Dict[str, Any], \n",
    "                                      research_topic: str) -> str:\n",
    "        \"\"\"Create alert from configuration\"\"\"\n",
    "        \n",
    "        user_id = alert_config.get('user_id', 'default_user')\n",
    "        alert_type = alert_config.get('type', 'keyword_trend')\n",
    "        \n",
    "        if alert_type == 'author_publication':\n",
    "            return await self.alerts_service.create_author_alert(\n",
    "                user_id,\n",
    "                alert_config.get('author_name', ''),\n",
    "                alert_config.get('frequency', 'weekly')\n",
    "            )\n",
    "        elif alert_type == 'keyword_trend':\n",
    "            keywords = alert_config.get('keywords', [research_topic])\n",
    "            return await self.alerts_service.create_keyword_trend_alert(\n",
    "                user_id, keywords, alert_config.get('threshold', 0.5)\n",
    "            )\n",
    "        elif alert_type == 'citation_threshold':\n",
    "            return await self.alerts_service.create_citation_threshold_alert(\n",
    "                user_id,\n",
    "                alert_config.get('paper_title', research_topic),\n",
    "                alert_config.get('threshold', 50),\n",
    "                alert_config.get('time_period', '1 year')\n",
    "            )\n",
    "        else:\n",
    "            # Default to keyword trend\n",
    "            return await self.alerts_service.create_keyword_trend_alert(\n",
    "                user_id, [research_topic]\n",
    "            )\n",
    "    \n",
    "    async def comprehensive_document_review(self, document_text: str,\n",
    "                                          document_metadata: Dict[str, Any] = None) -> Dict[str, Any]:\n",
    "        \"\"\"Comprehensive document review with all integrity checks\"\"\"\n",
    "        \n",
    "        # Perform integrity check\n",
    "        integrity_results = await self.integrity_checker.check_document_integrity(\n",
    "            document_text, document_metadata\n",
    "        )\n",
    "        \n",
    "        # Generate improvement recommendations\n",
    "        improvement_plan = self._generate_improvement_plan(integrity_results)\n",
    "        \n",
    "        # Create reproducible methodology if applicable\n",
    "        methodology_notebook = None\n",
    "        if 'methodology' in document_text.lower():\n",
    "            methodology_notebook = await self.notebook_generator.generate_notebook_from_paper(\n",
    "                document_text, document_metadata or {}\n",
    "            )\n",
    "        \n",
    "        return {\n",
    "            'integrity_analysis': integrity_results,\n",
    "            'improvement_plan': improvement_plan,\n",
    "            'methodology_notebook': methodology_notebook,\n",
    "            'review_timestamp': datetime.utcnow(),\n",
    "            'overall_score': integrity_results['overall_integrity_score']\n",
    "        }\n",
    "    \n",
    "    def _generate_improvement_plan(self, integrity_results: Dict[str, Any]) -> Dict[str, Any]:\n",
    "        \"\"\"Generate detailed improvement plan based on integrity analysis\"\"\"\n",
    "        \n",
    "        plan = {\n",
    "            'priority_actions': [],\n",
    "            'suggested_improvements': [],\n",
    "            'resources': [],\n",
    "            'timeline': {}\n",
    "        }\n",
    "        \n",
    "        # High priority actions\n",
    "        if integrity_results['plagiarism_analysis']['similarity_score'] > 0.5:\n",
    "            plan['priority_actions'].append({\n",
    "                'action': 'Address plagiarism concerns',\n",
    "                'description': 'Review and properly cite flagged passages',\n",
    "                'urgency': 'high'\n",
    "            })\n",
    "        \n",
    "        if integrity_results['ai_detection_analysis']['ai_probability'] > 0.7:\n",
    "            plan['priority_actions'].append({\n",
    "                'action': 'Enhance originality',\n",
    "                'description': 'Add personal insights and original analysis',\n",
    "                'urgency': 'high'\n",
    "            })\n",
    "        \n",
    "        # Suggested improvements\n",
    "        uncited_claims = len(integrity_results['citation_analysis']['uncited_claims'])\n",
    "        if uncited_claims > 0:\n",
    "            plan['suggested_improvements'].append({\n",
    "                'improvement': 'Citation enhancement',\n",
    "                'description': f'Add citations for {uncited_claims} uncited claims',\n",
    "                'estimated_time': f'{uncited_claims * 10} minutes'\n",
    "            })\n",
    "        \n",
    "        # Resources\n",
    "        if integrity_results['writing_quality_analysis']['quality_score'] < 0.6:\n",
    "            plan['resources'].extend([\n",
    "                {\n",
    "                    'type': 'writing_guide',\n",
    "                    'title': 'Academic Writing Style Guide',\n",
    "                    'description': 'Improve vocabulary and sentence structure'\n",
    "                },\n",
    "                {\n",
    "                    'type': 'tool',\n",
    "                    'title': 'Grammar checker',\n",
    "                    'description': 'Use tools like Grammarly for grammar improvement'\n",
    "                }\n",
    "            ])\n",
    "        \n",
    "        return plan\n",
    "\n",
    "# FastAPI Application with Enhanced Features\n",
    "app = FastAPI(\n",
    "    title=\"Enhanced Research Assistant API\",\n",
    "    description=\"Advanced research assistant with multilingual support, citation networks, and integrity checking\",\n",
    "    version=\"2.0.0\"\n",
    ")\n",
    "\n",
    "app.add_middleware(\n",
    "    CORSMiddleware,\n",
    "    allow_origins=[\"*\"],\n",
    "    allow_credentials=True,\n",
    "    allow_methods=[\"*\"],\n",
    "    allow_headers=[\"*\"],\n",
    ")\n",
    "\n",
    "# Initialize enhanced research assistant\n",
    "research_assistant = EnhancedResearchAssistant()\n",
    "\n",
    "@app.post(\"/api/v2/search/multilingual\")\n",
    "async def multilingual_search(request: MultilingualSearchRequest):\n",
    "    \"\"\"Enhanced multilingual literature search\"\"\"\n",
    "    try:\n",
    "        results = await research_assistant.multilingual_service.search_multilingual_papers(\n",
    "            request.query,\n",
    "            request.languages,\n",
    "            request.target_language\n",
    "        )\n",
    "        return {\n",
    "            \"success\": True,\n",
    "            \"results\": results[:request.max_results],\n",
    "            \"total_found\": len(results),\n",
    "            \"languages_searched\": request.languages,\n",
    "            \"target_language\": request.target_language\n",
    "        }\n",
    "    except Exception as e:\n",
    "        raise HTTPException(status_code=500, detail=str(e))\n",
    "\n",
    "@app.post(\"/api/v2/citation-network/visualize\")\n",
    "async def create_citation_network(request: CitationNetworkRequest):\n",
    "    \"\"\"Create interactive citation network visualization\"\"\"\n",
    "    try:\n",
    "        # Build citation network\n",
    "        network = await research_assistant.citation_network_service.build_citation_network(\n",
    "            request.paper_ids, request.depth\n",
    "        )\n",
    "        \n",
    "        # Create visualization\n",
    "        fig = research_assistant.citation_network_service.create_interactive_visualization(\n",
    "            network, request.layout_algorithm\n",
    "        )\n",
    "        \n",
    "        # Analyze network patterns\n",
    "        analysis = research_assistant.citation_network_service.analyze_citation_patterns(network)\n",
    "        \n",
    "        return {\n",
    "            \"success\": True,\n",
    "            \"network_stats\": analysis['network_stats'],\n",
    "            \"influential_papers\": analysis['influential_papers'],\n",
    "            \"research_clusters\": analysis['research_clusters'],\n",
    "            \"visualization_data\": fig.to_dict()\n",
    "        }\n",
    "    except Exception as e:\n",
    "        raise HTTPException(status_code=500, detail=str(e))\n",
    "\n",
    "@app.post(\"/api/v2/alerts/create\")\n",
    "async def create_research_alert(request: AlertCreationRequest):\n",
    "    \"\"\"Create custom research alert\"\"\"\n",
    "    try:\n",
    "        if request.alert_type == \"author_publication\":\n",
    "            alert_id = await research_assistant.alerts_service.create_author_alert(\n",
    "                request.parameters['user_id'],\n",
    "                request.parameters['author_name'],\n",
    "                request.frequency\n",
    "            )\n",
    "        elif request.alert_type == \"citation_threshold\":\n",
    "            alert_id = await research_assistant.alerts_service.create_citation_threshold_alert(\n",
    "                request.parameters['user_id'],\n",
    "                request.parameters['paper_title'],\n",
    "                request.parameters['threshold'],\n",
    "                request.parameters.get('time_period', '1 year')\n",
    "            )\n",
    "        elif request.alert_type == \"keyword_trend\":\n",
    "            alert_id = await research_assistant.alerts_service.create_keyword_trend_alert(\n",
    "                request.parameters['user_id'],\n",
    "                request.parameters['keywords'],\n",
    "                request.parameters.get('threshold', 0.5)\n",
    "            )\n",
    "        else:\n",
    "            raise HTTPException(status_code=400, detail=\"Unsupported alert type\")\n",
    "        \n",
    "        return {\n",
    "            \"success\": True,\n",
    "            \"alert_id\": alert_id,\n",
    "            \"alert_type\": request.alert_type,\n",
    "            \"frequency\": request.frequency\n",
    "        }\n",
    "    except Exception as e:\n",
    "        raise HTTPException(status_code=500, detail=str(e))\n",
    "\n",
    "@app.post(\"/api/v2/notebooks/generate\")\n",
    "async def generate_reproducible_notebook(request: NotebookGenerationRequest):\n",
    "    \"\"\"Generate reproducible Jupyter notebook from paper methodology\"\"\"\n",
    "    try:\n",
    "        notebook = await research_assistant.notebook_generator.generate_notebook_from_paper(\n",
    "            request.paper_text,\n",
    "            request.paper_metadata\n",
    "        )\n",
    "        \n",
    "        # Create reproduction package\n",
    "        package = await research_assistant.notebook_generator.create_paper_reproduction_package(\n",
    "            request.paper_metadata,\n",
    "            request.paper_text\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            \"success\": True,\n",
    "            \"notebook_generated\": True,\n",
    "            \"methodology_type\": request.methodology_type,\n",
    "            \"package_contents\": list(package.keys()),\n",
    "            \"requirements\": package['requirements_txt'],\n",
    "            \"readme\": package['readme_md']\n",
    "        }\n",
    "    except Exception as e:\n",
    "        raise HTTPException(status_code=500, detail=str(e))\n",
    "\n",
    "@app.post(\"/api/v2/integrity/check\")\n",
    "async def check_academic_integrity(request: IntegrityCheckRequest):\n",
    "    \"\"\"Comprehensive academic integrity check\"\"\"\n",
    "    try:\n",
    "        results = await research_assistant.comprehensive_document_review(\n",
    "            request.text,\n",
    "            request.metadata\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            \"success\": True,\n",
    "            \"integrity_score\": results['overall_score'],\n",
    "            \"plagiarism_detected\": results['integrity_analysis']['plagiarism_analysis']['similarity_score'] > 0.3,\n",
    "            \"ai_content_detected\": results['integrity_analysis']['ai_detection_analysis']['ai_probability'] > 0.7,\n",
    "            \"uncited_claims\": len(results['integrity_analysis']['citation_analysis']['uncited_claims']),\n",
    "            \"recommendations\": results['integrity_analysis']['recommendations'],\n",
    "            \"improvement_plan\": results['improvement_plan']\n",
    "        }\n",
    "    except Exception as e:\n",
    "        raise HTTPException(status_code=500, detail=str(e))\n",
    "\n",
    "@app.post(\"/api/v2/workflow/create\")\n",
    "async def create_research_workflow(research_topic: str, user_preferences: Dict[str, Any] = {}):\n",
    "    \"\"\"Create comprehensive research workflow\"\"\"\n",
    "    try:\n",
    "        workflow = await research_assistant.create_research_workflow(\n",
    "            research_topic, user_preferences\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            \"success\": True,\n",
    "            \"workflow_id\": str(uuid.uuid4()),\n",
    "            \"research_topic\": research_topic,\n",
    "            \"steps_completed\": len(workflow['workflow_steps']),\n",
    "            \"alerts_created\": len(workflow['alerts_created']),\n",
    "            \"notebooks_generated\": len(workflow['notebooks_generated']),\n",
    "            \"workflow_summary\": workflow\n",
    "        }\n",
    "    except Exception as e:\n",
    "        raise HTTPException(status_code=500, detail=str(e))\n",
    "\n",
    "@app.get(\"/api/v2/health\")\n",
    "async def health_check():\n",
    "    \"\"\"Health check endpoint\"\"\"\n",
    "    return {\n",
    "        \"status\": \"healthy\",\n",
    "        \"version\": \"2.0.0\",\n",
    "        \"features\": [\n",
    "            \"multilingual_search\",\n",
    "            \"citation_networks\", \n",
    "            \"custom_alerts\",\n",
    "            \"notebook_generation\",\n",
    "            \"integrity_checking\"\n",
    "        ],\n",
    "        \"timestamp\": datetime.utcnow()\n",
    "    }\n",
    "\n",
    "print(\"üöÄ Enhanced Research Assistant API implemented!\")\n",
    "print(\"‚úÖ New Features: Multilingual support, citation networks, custom alerts\")\n",
    "print(\"üìì Capabilities: Notebook generation, integrity checking, comprehensive workflows\")\n",
    "print(\"üåê API Endpoints: /api/v2/* with full feature integration\")\n",
    "print(\"üîß Ready to revolutionize research assistance!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "331f0e20",
   "metadata": {},
   "source": [
    "## üß™ Testing and Example Usage\n",
    "\n",
    "Let's test all the enhanced features we've implemented! This section demonstrates how to use each of the new capabilities.\n",
    "\n",
    "### Example Use Cases:\n",
    "\n",
    "1. **Multilingual Research**: Search for papers in multiple languages and get translations\n",
    "2. **Citation Network Analysis**: Visualize how papers connect and influence each other  \n",
    "3. **Custom Research Alerts**: Set up personalized notifications for research updates\n",
    "4. **Reproducible Notebooks**: Auto-generate code from paper methodologies\n",
    "5. **Academic Integrity**: Check documents for plagiarism and AI content\n",
    "\n",
    "Let's see these features in action!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da6aad61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example Usage and Testing of Enhanced Features\n",
    "\n",
    "async def demo_enhanced_research_assistant():\n",
    "    \"\"\"Comprehensive demo of all enhanced research assistant features\"\"\"\n",
    "    \n",
    "    print(\"üöÄ Starting Enhanced Research Assistant Demo!\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Initialize the enhanced research assistant\n",
    "    # Note: In production, you would provide actual API keys\n",
    "    assistant = EnhancedResearchAssistant(openai_api_key=\"your-openai-key-here\")\n",
    "    \n",
    "    # Example 1: Multilingual Research Search\n",
    "    print(\"\\n1. üåç Multilingual Research Search\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    try:\n",
    "        multilingual_results = await assistant.multilingual_service.search_multilingual_papers(\n",
    "            query=\"machine learning in healthcare\",\n",
    "            languages=['en', 'es', 'fr', 'de'],\n",
    "            target_language='en'\n",
    "        )\n",
    "        \n",
    "        print(f\"‚úÖ Found {len(multilingual_results)} papers across multiple languages\")\n",
    "        for i, paper in enumerate(multilingual_results[:3]):\n",
    "            print(f\"  {i+1}. {paper.get('title_translated', paper.get('title', 'N/A'))}\")\n",
    "            print(f\"     Language: {paper.get('search_language', 'en')}\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Multilingual search demo failed: {e}\")\n",
    "    \n",
    "    # Example 2: Citation Network Visualization\n",
    "    print(\"\\n2. üï∏Ô∏è Citation Network Analysis\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    try:\n",
    "        # Simulate paper IDs for demo\n",
    "        sample_paper_ids = [1, 2, 3, 4, 5]\n",
    "        \n",
    "        citation_network = await assistant.citation_network_service.build_citation_network(\n",
    "            sample_paper_ids, depth=2\n",
    "        )\n",
    "        \n",
    "        network_analysis = assistant.citation_network_service.analyze_citation_patterns(citation_network)\n",
    "        \n",
    "        print(f\"‚úÖ Network built with {network_analysis['network_stats']['total_papers']} papers\")\n",
    "        print(f\"   Total citations: {network_analysis['network_stats']['total_citations']}\")\n",
    "        print(f\"   Network density: {network_analysis['network_stats']['density']:.3f}\")\n",
    "        \n",
    "        # Create interactive visualization\n",
    "        visualization = assistant.citation_network_service.create_interactive_visualization(citation_network)\n",
    "        print(\"   üìä Interactive visualization created (Plotly figure)\")\n",
    "        \n",
    "        # Detect research clusters\n",
    "        clusters = assistant.citation_network_service.detect_research_clusters(citation_network)\n",
    "        print(f\"   üîç Detected {len(clusters)} research clusters\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Citation network demo failed: {e}\")\n",
    "    \n",
    "    # Example 3: Custom Research Alerts\n",
    "    print(\"\\n3. üö® Custom Research Alerts\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    try:\n",
    "        # Create different types of alerts\n",
    "        author_alert = await assistant.alerts_service.create_author_alert(\n",
    "            user_id=\"demo_user\",\n",
    "            author_name=\"Geoffrey Hinton\",\n",
    "            alert_frequency=\"weekly\"\n",
    "        )\n",
    "        print(f\"‚úÖ Author alert created: {author_alert[:8]}...\")\n",
    "        \n",
    "        keyword_alert = await assistant.alerts_service.create_keyword_trend_alert(\n",
    "            user_id=\"demo_user\",\n",
    "            keywords=[\"transformers\", \"attention mechanism\"],\n",
    "            trend_threshold=0.6\n",
    "        )\n",
    "        print(f\"‚úÖ Keyword trend alert created: {keyword_alert[:8]}...\")\n",
    "        \n",
    "        citation_alert = await assistant.alerts_service.create_citation_threshold_alert(\n",
    "            user_id=\"demo_user\",\n",
    "            paper_title=\"Attention Is All You Need\",\n",
    "            citation_threshold=1000,\n",
    "            time_period=\"6 months\"\n",
    "        )\n",
    "        print(f\"‚úÖ Citation threshold alert created: {citation_alert[:8]}...\")\n",
    "        \n",
    "        # Get user alerts\n",
    "        user_alerts = assistant.alerts_service.get_user_alerts(\"demo_user\")\n",
    "        print(f\"   üì± Total alerts for user: {len(user_alerts)}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Custom alerts demo failed: {e}\")\n",
    "    \n",
    "    # Example 4: Reproducible Notebook Generation\n",
    "    print(\"\\n4. üìì Reproducible Notebook Generation\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    try:\n",
    "        # Sample paper metadata and text\n",
    "        sample_paper = {\n",
    "            'title': 'Deep Learning for Image Classification',\n",
    "            'authors': ['Dr. Jane Smith', 'Dr. John Doe'],\n",
    "            'doi': '10.1000/example',\n",
    "            'year': 2024\n",
    "        }\n",
    "        \n",
    "        sample_methodology = \"\"\"\n",
    "        This paper presents a deep learning approach for image classification.\n",
    "        We used a convolutional neural network with the following architecture:\n",
    "        - Input layer: 224x224x3 images\n",
    "        - Convolutional layers with ReLU activation\n",
    "        - Max pooling layers\n",
    "        - Fully connected layers\n",
    "        - Softmax output layer\n",
    "        \n",
    "        The model was trained using the Adam optimizer with a learning rate of 0.001.\n",
    "        We evaluated the model using accuracy, precision, and recall metrics.\n",
    "        \"\"\"\n",
    "        \n",
    "        notebook = await assistant.notebook_generator.generate_notebook_from_paper(\n",
    "            sample_methodology, sample_paper\n",
    "        )\n",
    "        \n",
    "        print(f\"‚úÖ Jupyter notebook generated with {len(notebook.cells)} cells\")\n",
    "        \n",
    "        # Create reproduction package\n",
    "        reproduction_package = await assistant.notebook_generator.create_paper_reproduction_package(\n",
    "            sample_paper, sample_methodology\n",
    "        )\n",
    "        \n",
    "        print(f\"   üì¶ Reproduction package created with {len(reproduction_package)} components:\")\n",
    "        for component in reproduction_package.keys():\n",
    "            print(f\"      - {component}\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Notebook generation demo failed: {e}\")\n",
    "    \n",
    "    # Example 5: Academic Integrity Checking\n",
    "    print(\"\\n5. üîç Academic Integrity Checking\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    try:\n",
    "        # Sample text for integrity checking\n",
    "        sample_text = \"\"\"\n",
    "        Machine learning has revolutionized many fields in recent years. \n",
    "        Research shows that deep learning models can achieve remarkable performance\n",
    "        on various tasks. However, it is important to note that these models\n",
    "        require large amounts of data for training. Furthermore, the interpretability\n",
    "        of these models remains a significant challenge in the field.\n",
    "        \"\"\"\n",
    "        \n",
    "        integrity_results = await assistant.integrity_checker.check_document_integrity(\n",
    "            sample_text, {'title': 'Sample Research Text'}\n",
    "        )\n",
    "        \n",
    "        print(f\"‚úÖ Integrity analysis completed:\")\n",
    "        print(f\"   Overall integrity score: {integrity_results['overall_integrity_score']:.3f}\")\n",
    "        print(f\"   Plagiarism similarity: {integrity_results['plagiarism_analysis']['similarity_score']:.3f}\")\n",
    "        print(f\"   AI content probability: {integrity_results['ai_detection_analysis']['ai_probability']:.3f}\")\n",
    "        print(f\"   Citation analysis: {len(integrity_results['citation_analysis']['uncited_claims'])} uncited claims\")\n",
    "        print(f\"   Writing quality score: {integrity_results['writing_quality_analysis']['quality_score']:.3f}\")\n",
    "        \n",
    "        print(f\"   üìã Recommendations:\")\n",
    "        for rec in integrity_results['recommendations']:\n",
    "            print(f\"      - {rec}\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Integrity checking demo failed: {e}\")\n",
    "    \n",
    "    # Example 6: Complete Research Workflow\n",
    "    print(\"\\n6. üîÑ Complete Research Workflow\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    try:\n",
    "        user_preferences = {\n",
    "            'languages': ['en', 'es'],\n",
    "            'alerts': [\n",
    "                {\n",
    "                    'type': 'keyword_trend',\n",
    "                    'user_id': 'demo_user',\n",
    "                    'keywords': ['artificial intelligence', 'neural networks'],\n",
    "                    'threshold': 0.7\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "        \n",
    "        workflow = await assistant.create_research_workflow(\n",
    "            research_topic=\"Artificial Intelligence in Healthcare\",\n",
    "            user_preferences=user_preferences\n",
    "        )\n",
    "        \n",
    "        print(f\"‚úÖ Research workflow created:\")\n",
    "        print(f\"   Topic: {workflow['research_topic']}\")\n",
    "        print(f\"   Steps completed: {len(workflow['workflow_steps'])}\")\n",
    "        print(f\"   Alerts created: {len(workflow['alerts_created'])}\")\n",
    "        print(f\"   Notebooks generated: {len(workflow['notebooks_generated'])}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Research workflow demo failed: {e}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"üéâ Enhanced Research Assistant Demo Completed!\")\n",
    "    print(\"üöÄ All features successfully demonstrated!\")\n",
    "\n",
    "# Test individual components\n",
    "def test_multilingual_translation():\n",
    "    \"\"\"Test multilingual translation capabilities\"\"\"\n",
    "    print(\"\\nüîß Testing Multilingual Translation\")\n",
    "    print(\"-\" * 30)\n",
    "    \n",
    "    service = MultilingualResearchService()\n",
    "    \n",
    "    # Test language detection\n",
    "    sample_texts = [\n",
    "        \"This is an English sentence about machine learning.\",\n",
    "        \"Esto es una oraci√≥n en espa√±ol sobre aprendizaje autom√°tico.\",\n",
    "        \"Ceci est une phrase en fran√ßais sur l'apprentissage automatique.\",\n",
    "        \"Dies ist ein deutscher Satz √ºber maschinelles Lernen.\"\n",
    "    ]\n",
    "    \n",
    "    for text in sample_texts:\n",
    "        detected_lang = asyncio.run(service.detect_language(text))\n",
    "        print(f\"'{text[:30]}...' -> Language: {detected_lang}\")\n",
    "\n",
    "def test_citation_network_metrics():\n",
    "    \"\"\"Test citation network analysis metrics\"\"\"\n",
    "    print(\"\\nüîß Testing Citation Network Metrics\")\n",
    "    print(\"-\" * 30)\n",
    "    \n",
    "    import networkx as nx\n",
    "    \n",
    "    service = CitationNetworkService()\n",
    "    \n",
    "    # Create sample network\n",
    "    G = nx.DiGraph()\n",
    "    G.add_edges_from([(1, 2), (2, 3), (3, 4), (4, 1), (1, 3)])\n",
    "    \n",
    "    # Add sample node attributes\n",
    "    for node in G.nodes():\n",
    "        G.nodes[node]['title'] = f'Paper {node}'\n",
    "        G.nodes[node]['citation_count'] = node * 10\n",
    "        G.nodes[node]['year'] = 2020 + node\n",
    "    \n",
    "    service._calculate_network_metrics(G)\n",
    "    \n",
    "    print(\"Network metrics calculated:\")\n",
    "    for node in G.nodes():\n",
    "        print(f\"  Paper {node}: PageRank={G.nodes[node]['pagerank']:.3f}\")\n",
    "\n",
    "def test_alert_configurations():\n",
    "    \"\"\"Test different alert configurations\"\"\"\n",
    "    print(\"\\nüîß Testing Alert Configurations\")\n",
    "    print(\"-\" * 30)\n",
    "    \n",
    "    service = ResearchAlertsService()\n",
    "    \n",
    "    # Test notification channel configuration\n",
    "    service.configure_notification_channel(\"test_user\", [\"email\", \"webhook\"])\n",
    "    \n",
    "    print(\"‚úÖ Notification channels configured\")\n",
    "    print(f\"User channels: {service.notification_channels.get('test_user', [])}\")\n",
    "    \n",
    "    # Test alert subscription management\n",
    "    print(f\"Total alerts in system: {len(service.alert_subscriptions)}\")\n",
    "\n",
    "def test_notebook_code_generation():\n",
    "    \"\"\"Test notebook code generation\"\"\"\n",
    "    print(\"\\nüîß Testing Notebook Code Generation\")\n",
    "    print(\"-\" * 30)\n",
    "    \n",
    "    generator = ReproducibleNotebookGenerator()\n",
    "    \n",
    "    # Test code template loading\n",
    "    templates = generator.code_templates\n",
    "    print(f\"‚úÖ Loaded {len(templates)} code templates:\")\n",
    "    for template_name in templates.keys():\n",
    "        print(f\"   - {template_name}\")\n",
    "    \n",
    "    # Test requirements generation\n",
    "    sample_text = \"This paper uses deep learning, nltk for natural language processing, and plotly for visualization.\"\n",
    "    requirements = generator._generate_requirements_file(sample_text)\n",
    "    \n",
    "    print(f\"\\nGenerated requirements.txt:\")\n",
    "    for req in requirements.split('\\n')[:5]:  # Show first 5 requirements\n",
    "        print(f\"   {req}\")\n",
    "\n",
    "def test_integrity_checker_patterns():\n",
    "    \"\"\"Test integrity checker pattern detection\"\"\"\n",
    "    print(\"\\nüîß Testing Integrity Checker Patterns\")\n",
    "    print(\"-\" * 30)\n",
    "    \n",
    "    checker = AcademicIntegrityChecker()\n",
    "    \n",
    "    # Test AI detection patterns\n",
    "    sample_ai_text = \"\"\"\n",
    "    As a language model, I can provide various insights. It is important to note that\n",
    "    furthermore, numerous factors should be considered. Moreover, various aspects\n",
    "    require attention.\n",
    "    \"\"\"\n",
    "    \n",
    "    detected_patterns = []\n",
    "    for pattern_info in checker.ai_detection_patterns:\n",
    "        matches = re.findall(pattern_info['pattern'], sample_ai_text.lower())\n",
    "        if matches:\n",
    "            detected_patterns.append({\n",
    "                'type': pattern_info['type'],\n",
    "                'matches': matches,\n",
    "                'weight': pattern_info['weight']\n",
    "            })\n",
    "    \n",
    "    print(f\"‚úÖ Detected {len(detected_patterns)} AI patterns:\")\n",
    "    for pattern in detected_patterns:\n",
    "        print(f\"   {pattern['type']}: {pattern['matches']} (weight: {pattern['weight']})\")\n",
    "\n",
    "# Run individual tests\n",
    "print(\"üß™ Running Individual Component Tests\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "test_multilingual_translation()\n",
    "test_citation_network_metrics() \n",
    "test_alert_configurations()\n",
    "test_notebook_code_generation()\n",
    "test_integrity_checker_patterns()\n",
    "\n",
    "print(\"\\nüéØ Individual tests completed!\")\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"üìö To run the full demo, call: await demo_enhanced_research_assistant()\")\n",
    "print(\"‚ö†Ô∏è  Note: Some features require API keys for full functionality\")\n",
    "print(\"üîó API endpoints available at /api/v2/* when FastAPI server is running\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
